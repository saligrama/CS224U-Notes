<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CS 149, Fall 2022 on Aditya's notes</title><link>https://saligrama.io/notes/cs149/</link><description>Recent content in CS 149, Fall 2022 on Aditya's notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://saligrama.io/notes/cs149/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://saligrama.io/notes/cs149/2022-09-27-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-09-27-intro/</guid><description>Parallel Computing # Course themes # Designing parallel programs that scale # Parallel thinking Decomposing work into pieces that can safely be performed in parallel Assigning work to processors Managing communication/synchronization between the processors to not limit speedup Parallel computer hardware implementation # How do parallel computers work? Mechanisms used to implement different abstractions efficiently Performance characteristics of implementations Design tradeoffs: performance vs. convenience vs. cost Why know about hardware?</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-09-29-modern-multicore-processors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-09-29-modern-multicore-processors/</guid><description>A Modern Multi-core Processor # Part 1: Speeding up an example sine program # Computes sin across an array of float values using a Taylor series expansion
void sinx(int N, int terms, float *x, float *y) { for (int i = 0; i &amp;lt; N; i++) { float value = x[i]; float numer = x[i] * x[i] * x[i]; int denom = 6; // 3! int sign = -1; for (int j = 1; j &amp;lt;= terms; j++) { value += sign * numer / denom; numer *= x[i] * x[i]; denom *= (2 * j + 2) * (2 * j + 3); sign *= -1; } y[i] = value; } } This compiles to some assembly:</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-04-parallel-abstractions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-04-parallel-abstractions/</guid><description>Parallel Abstractions and Implementation # ISPC # Intel SPMD (single program, multiple dat) Program Compiler: https://ispc.github.io e.g. sinx using ISPC
export void ispc_sinx( // uniform: type modifier for optimization, not necessary for correctness uniform int N, uniform int terms, uniform float *x, uniform float *result ) { // programCount: keyword, number of simultaneously executing gang instances for (uniform int i = 0; i &amp;lt; N; i += programCount) { // programIndex: id of current gang instance int idx = i + programIndex; float value = x[idx]; float numer = x[idx] * x[idx] * x[idx]; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 1; j &amp;lt;= terms; j++) { value += sign * numer / denom; numer *= x[idx] * x[idx]; denom *= (2*j+2) * (2*j + 3); sign *= -1; } result[idx] = value; } } int main (int argc, char **argv) { // initializations ispc_sinx(N, terms, x, result); return 0; } ISPC execution Call to ISPC function spawns &amp;ldquo;gang&amp;rdquo; of ISPC &amp;ldquo;program instances&amp;rdquo; All instances run ISPC code concurrently Each instance has its own copy of local variables Upon return, all instances have completed What ISPC does Turns instructions into SIMD at compilation time Number of instances in a gang: SIMD width of the hardware (or small multiple thereof) Does not spawn new threads!</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-06-parallel-models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-06-parallel-models/</guid><description>Models of Parallel Programming # Three types of models:
Shared address space: very little structure to communication Message passing: communication is structured in the form of messages Communication explicit in source code Data parallel structure: more rigid structure to computation Same function on collection of large elements Shared address space model # All threads access the same memory Requires coordinated access to shared data using locks, e.g. // thread 1 int x = 0; Lock my_lock; spawn_thread(foo, &amp;amp;x, &amp;amp;my_lock); my_lock.</description></item></channel></rss>