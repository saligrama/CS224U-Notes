<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CS 149, Fall 2022 on Aditya's notes</title><link>https://saligrama.io/notes/cs149/</link><description>Recent content in CS 149, Fall 2022 on Aditya's notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://saligrama.io/notes/cs149/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://saligrama.io/notes/cs149/2022-09-27-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-09-27-intro/</guid><description>Parallel Computing # Course themes # Designing parallel programs that scale # Parallel thinking Decomposing work into pieces that can safely be performed in parallel Assigning work to processors Managing communication/synchronization between the processors to not limit speedup Parallel computer hardware implementation # How do parallel computers work? Mechanisms used to implement different abstractions efficiently Performance characteristics of implementations Design tradeoffs: performance vs. convenience vs. cost Why know about hardware?</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-09-29-modern-multicore-processors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-09-29-modern-multicore-processors/</guid><description>A Modern Multi-core Processor # Part 1: Speeding up an example sine program # Computes sin across an array of float values using a Taylor series expansion
void sinx(int N, int terms, float *x, float *y) { for (int i = 0; i &amp;lt; N; i++) { float value = x[i]; float numer = x[i] * x[i] * x[i]; int denom = 6; // 3! int sign = -1; for (int j = 1; j &amp;lt;= terms; j++) { value += sign * numer / denom; numer *= x[i] * x[i]; denom *= (2 * j + 2) * (2 * j + 3); sign *= -1; } y[i] = value; } } This compiles to some assembly:</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-04-parallel-abstractions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-04-parallel-abstractions/</guid><description>Parallel Abstractions and Implementation # ISPC # Intel SPMD (single program, multiple dat) Program Compiler: https://ispc.github.io e.g. sinx using ISPC
export void ispc_sinx( // uniform: type modifier for optimization, not necessary for correctness uniform int N, uniform int terms, uniform float *x, uniform float *result ) { // programCount: keyword, number of simultaneously executing gang instances for (uniform int i = 0; i &amp;lt; N; i += programCount) { // programIndex: id of current gang instance int idx = i + programIndex; float value = x[idx]; float numer = x[idx] * x[idx] * x[idx]; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 1; j &amp;lt;= terms; j++) { value += sign * numer / denom; numer *= x[idx] * x[idx]; denom *= (2*j+2) * (2*j + 3); sign *= -1; } result[idx] = value; } } int main (int argc, char **argv) { // initializations ispc_sinx(N, terms, x, result); return 0; } ISPC execution Call to ISPC function spawns &amp;ldquo;gang&amp;rdquo; of ISPC &amp;ldquo;program instances&amp;rdquo; All instances run ISPC code concurrently Each instance has its own copy of local variables Upon return, all instances have completed What ISPC does Turns instructions into SIMD at compilation time Number of instances in a gang: SIMD width of the hardware (or small multiple thereof) Does not spawn new threads!</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-06-parallel-models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-06-parallel-models/</guid><description>Models of Parallel Programming # Three types of models:
Shared address space: very little structure to communication Message passing: communication is structured in the form of messages Communication explicit in source code Data parallel structure: more rigid structure to computation Same function on collection of large elements Shared address space model # All threads access the same memory Requires coordinated access to shared data using locks, e.g. // thread 1 int x = 0; Lock my_lock; spawn_thread(foo, &amp;amp;x, &amp;amp;my_lock); my_lock.</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-11-work-distribution-and-scheduling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-11-work-distribution-and-scheduling/</guid><description>Performance Optimization: Work Distribution and Scheduling # Programming for high performance # Iterative process: refine choices for decomposition, assignment, orchestration Goals (that can be at odds with each other): Balance workload onto available execution resources Reduce communication Reduce extra work overhead Note: always implement the simplest solution first, then profile to figure out where and how to do better for best impact Balancing the workload # Static assignment # Assignment of work to threads not dependent on runtime factors Note: distribution not determined at compile-time; can be impacted by provided parameters Good properties of static assignment: Simple, near-zero runtime overhead for assignment (e.</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-13-locality-communication-and-contention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-13-locality-communication-and-contention/</guid><description>Performance Optimization: Locality, Communication, Contention # Message passing review # Machine model: cluster of two machines, each with own processor/memory/cache, and communication via network (costly) Note: communication can also be between cores on a chip, core and its cache, core and memory This model allows us to think of a parallel system as extended memory hierarchy, in terms of increasing latency: Processor Registers Local L1 cache Local L2 cache L2 from another core L3 cache Local memory Remote memory (one network hop) Remote memory (N network hops) Communication: via send/receive messages send: specifies recipient, buffer to be transmitted, identifier (tag) receive: sender, specifies buffer to store data, identifier (tag) Only way for thread communication Synchronous (blocking) send/receive # send(): call returns once sender receives acknowledgement that message data resides in address space of receiver recv(): call returns when data from received message copied into receiver address space and acknowledgement sent back to sender Non-blocking asynchronous send/receive # send(): call returns immediately Buffer provided to send cannot be modified by calling thread since message processing occurs concurrently with thread execution Calling thread can perform other work while waiting for send recv(): posts intent to receive in the future, returns immediately Use checksend(), checkrecv() to determine actual status of send/receipt Calling thread can perform other work while waiting for receive Communicaton-to-computation ratio # Fraction: (# bytes communication)/(# instructions computation) If denominator is execution time of computation, then ratio gives avg.</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-18-gpu-architecture-and-cuda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-18-gpu-architecture-and-cuda/</guid><description>GPU architecture and CUDA # GPU history # Graphical purpose of a GPU # Initially designed for: Input: description of a scene; mathematical description e.g. 3D surface geometry, surface materials, lights, camera, etc. Output: image of the scene Rendering task # Real-time graphics primitives (entities) Surfaces represented as 3D triangle meshes: vertices (points in space), primitives (points, lines, triangles) Goal: compute how each triangle in 3D mesh contributes to overall image Subtask workload: given triangle, determine where it lies on screen given position of virtual cameras For all output image pixels covered by triangle, compute color of surface at that pixel Shader program: run once per fragment (per pixel covered by triangle) Inputs: variable values that change per pixel Outputs: colors at those pixels Pixels covered by multiple surfaces contain output from surfaces closest to camera To optimize: GPUs designed with multiple core, high-throughput (lots of SIMD and multithreading) architecture GPUs for scientific and compute task # Initial observation (2001-2003): GPUs are very fast processors for performing same computation in parallel on large collections of data Data parallelism!</description></item><item><title/><link>https://saligrama.io/notes/cs149/2022-10-20-data-parallel-architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://saligrama.io/notes/cs149/2022-10-20-data-parallel-architecture/</guid><description>Data-parallel thinking # Motivation # Idea: express algorithms in terms of operations on sequences of data e.g. map, filter, fold/reduce, scan/segmented scan, sort, groupby, join, partition/flatten High-performance parallel versions exist; so can we reframe programs in these terms to make them run efficiently on a parallel machine? In general: applications need to expose a large amount of data parallelism to make use of High core counts Multiple machines SIMD processing GPU architectures Key terms and operations # Data-parallel model, sequences, map: recall Lecture 4&amp;rsquo;s intro on data parallelism Fold (fold left) # Apply binary operation f to each element and an accumulated value; seeded by initial value of type b f :: (b, a) -&amp;gt; b fold :: b -&amp;gt; # initial element ((b, a) -&amp;gt; b) -&amp;gt; # function to fold seq a -&amp;gt; # input sequence b # output e.</description></item></channel></rss>