<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="GPU architecture and CUDA # GPU history # Graphical purpose of a GPU # Initially designed for: Input: description of a scene; mathematical description e.g. 3D surface geometry, surface materials, lights, camera, etc. Output: image of the scene Rendering task # Real-time graphics primitives (entities) Surfaces represented as 3D triangle meshes: vertices (points in space), primitives (points, lines, triangles) Goal: compute how each triangle in 3D mesh contributes to overall image Subtask workload: given triangle, determine where it lies on screen given position of virtual cameras For all output image pixels covered by triangle, compute color of surface at that pixel Shader program: run once per fragment (per pixel covered by triangle) Inputs: variable values that change per pixel Outputs: colors at those pixels Pixels covered by multiple surfaces contain output from surfaces closest to camera To optimize: GPUs designed with multiple core, high-throughput (lots of SIMD and multithreading) architecture GPUs for scientific and compute task # Initial observation (2001-2003): GPUs are very fast processors for performing same computation in parallel on large collections of data Data parallelism!"><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="GPU architecture and CUDA # GPU history # Graphical purpose of a GPU # Initially designed for: Input: description of a scene; mathematical description e.g. 3D surface geometry, surface materials, lights, camera, etc. Output: image of the scene Rendering task # Real-time graphics primitives (entities) Surfaces represented as 3D triangle meshes: vertices (points in space), primitives (points, lines, triangles) Goal: compute how each triangle in 3D mesh contributes to overall image Subtask workload: given triangle, determine where it lies on screen given position of virtual cameras For all output image pixels covered by triangle, compute color of surface at that pixel Shader program: run once per fragment (per pixel covered by triangle) Inputs: variable values that change per pixel Outputs: colors at those pixels Pixels covered by multiple surfaces contain output from surfaces closest to camera To optimize: GPUs designed with multiple core, high-throughput (lots of SIMD and multithreading) architecture GPUs for scientific and compute task # Initial observation (2001-2003): GPUs are very fast processors for performing same computation in parallel on large collections of data Data parallelism!"><meta property="og:type" content="article"><meta property="og:url" content="https://saligrama.io/notes/cs149/2022-10-18-gpu-architecture-and-cuda/"><meta property="article:section" content="cs149"><title>Gpu Architecture and Cuda | Aditya's notes</title><link rel=manifest href=/notes/manifest.json><link rel=icon href=/notes/favicon.png type=image/x-icon><link rel=stylesheet href=/notes/book.min.395a67680f48b8d23bbf267f26d0d1259e69554b2b704e371e8e15cbe656e05f.css integrity="sha256-OVpnaA9IuNI7vyZ/JtDRJZ5pVUsrcE43Ho4Vy+ZW4F8=" crossorigin=anonymous><script defer src=/notes/flexsearch.min.js></script>
<script defer src=/notes/en.search.min.20185cea7506331f915ab1ca9873ed3df866348f1939b82a7777023f528a3a47.js integrity="sha256-IBhc6nUGMx+RWrHKmHPtPfhmNI8ZObgqd3cCP1KKOkc=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/notes/><span>Aditya's notes</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><input type=checkbox id=section-a88f46c31d1ebdf165810583424fda37 class=toggle>
<label for=section-a88f46c31d1ebdf165810583424fda37 class="flex justify-between"><a role=button>CS 103, Fall 2020</a></label><ul><li><a href=/notes/cs103/2020-09-16-set-theory/>Set Theory</a></li><li><a href=/notes/cs103/2020-09-18-indirect-proofs/>Indirect Proofs</a></li><li><a href=/notes/cs103/2020-09-18-mathematical-proofs/>Mathematical Proofs</a></li><li><a href=/notes/cs103/2020-09-26-first-order-logic/>First Order Logic</a></li><li><a href=/notes/cs103/2020-09-26-propositional-logic/>Propositional Logic</a></li><li><a href=/notes/cs103/2020-09-27-first-order-logic-continued/>First Order Logic Continued</a></li><li><a href=/notes/cs103/2020-09-30-binary-relations/>Binary Relations</a></li><li><a href=/notes/cs103/2020-10-01-binary-relations-continued/>Binary Relations Continued</a></li><li><a href=/notes/cs103/2020-10-04-functions/>Functions</a></li><li><a href=/notes/cs103/2020-10-10-cardinality/>Cardinality</a></li><li><a href=/notes/cs103/2020-10-11-graph-theory/>Graph Theory</a></li><li><a href=/notes/cs103/2020-10-17-pigeonhole-principle/>Pigeonhole Principle</a></li><li><a href=/notes/cs103/2020-10-18-induction/>Induction</a></li><li><a href=/notes/cs103/2020-10-19-induction-variants/>Induction Variants</a></li><li><a href=/notes/cs103/2020-10-20-computability-and-formal-languages/>Computability and Formal Languages</a></li><li><a href=/notes/cs103/2020-10-25-nondeterministic-finite-automata/>Nondeterministic Finite Automata</a></li><li><a href=/notes/cs103/2020-10-26-nfa-dfa-equivalence/>Nfa Dfa Equivalence</a></li><li><a href=/notes/cs103/2020-10-26-regular-expressions/>Regular Expressions</a></li><li><a href=/notes/cs103/2020-11-01-nonregular-languages/>Nonregular Languages</a></li><li><a href=/notes/cs103/2020-11-02-context-free-grammars/>Context Free Grammars</a></li><li><a href=/notes/cs103/2020-11-03-turing-machines/>Turing Machines</a></li><li><a href=/notes/cs103/2020-11-08-turing-machine-subroutines/>Turing Machine Subroutines</a></li><li><a href=/notes/cs103/2020-11-08-universal-turing-machine/>Universal Turing Machine</a></li><li><a href=/notes/cs103/2020-11-08-unsolvable-problems/>Unsolvable Problems</a></li><li><a href=/notes/cs103/2020-11-14-unsolvable-problems-continued/>Unsolvable Problems Continued</a></li></ul></li><li><input type=checkbox id=section-cf13475a7a80a5dd57b5a55dce73d171 class=toggle>
<label for=section-cf13475a7a80a5dd57b5a55dce73d171 class="flex justify-between"><a role=button>CS 107, Fall 2020</a></label><ul><li><a href=/notes/cs107/2020-09-18-integer-representations/>Integer Representations</a></li><li><a href=/notes/cs107/2020-09-21-bitwise-operations/>Bitwise Operations</a></li><li><a href=/notes/cs107/2020-09-25-c-chars-and-strings/>C Chars and Strings</a></li><li><a href=/notes/cs107/2020-09-28-more-c-strings/>More C Strings</a></li><li><a href=/notes/cs107/2020-10-02-pointers-arrays/>Pointers Arrays</a></li><li><a href=/notes/cs107/2020-10-05-stack-and-heap/>Stack and Heap</a></li><li><a href=/notes/cs107/2020-10-09-c-generics/>C Generics</a></li><li><a href=/notes/cs107/2020-10-12-function-pointers/>Function Pointers</a></li><li><a href=/notes/cs107/2020-10-16-assembly/>Assembly</a></li><li><a href=/notes/cs107/2020-10-19-assembly-arithmetic-logic/>Assembly Arithmetic Logic</a></li><li><a href=/notes/cs107/2020-10-23-assembly-control-flow/>Assembly Control Flow</a></li><li><a href=/notes/cs107/2020-10-26-assembly-function-calls-and-return-stack/>Assembly Function Calls and Return Stack</a></li><li><a href=/notes/cs107/2020-10-30-heap-management/>Heap Management</a></li><li><a href=/notes/cs107/2020-11-09-program-optimization/>Program Optimization</a></li></ul></li><li><input type=checkbox id=section-4e3bc6e2f7a0feae33c3e43356d49c80 class=toggle>
<label for=section-4e3bc6e2f7a0feae33c3e43356d49c80 class="flex justify-between"><a role=button>CS 110L, Spring 2021</a></label><ul><li><a href=/notes/cs110l/2021-03-30-course-overview/>Course Overview</a></li><li><a href=/notes/cs110l/2021-04-01-fixing-c/>Fixing C</a></li><li><a href=/notes/cs110l/2021-04-06-intro-to-rust/>Intro to Rust</a></li><li><a href=/notes/cs110l/2021-04-08-ownership/>Ownership</a></li><li><a href=/notes/cs110l/2021-04-13-error-handling/>Error Handling</a></li><li><a href=/notes/cs110l/2021-04-22-traits/>Traits</a></li><li><a href=/notes/cs110l/2021-04-27-generics/>Generics</a></li><li><a href=/notes/cs110l/2021-04-29-multiprocessing/>Multiprocessing</a></li></ul></li><li><input type=checkbox id=section-23e7773af736437c02202412a988f2db class=toggle>
<label for=section-23e7773af736437c02202412a988f2db class="flex justify-between"><a role=button>CS 111, Spring 2021</a></label><ul><li><a href=/notes/cs111/2021-03-31-threads-and-dispatching/>Threads and Dispatching</a></li><li><a href=/notes/cs111/2021-04-02-concurrency/>Concurrency</a></li><li><a href=/notes/cs111/2021-04-05-synchronization/>Synchronization</a></li><li><a href=/notes/cs111/2021-04-07-shared-memory-and-condition-variables-and-locks/>Shared Memory and Condition Variables and Locks</a></li><li><a href=/notes/cs111/2021-04-09-lock-implementation-and-deadlocking/>Lock Implementation and Deadlocking</a></li><li><a href=/notes/cs111/2021-04-12-scheduling/>Scheduling</a></li><li><a href=/notes/cs111/2021-04-14-multiprocessing/>Multiprocessing</a></li><li><a href=/notes/cs111/2021-04-16-linking/>Linking</a></li><li><a href=/notes/cs111/2021-04-19-storage-management/>Storage Management</a></li><li><a href=/notes/cs111/2021-04-21-virtual-memory/>Virtual Memory</a></li><li><a href=/notes/cs111/2021-04-23-dynamic-address-translation/>Dynamic Address Translation</a></li><li><a href=/notes/cs111/2021-04-26-segmentation-and-paging/>Segmentation and Paging</a></li><li><a href=/notes/cs111/2021-04-30-demand-paging/>Demand Paging</a></li><li><a href=/notes/cs111/2021-05-05-disks/>Disks</a></li><li><a href=/notes/cs111/2021-05-07-file-systems/>File Systems</a></li><li><a href=/notes/cs111/2021-05-10-realworld-filesystem-structures/>Realworld Filesystem Structures</a></li><li><a href=/notes/cs111/2021-05-12-directories/>Directories</a></li><li><a href=/notes/cs111/2021-05-14-crash-recovery/>Crash Recovery</a></li><li><a href=/notes/cs111/2021-05-19-protection/>Protection</a></li><li><a href=/notes/cs111/2021-05-24-flash-memory/>Flash Memory</a></li><li><a href=/notes/cs111/2021-05-28-virtual-machines/>Virtual Machines</a></li></ul></li><li><input type=checkbox id=section-a586b190ebf0a4874cd21a1d76743261 class=toggle>
<label for=section-a586b190ebf0a4874cd21a1d76743261 class="flex justify-between"><a role=button>CS 143, Spring 2022</a></label><ul><li><a href=/notes/cs143/2022-03-29-intro/>Intro</a></li><li><a href=/notes/cs143/2022-03-31-language-design-and-cool/>Language Design and Cool</a></li><li><a href=/notes/cs143/2022-04-05-lexical-analysis/>Lexical Analysis</a></li><li><a href=/notes/cs143/2022-04-07-lexical-analysis-implementation/>Lexical Analysis Implementation</a></li><li><a href=/notes/cs143/2022-04-12-parsing/>Parsing</a></li><li><a href=/notes/cs143/2022-04-14-syntax-directed-translation/>Syntax Directed Translation</a></li><li><a href=/notes/cs143/2022-04-19-top-down-parsing/>Top Down Parsing</a></li><li><a href=/notes/cs143/2022-04-26-semantic-analysis/>Semantic Analysis</a></li></ul></li><li><input type=checkbox id=section-37836de7a703e433b2642097d511f482 class=toggle checked>
<label for=section-37836de7a703e433b2642097d511f482 class="flex justify-between"><a role=button>CS 149, Fall 2022</a></label><ul><li><a href=/notes/cs149/2022-09-27-intro/>Intro</a></li><li><a href=/notes/cs149/2022-09-29-modern-multicore-processors/>Modern Multicore Processors</a></li><li><a href=/notes/cs149/2022-10-04-parallel-abstractions/>Parallel Abstractions</a></li><li><a href=/notes/cs149/2022-10-06-parallel-models/>Parallel Models</a></li><li><a href=/notes/cs149/2022-10-11-work-distribution-and-scheduling/>Work Distribution and Scheduling</a></li><li><a href=/notes/cs149/2022-10-13-locality-communication-and-contention/>Locality Communication and Contention</a></li><li><a href=/notes/cs149/2022-10-18-gpu-architecture-and-cuda/ class=active>Gpu Architecture and Cuda</a></li><li><a href=/notes/cs149/2022-10-20-data-parallel-architecture/>Data Parallel Architecture</a></li><li><a href=/notes/cs149/2022-10-25-spark/>Spark</a></li><li><a href=/notes/cs149/2022-10-27-cache-coherence/>Cache Coherence</a></li><li><a href=/notes/cs149/2022-11-01-memory-consistency/>Memory Consistency</a></li><li><a href=/notes/cs149/2022-11-03-lock-implementation-and-lock-free-programming/>Lock Implementation and Lock Free Programming</a></li><li><a href=/notes/cs149/2022-11-10-transactional-memory/>Transactional Memory</a></li><li><a href=/notes/cs149/2022-11-29-heterogeneous-processing-and-domain-specific-languages/>Heterogeneous Processing and Domain Specific Languages</a></li><li><a href=/notes/cs149/2022-12-06-asics-and-fpgas/>Asics and Fpgas</a></li></ul></li><li><input type=checkbox id=section-4f9fa520660f975442d4640415905b3c class=toggle>
<label for=section-4f9fa520660f975442d4640415905b3c class="flex justify-between"><a role=button>CS 154, Fall 2021</a></label><ul><li><a href=/notes/cs154/2021-09-28-finite-automata/>Finite Automata</a></li><li><a href=/notes/cs154/2021-10-05-pumping-lemma-and-myhill-nerode/>Pumping Lemma and Myhill Nerode</a></li><li><a href=/notes/cs154/2021-10-12-streaming-algorithms-and-turing-machines/>Streaming Algorithms and Turing Machines</a></li></ul></li><li><input type=checkbox id=section-d24cd7d7d21fe73135d596a09b50887f class=toggle>
<label for=section-d24cd7d7d21fe73135d596a09b50887f class="flex justify-between"><a role=button>CS 155, Spring 2022</a></label><ul><li><a href=/notes/cs155/2022-03-28-intro/>Intro</a></li><li><a href=/notes/cs155/2022-03-30-control-hijacking/>Control Hijacking</a></li><li><a href=/notes/cs155/2022-04-04-control-hijacking-defenses/>Control Hijacking Defenses</a></li><li><a href=/notes/cs155/2022-04-06-security-principles/>Security Principles</a></li><li><a href=/notes/cs155/2022-04-11-isolation-and-sandboxing/>Isolation and Sandboxing</a></li><li><a href=/notes/cs155/2022-04-13-vuln-finding/>Vuln Finding</a></li><li><a href=/notes/cs155/2022-04-18-web-security/>Web Security</a></li><li><a href=/notes/cs155/2022-04-20-web-attacks/>Web Attacks</a></li><li><a href=/notes/cs155/2022-04-25-web-defenses/>Web Defenses</a></li><li><a href=/notes/cs155/2022-05-04-processor-security/>Processor Security</a></li><li><a href=/notes/cs155/2022-05-09-internet-protocol-security/>Internet Protocol Security</a></li></ul></li><li><input type=checkbox id=section-471622039e8d87cd8d642483c8d218b8 class=toggle>
<label for=section-471622039e8d87cd8d642483c8d218b8 class="flex justify-between"><a role=button>CS 161, Winter 2022</a></label><ul><li><a href=/notes/cs161/2022-01-03-intro/>Intro</a></li><li><a href=/notes/cs161/2022-01-05-worst-case-and-asymptotic-analysis/>Worst Case and Asymptotic Analysis</a></li><li><a href=/notes/cs161/2022-01-10-recurrence-relations/>Recurrence Relations</a></li><li><a href=/notes/cs161/2022-01-12-median-and-selection/>Median and Selection</a></li><li><a href=/notes/cs161/2022-01-19-randomized-algorithms-and-quicksort/>Randomized Algorithms and Quicksort</a></li><li><a href=/notes/cs161/2022-01-24-sorting-lower-bounds/>Sorting Lower Bounds</a></li><li><a href=/notes/cs161/2022-01-26-binary-search-trees/>Binary Search Trees</a></li><li><a href=/notes/cs161/2022-01-31-hashing/>Hashing</a></li><li><a href=/notes/cs161/2022-02-02-graphs-and-graph-search/>Graphs and Graph Search</a></li><li><a href=/notes/cs161/2022-02-07-strongly-connected-components/>Strongly Connected Components</a></li><li><a href=/notes/cs161/2022-02-09-weighted-graphs-and-dijkstra/>Weighted Graphs and Dijkstra</a></li><li><a href=/notes/cs161/2022-02-14-dynamic-programming/>Dynamic Programming</a></li><li><a href=/notes/cs161/2022-02-16-dynamic-programming-applications/>Dynamic Programming Applications</a></li><li><a href=/notes/cs161/2022-02-23-greedy-algorithms/>Greedy Algorithms</a></li><li><a href=/notes/cs161/2022-02-28-minimum-spanning-trees/>Minimum Spanning Trees</a></li></ul></li><li><input type=checkbox id=section-0794dc87987599843ba69a8ea82e9b6a class=toggle>
<label for=section-0794dc87987599843ba69a8ea82e9b6a class="flex justify-between"><a role=button>CS 224U, Spring 2021</a></label><ul><li><a href=/notes/cs224u/2021-03-29-course-overview/>Course Overview</a></li><li><a href=/notes/cs224u/2021-03-31-vector-space-models/>Vector Space Models</a></li><li><a href=/notes/cs224u/2021-04-12-sentiment-analysis/>Sentiment Analysis</a></li></ul></li><li><input type=checkbox id=section-efae8264f5fe410feb1b40be6f99baea class=toggle>
<label for=section-efae8264f5fe410feb1b40be6f99baea class="flex justify-between"><a role=button>CS 229, Fall 2021</a></label><ul><li><a href=/notes/cs229/2021-09-21-intro/>Intro</a></li><li><a href=/notes/cs229/2021-09-23-supervised-learning-setup/>Supervised Learning Setup</a></li><li><a href=/notes/cs229/2021-09-28-logistic-regression/>Logistic Regression</a></li><li><a href=/notes/cs229/2021-09-30-generalized-linear-models/>Generalized Linear Models</a></li><li><a href=/notes/cs229/2021-10-05-generative-learning-algorithms/>Generative Learning Algorithms</a></li><li><a href=/notes/cs229/2021-10-07-naive-bayes/>Naive Bayes</a></li><li><a href=/notes/cs229/2021-10-12-kernel-methods-and-svm/>Kernel Methods and Svm</a></li><li><a href=/notes/cs229/2021-10-14-deep-learning/>Deep Learning</a></li><li><a href=/notes/cs229/2021-10-19-deep-learning-optimization/>Deep Learning Optimization</a></li><li><a href=/notes/cs229/2021-10-21-model-selection/>Model Selection</a></li></ul></li><li><input type=checkbox id=section-58493bdbfe80396f07f44c40ecf068c8 class=toggle>
<label for=section-58493bdbfe80396f07f44c40ecf068c8 class="flex justify-between"><a role=button>CS 249I, Winter 2023</a></label><ul><li><a href=/notes/cs249i/2023-01-09-internet-players/>Internet Players</a></li><li><a href=/notes/cs249i/2023-01-18-modern-routing-practices/>Modern Routing Practices</a></li><li><a href=/notes/cs249i/2023-01-23-last-mile-access/>Last Mile Access</a></li><li><a href=/notes/cs249i/2023-01-25-host-and-network-addressing/>Host and Network Addressing</a></li><li><a href=/notes/cs249i/2023-01-30-domain-name-system/>Domain Name System</a></li><li><a href=/notes/cs249i/2023-02-06-internet-governance/>Internet Governance</a></li><li><a href=/notes/cs249i/2023-02-08-modern-web-protocols/>Modern Web Protocols</a></li><li><a href=/notes/cs249i/2023-02-13-tls-and-webpki/>Tls and Webpki</a></li><li><a href=/notes/cs249i/2023-02-13-web-content/>Web Content</a></li><li><a href=/notes/cs249i/2023-02-22-internet-crime/>Internet Crime</a></li><li><a href=/notes/cs249i/2023-02-27-middleboxes-and-nat-and-https-interception/>Middleboxes and Nat and Https Interception</a></li><li><a href=/notes/cs249i/2023-03-01-modern-cryptography/>Modern Cryptography</a></li></ul></li><li><input type=checkbox id=section-292ea4e126876f1048e61c4dc744546c class=toggle>
<label for=section-292ea4e126876f1048e61c4dc744546c class="flex justify-between"><a role=button>CS 251, Fall 2022</a></label><ul><li><a href=/notes/cs251/2022-09-26-intro/>Intro</a></li><li><a href=/notes/cs251/2022-09-28-bitcoin-mechanics/>Bitcoin Mechanics</a></li><li><a href=/notes/cs251/2022-10-03-bitcoin-scripts-and-wallets/>Bitcoin Scripts and Wallets</a></li><li><a href=/notes/cs251/2022-10-05-consensus/>Consensus</a></li><li><a href=/notes/cs251/2022-10-10-internet-consensus/>Internet Consensus</a></li><li><a href=/notes/cs251/2022-10-17-ethereum/>Ethereum</a></li><li><a href=/notes/cs251/2022-10-19-solidity/>Solidity</a></li><li><a href=/notes/cs251/2022-11-02-legal-aspects-and-regulation/>Legal Aspects and Regulation</a></li><li><a href=/notes/cs251/2022-11-07-privacy-and-deanonymization-and-mixing/>Privacy and Deanonymization and Mixing</a></li><li><a href=/notes/cs251/2022-11-09-privacy-via-zk-snarks/>Privacy via Zk Snarks</a></li></ul></li><li><input type=checkbox id=section-98d861db134658b3e020a9d74c50ad24 class=toggle>
<label for=section-98d861db134658b3e020a9d74c50ad24 class="flex justify-between"><a role=button>CS 255, Winter 2022</a></label><ul><li><a href=/notes/cs255/2022-01-03-intro/>Intro</a></li><li><a href=/notes/cs255/2022-01-05-stream-ciphers/>Stream Ciphers</a></li><li><a href=/notes/cs255/2022-01-10-block-ciphers/>Block Ciphers</a></li><li><a href=/notes/cs255/2022-01-12-pseudorandom-functions/>Pseudorandom Functions</a></li><li><a href=/notes/cs255/2022-01-19-data-integrity-and-macs/>Data Integrity and Macs</a></li><li><a href=/notes/cs255/2022-01-24-collision-resistance/>Collision Resistance</a></li><li><a href=/notes/cs255/2022-01-26-authenticated-encryption/>Authenticated Encryption</a></li><li><a href=/notes/cs255/2022-01-31-key-management/>Key Management</a></li><li><a href=/notes/cs255/2022-02-02-key-exchange-math/>Key Exchange Math</a></li><li><a href=/notes/cs255/2022-02-07-public-key-encryption/>Public Key Encryption</a></li><li><a href=/notes/cs255/2022-02-09-pke-schemes/>Pke Schemes</a></li><li><a href=/notes/cs255/2022-02-14-digital-signatures/>Digital Signatures</a></li><li><a href=/notes/cs255/2022-02-16-certificates/>Certificates</a></li><li><a href=/notes/cs255/2022-02-23-id-protocols/>Id Protocols</a></li><li><a href=/notes/cs255/2022-02-28-key-exchange-protocols/>Key Exchange Protocols</a></li><li><a href=/notes/cs255/2022-03-02-zero-knowledge-protocols/>Zero Knowledge Protocols</a></li><li><a href=/notes/cs255/2022-03-07-quantum-cryptography/>Quantum Cryptography</a></li></ul></li><li><input type=checkbox id=section-3c4550884ed49f470e55fabac7077d16 class=toggle>
<label for=section-3c4550884ed49f470e55fabac7077d16 class="flex justify-between"><a role=button>Cs153</a></label><ul><li><a href=/notes/cs153/2023-01-12-intro/>Intro</a></li></ul></li><li><input type=checkbox id=section-568ed1bc8289796d56a6d06a22d7eb40 class=toggle>
<label for=section-568ed1bc8289796d56a6d06a22d7eb40 class="flex justify-between"><a role=button>INTLPOL 268, Fall 2021</a></label><ul><li><a href=/notes/intlpol268/2021-09-20-intro/>Intro</a></li><li><a href=/notes/intlpol268/2021-09-22-legal-intro-and-electronic-communications-privacy-act/>Legal Intro and Electronic Communications Privacy Act</a></li><li><a href=/notes/intlpol268/2021-09-27-web-requests-and-attacks/>Web Requests and Attacks</a></li><li><a href=/notes/intlpol268/2021-09-29-ecpa-for-private-actors/>Ecpa for Private Actors</a></li><li><a href=/notes/intlpol268/2021-10-04-cyberattacks/>Cyberattacks</a></li><li><a href=/notes/intlpol268/2021-10-06-computer-fraud-and-abuse-act/>Computer Fraud and Abuse Act</a></li><li><a href=/notes/intlpol268/2021-10-11-network-security/>Network Security</a></li><li><a href=/notes/intlpol268/2021-10-13-cfaa-dmca-and-security-research/>Cfaa Dmca and Security Research</a></li><li><a href=/notes/intlpol268/2021-10-20-data-security-laws/>Data Security Laws</a></li><li><a href=/notes/intlpol268/2021-10-25-corporate-intrusion/>Corporate Intrusion</a></li><li><a href=/notes/intlpol268/2021-10-27-ransomware-and-foreign-hackers/>Ransomware and Foreign Hackers</a></li><li><a href=/notes/intlpol268/2021-11-01-cryptography/>Cryptography</a></li><li><a href=/notes/intlpol268/2021-11-03-cyber-conflict/>Cyber Conflict</a></li><li><a href=/notes/intlpol268/2021-11-08-dark-web-and-cryptocurrencies/>Dark Web and Cryptocurrencies</a></li><li><a href=/notes/intlpol268/2021-11-10-encryption-and-technical-assistance/>Encryption and Technical Assistance</a></li><li><a href=/notes/intlpol268/2021-11-15-malware/>Malware</a></li><li><a href=/notes/intlpol268/2021-11-17-government-hacking/>Government Hacking</a></li><li><a href=/notes/intlpol268/2021-11-29-new-frontiers/>New Frontiers</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notes/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Gpu Architecture and Cuda</strong>
<label for=toc-control><img src=/notes/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#gpu-architecture-and-cuda>GPU architecture and CUDA</a><ul><li><a href=#gpu-history>GPU history</a><ul><li><a href=#graphical-purpose-of-a-gpu>Graphical purpose of a GPU</a></li><li><a href=#rendering-task>Rendering task</a></li><li><a href=#gpus-for-scientific-and-compute-task>GPUs for scientific and compute task</a></li></ul></li><li><a href=#cuda-modern-gpu-compute>CUDA: Modern GPU compute</a><ul><li><a href=#terminology>Terminology</a></li><li><a href=#cuda-progams-and-syntax>CUDA progams and syntax</a></li><li><a href=#cuda-memory-model>CUDA memory model</a></li><li><a href=#cuda-device-memory-model>CUDA device memory model</a></li><li><a href=#cuda-example-1d-convolution>CUDA example: 1D Convolution</a></li><li><a href=#cuda-synchronization-constructs>CUDA synchronization constructs</a></li><li><a href=#assigning-work>Assigning work</a></li></ul></li><li><a href=#gpu-architecture-and-threading>GPU architecture and threading</a><ul><li><a href=#sub-core>Sub-core</a></li><li><a href=#streaming-multiprocessor-sm>Streaming multiprocessor (SM)</a></li></ul></li><li><a href=#running-a-cuda-program-on-a-gpu>Running a CUDA program on a GPU</a><ul><li><a href=#execution-steps>Execution steps</a></li></ul></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=gpu-architecture-and-cuda>GPU architecture and CUDA
<a class=anchor href=#gpu-architecture-and-cuda>#</a></h1><h2 id=gpu-history>GPU history
<a class=anchor href=#gpu-history>#</a></h2><h3 id=graphical-purpose-of-a-gpu>Graphical purpose of a GPU
<a class=anchor href=#graphical-purpose-of-a-gpu>#</a></h3><ul><li>Initially designed for:<ul><li>Input: description of a scene; mathematical description<ul><li>e.g. 3D surface geometry, surface materials, lights, camera, etc.</li></ul></li><li>Output: image of the scene</li></ul></li></ul><h3 id=rendering-task>Rendering task
<a class=anchor href=#rendering-task>#</a></h3><ul><li>Real-time graphics primitives (entities)<ul><li>Surfaces represented as 3D triangle meshes: vertices (points in space), primitives (points, lines, triangles)</li></ul></li><li>Goal: compute how each triangle in 3D mesh contributes to overall image<ul><li>Subtask workload: given triangle, determine where it lies on screen given position of virtual cameras</li><li>For all output image pixels covered by triangle, compute color of surface at that pixel</li></ul></li><li>Shader program: run once per fragment (per pixel covered by triangle)<ul><li>Inputs: variable values that change per pixel</li><li>Outputs: colors at those pixels</li><li>Pixels covered by multiple surfaces contain output from surfaces closest to camera</li></ul></li><li>To optimize: GPUs designed with multiple core, high-throughput (lots of SIMD and multithreading) architecture</li></ul><h3 id=gpus-for-scientific-and-compute-task>GPUs for scientific and compute task
<a class=anchor href=#gpus-for-scientific-and-compute-task>#</a></h3><ul><li>Initial observation (2001-2003): GPUs are <em>very fast</em> processors for performing same computation in parallel on large collections of data<ul><li>Data parallelism!</li><li>Packing more transistors on the same chip = more parallelism</li></ul></li><li>Lead to early GPU-based scientific computation: hack<ul><li>Map 512x512 array onto on &ldquo;image&rdquo;</li><li>Render two triangles that exactly cover screen</li><li>Apply &ldquo;shader&rdquo; computation on the collection</li></ul></li><li>Brook Stream programming language (Stanford, 2004): abstracted GPU hardware as a data-parallel processor<ul><li>Translated generic stream program into graphics commands that could be run on GPUs</li><li>However: programs limited to graphics-specific APIs; needed to set image sizes, vertices, etc. and use &ldquo;drawing&rdquo; abstractions for computation</li></ul></li></ul><h2 id=cuda-modern-gpu-compute>CUDA: Modern GPU compute
<a class=anchor href=#cuda-modern-gpu-compute>#</a></h2><ul><li>NVIDIA Tesla architecture (2007)<ul><li>First alternative, &ldquo;compute mode&rdquo; interface to GPU hardware</li><li>Application can allocate buffers in GPU memory and copy data to/from buffers</li><li>Application (via graphics driver) provides GPU a single kernel program binary</li><li>Application tells GPU to run kernel in SPMD fashion (i.e., run N instances of the kernel): <code>launch(myKernel, N)</code></li></ul></li></ul><h3 id=terminology>Terminology
<a class=anchor href=#terminology>#</a></h3><ul><li>CUDA: the abstraction above; &ldquo;C-like&rdquo; language to express GPU programs using compute hardware interface<ul><li>Now subset of C++</li><li>Low-level; abstractions match capabilities/perf. characteristics of modern GPUs</li></ul></li><li><strong>Note on CUDA thread</strong>: similar API abstraction as <code>pthread</code> corresponding to logical thread of control<ul><li>However: <strong>very different implementation!!</strong></li></ul></li></ul><h3 id=cuda-progams-and-syntax>CUDA progams and syntax
<a class=anchor href=#cuda-progams-and-syntax>#</a></h3><ul><li>Hierarchy of concurrent threads<ul><li>Thread IDs can be up to 3-dimensional; abstraction useful for naturally N-dimensional programs</li></ul></li><li>Example: <code>matrixAdd</code><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++><span style=display:flex><span><span style=color:#6272a4>// begin host code: C/C++ on CPU
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span><span style=color:#ff79c6>const</span> <span style=color:#8be9fd>int</span> Nx <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>12</span>;
</span></span><span style=display:flex><span><span style=color:#ff79c6>const</span> <span style=color:#8be9fd>int</span> Ny <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>6</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dim3 <span style=color:#50fa7b>threadsPerBlock</span>(<span style=color:#bd93f9>4</span>, <span style=color:#bd93f9>3</span>);
</span></span><span style=display:flex><span>dim3 <span style=color:#50fa7b>numBlocks</span>(Nx<span style=color:#ff79c6>/</span>threadsPerBlock.x, Ny<span style=color:#ff79c6>/</span>threadsPerBlock.y);
</span></span><span style=display:flex><span><span style=color:#6272a4>// end host code
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span><span style=color:#6272a4>// assume A, B, C **on GPU** allocated as Nx x Ny float arrays
</span></span></span><span style=display:flex><span><span style=color:#6272a4>// call launches 72 CUDA threads: 6 thread blocks of 12 threads each
</span></span></span><span style=display:flex><span><span style=color:#6272a4>// &#34;launch a grid of CUDA thread blocks, return when all threads are terminated&#34;
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>matrixAdd<span style=color:#ff79c6>&lt;&lt;&lt;</span>numBlocks, threadsPerBlock<span style=color:#ff79c6>&gt;&gt;&gt;</span>(A, B, C)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4>// kernel definition: runs on GPU
</span></span></span><span style=display:flex><span><span style=color:#6272a4>// __global__ denotes CUDA kernel function for device
</span></span></span><span style=display:flex><span><span style=color:#6272a4>// A, B, C are pointers/references to GPU memory
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>__global__ <span style=color:#8be9fd>void</span> matrixAdd(<span style=color:#8be9fd>float</span> A[Ny][Nx], <span style=color:#8be9fd>float</span> B[Ny][Nx], <span style=color:#8be9fd>float</span> C[Ny, Nx]) {
</span></span><span style=display:flex><span>    <span style=color:#6272a4>// each thread computes overall grid thread id from position in block (threadIdx) and block position in grid (blockIdx)
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    <span style=color:#8be9fd>int</span> i <span style=color:#ff79c6>=</span> blockIdx.x <span style=color:#ff79c6>*</span> blockDim.x <span style=color:#ff79c6>+</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>int</span> j <span style=color:#ff79c6>=</span> blockIdx.y <span style=color:#ff79c6>*</span> blockDim.y <span style=color:#ff79c6>+</span> threadIdx.y;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    C[j][i] <span style=color:#ff79c6>=</span> A[j][i] <span style=color:#ff79c6>+</span> B[j][i];
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ul><li>Note: number of SPMD &ldquo;CUDA threads&rdquo; is explicit in the program<ul><li>Number of kernel invocations not determined by size of data collection</li><li>Kernel launch not specified by <code>map(kernel, collection)</code> like with graphics shader processing</li></ul></li></ul></li><li>Compiled CUDA device binary includes program text (instructions), and information about required resources<ul><li>Threads per block</li><li>Bytes of local data per thread</li><li>Shared space per thread block</li></ul></li></ul><h3 id=cuda-memory-model>CUDA memory model
<a class=anchor href=#cuda-memory-model>#</a></h3><ul><li>CPUs and GPUs have different memory address spaces!</li><li>To reconcile: <code>memcpy</code> primitive, like message-passing setup<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++><span style=display:flex><span><span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>A <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>new</span> <span style=color:#8be9fd>float</span>[N]; <span style=color:#6272a4>// allocate buffer in host CPU memory
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span><span style=color:#ff79c6>for</span> (<span style=color:#8be9fd>int</span> i <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>; i <span style=color:#ff79c6>&lt;</span> N; i<span style=color:#ff79c6>++</span>) <span style=color:#6272a4>// fill the buffer in CPU host space
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    A[i] <span style=color:#ff79c6>=</span> (<span style=color:#8be9fd>float</span>) i;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4>// allocate buffer in device address space
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span><span style=color:#8be9fd>int</span> bytes <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>sizeof</span>(<span style=color:#8be9fd>float</span>) <span style=color:#ff79c6>*</span> N;
</span></span><span style=display:flex><span><span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>deviceA;
</span></span><span style=display:flex><span>cudaMalloc(<span style=color:#ff79c6>&amp;</span>deviceA, bytes);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4>// populate deviceA
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>cudaMemcpy(deviceA, A, bytes, cudaMemcpyHostToDevice);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#6272a4>// note: cannot manipulate deviceA[i] from host (invalid operation), due to different address spaces
</span></span></span></code></pre></div></li></ul><h3 id=cuda-device-memory-model>CUDA device memory model
<a class=anchor href=#cuda-device-memory-model>#</a></h3><ul><li>Three distinct address spaces visible to kernels:<ul><li>Per-block shared memory: r/w by all threads in block</li><li>Per-thread private memory: r/w by thread</li><li>Device global memory: r/w by all threads</li></ul></li><li>Address spaces represent different regions of locality<ul><li>Has implications on CUDA implementation efficiency</li><li>e.g. consider scheduling of threads based on prior knowledge of which threads access the same variables</li></ul></li></ul><h3 id=cuda-example-1d-convolution>CUDA example: 1D Convolution
<a class=anchor href=#cuda-example-1d-convolution>#</a></h3><ul><li>Convolution: each output is an average of the inputs surrounding it<ul><li><code>output[i] = (input[i] + input[i+1] + input[i+2]) / 3.f</code></li></ul></li><li>CUDA, version 1: one thread per output element<ul><li>CUDA kernel:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#ff79c6>#define THREADS_PER_BLK 128
</span></span></span><span style=display:flex><span><span style=color:#ff79c6></span>
</span></span><span style=display:flex><span>__global__ <span style=color:#8be9fd>void</span> <span style=color:#50fa7b>convolve</span>(<span style=color:#8be9fd>int</span> N, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>input, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>output) {
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>int</span> index <span style=color:#ff79c6>=</span> blockIdx.x <span style=color:#ff79c6>*</span> blockDim.x <span style=color:#ff79c6>+</span> threadIdx.x; <span style=color:#6272a4>// thread-local variable
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>float</span> result <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.0f</span>; <span style=color:#6272a4>// thread-local variable
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    <span style=color:#ff79c6>for</span> (<span style=color:#8be9fd>int</span> i <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>; i <span style=color:#ff79c6>&lt;</span> <span style=color:#bd93f9>3</span>; i<span style=color:#ff79c6>++</span>)
</span></span><span style=display:flex><span>        result <span style=color:#ff79c6>+=</span> input[index <span style=color:#ff79c6>+</span> i]; <span style=color:#6272a4>// thread computes result for one element
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>    output[index] <span style=color:#ff79c6>=</span> result <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>3.f</span>; <span style=color:#6272a4>// write result to global memory
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>}
</span></span></code></pre></div></li><li>Host code:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++><span style=display:flex><span><span style=color:#8be9fd>int</span> N <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span>;
</span></span><span style=display:flex><span>cudaMalloc(<span style=color:#ff79c6>&amp;</span>devInput, <span style=color:#ff79c6>sizeof</span>(<span style=color:#8be9fd>float</span>) <span style=color:#ff79c6>*</span> (N<span style=color:#ff79c6>+</span><span style=color:#bd93f9>2</span>)); <span style=color:#6272a4>// allocate in dev mem
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>cudaMalloc(<span style=color:#ff79c6>&amp;</span>devOutput, <span style=color:#ff79c6>sizeof</span>(<span style=color:#8be9fd>float</span>) <span style=color:#ff79c6>*</span> N); <span style=color:#6272a4>// allocate in dev mem
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span><span style=color:#6272a4>// omitted: initialize arrays here
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>convolve<span style=color:#ff79c6>&lt;&lt;&lt;</span>N<span style=color:#ff79c6>/</span>THREADS_PER_BLK, THREADS_PER_BLK<span style=color:#ff79c6>&gt;&gt;&gt;</span>(N, devInput, devOutput);
</span></span></code></pre></div></li><li>Lots of overhead from threads only computing result for one element (more loads per thread than necessary)</li></ul></li><li>CUDA, version 2: one thread per output element: stage input data in per-block shared memory<ul><li>CUDA kernel:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#ff79c6>#define THREADS_PER_BLK 128
</span></span></span><span style=display:flex><span><span style=color:#ff79c6></span>
</span></span><span style=display:flex><span>__global__ <span style=color:#8be9fd>void</span> <span style=color:#50fa7b>convolve</span>(<span style=color:#8be9fd>int</span> N, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>input, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>output) {
</span></span><span style=display:flex><span>    __shared__ <span style=color:#8be9fd>float</span> support[THREADS_PER_BLK<span style=color:#ff79c6>+</span><span style=color:#bd93f9>2</span>]; <span style=color:#6272a4>// per-block allocation
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    <span style=color:#8be9fd>int</span> index <span style=color:#ff79c6>=</span> blockIdx.x <span style=color:#ff79c6>*</span> blockDim.x <span style=color:#ff79c6>+</span> threadIdx.x; <span style=color:#6272a4>// thread-local variable
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>// all threads cooperatively load block&#39;s support region from global mem to shared mem: 130 load instructions vs 3*128
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    support[threadIdx.x] <span style=color:#ff79c6>=</span> input[index];
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>if</span> (threadIdx.x <span style=color:#ff79c6>&lt;</span> <span style=color:#bd93f9>2</span>) {
</span></span><span style=display:flex><span>        support[THREADS_PER_BLK <span style=color:#ff79c6>+</span> threadIdx.x] <span style=color:#ff79c6>=</span> input[index <span style=color:#ff79c6>+</span> THREADS_PER_BLK];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __syncthreads(); <span style=color:#6272a4>// barrier: all threads in block
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>float</span> result <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0.0f</span>; <span style=color:#6272a4>// thread-local variable
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    <span style=color:#ff79c6>for</span> (<span style=color:#8be9fd>int</span> i <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>; i <span style=color:#ff79c6>&lt;</span> <span style=color:#bd93f9>3</span>; i<span style=color:#ff79c6>++</span>)
</span></span><span style=display:flex><span>        result <span style=color:#ff79c6>+=</span> support[threadIdx.x <span style=color:#ff79c6>+</span> i]; <span style=color:#6272a4>// thread computes result for one element
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>    output[index] <span style=color:#ff79c6>=</span> result <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>3.f</span>; <span style=color:#6272a4>// write result to global memory
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>}
</span></span></code></pre></div></li><li>Host code:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++><span style=display:flex><span><span style=color:#8be9fd>int</span> N <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>1024</span>;
</span></span><span style=display:flex><span>cudaMalloc(<span style=color:#ff79c6>&amp;</span>devInput, <span style=color:#ff79c6>sizeof</span>(<span style=color:#8be9fd>float</span>) <span style=color:#ff79c6>*</span> (N<span style=color:#ff79c6>+</span><span style=color:#bd93f9>2</span>)); <span style=color:#6272a4>// allocate in dev mem
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>cudaMalloc(<span style=color:#ff79c6>&amp;</span>devOutput, <span style=color:#ff79c6>sizeof</span>(<span style=color:#8be9fd>float</span>) <span style=color:#ff79c6>*</span> N); <span style=color:#6272a4>// allocate in dev mem
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span><span style=color:#6272a4>// omitted: initialize arrays here
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>convolve<span style=color:#ff79c6>&lt;&lt;&lt;</span>N<span style=color:#ff79c6>/</span>THREADS_PER_BLK, THREADS_PER_BLK<span style=color:#ff79c6>&gt;&gt;&gt;</span>(N, devInput, devOutput);
</span></span></code></pre></div></li><li>Faster, due to less load operations per thread</li></ul></li></ul><h3 id=cuda-synchronization-constructs>CUDA synchronization constructs
<a class=anchor href=#cuda-synchronization-constructs>#</a></h3><ul><li><code>__syncthreads()</code>: barrier, wait for all threads in the block to arrive at this point</li><li>Atomic operations<ul><li>e.g. <code>float atomicAdd(float *addr, float amount)</code></li><li>Provided on both global and per-block shared mem addrs</li></ul></li><li>Host/device synchronization: implicit barrier across threads at kernel return</li></ul><h3 id=assigning-work>Assigning work
<a class=anchor href=#assigning-work>#</a></h3><ul><li>Desirable for CUDA program to run on any size of GPU without modification</li><li>Thread-block assignment:<ul><li>Assumption: thread block execution can be carried out in any order w/o dependencies between blocks</li><li>GPU implementation maps thread blocks (&ldquo;work&rdquo;) to cores using dynamic scheduling policy respecting resource requirements</li><li>Just like thread-pool model!</li></ul></li></ul><h2 id=gpu-architecture-and-threading>GPU architecture and threading
<a class=anchor href=#gpu-architecture-and-threading>#</a></h2><h3 id=sub-core>Sub-core
<a class=anchor href=#sub-core>#</a></h3><ul><li>A group of 32 threads in a thread block is called a <em>warp</em><ul><li>Thread IDs numbered consecutively: warp with 0-31, warp with 32-63, etc.</li><li>A block with 256 CUDA threads is mapped 8 warps</li><li>Each sub-core can schedule and interleave execution of up to 16 warps (NVIDIA V100, 2017)</li></ul></li><li>Threads in a warp executed SIMD if they share the same instruction<ul><li>Otherwise, performance suffers due to divergent execution</li></ul></li></ul><h3 id=streaming-multiprocessor-sm>Streaming multiprocessor (SM)
<a class=anchor href=#streaming-multiprocessor-sm>#</a></h3><ul><li>SM architecture each clock:<ul><li>Each sub-core selects one runnable warp (from 16 warps in partition)</li><li>Each sub-core runs next instruction for CUDA threads in warp<ul><li>May apply to all or subset of CUDA threads in warp due to divergence</li></ul></li></ul></li><li>NVIDIA V100 (2017) has 80 SMs<ul><li>Overall architecture: 1.245 GHz per clock, 80 SM cores per chip: 5120 fp32 mul-add ALUs = 12.7 TFLOPs</li><li>Up to 5120 interleaved warps per chip (163,840 CUDA threads/chip)</li></ul></li></ul><h2 id=running-a-cuda-program-on-a-gpu>Running a CUDA program on a GPU
<a class=anchor href=#running-a-cuda-program-on-a-gpu>#</a></h2><ul><li>Running a CUDA kernel has execution requirements, e.g. for <code>convolve</code>:<ul><li>Each thread block must execute 128 CUDA threads</li><li>Each thread block must alloc <code>130 * sizeof(float)</code> bytes of shared mem.</li><li>Assume <code>N</code> very large, so host-side kernel launch generates thousands of thread blocks</li><li>Assume fictituous two-core GPU</li></ul></li></ul><h3 id=execution-steps>Execution steps
<a class=anchor href=#execution-steps>#</a></h3><ol><li>Host sends CUDA device a command: execute kernel<pre tabindex=0><code>EXECUTE: convolve
ARGS: N, input_array, output_array
NUM_BLOCKS: 1000
</code></pre></li><li>Scheduler maps block 0 to core 0: reserves execution contexts for required resources<pre tabindex=0><code>NEXT = 1
TOTAL = 1000
</code></pre></li><li>Scheduler continues blocks to available execution contexts in interleaved fashion<pre tabindex=0><code>NEXT = 2
TOTAL = 1000
</code></pre></li><li>If next thread block won&rsquo;t fit on a core (e.g. due to insufficient storage): wait for a block to finish, then schedule the next block on that core</li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#gpu-architecture-and-cuda>GPU architecture and CUDA</a><ul><li><a href=#gpu-history>GPU history</a><ul><li><a href=#graphical-purpose-of-a-gpu>Graphical purpose of a GPU</a></li><li><a href=#rendering-task>Rendering task</a></li><li><a href=#gpus-for-scientific-and-compute-task>GPUs for scientific and compute task</a></li></ul></li><li><a href=#cuda-modern-gpu-compute>CUDA: Modern GPU compute</a><ul><li><a href=#terminology>Terminology</a></li><li><a href=#cuda-progams-and-syntax>CUDA progams and syntax</a></li><li><a href=#cuda-memory-model>CUDA memory model</a></li><li><a href=#cuda-device-memory-model>CUDA device memory model</a></li><li><a href=#cuda-example-1d-convolution>CUDA example: 1D Convolution</a></li><li><a href=#cuda-synchronization-constructs>CUDA synchronization constructs</a></li><li><a href=#assigning-work>Assigning work</a></li></ul></li><li><a href=#gpu-architecture-and-threading>GPU architecture and threading</a><ul><li><a href=#sub-core>Sub-core</a></li><li><a href=#streaming-multiprocessor-sm>Streaming multiprocessor (SM)</a></li></ul></li><li><a href=#running-a-cuda-program-on-a-gpu>Running a CUDA program on a GPU</a><ul><li><a href=#execution-steps>Execution steps</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>