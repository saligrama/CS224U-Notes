<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A Modern Multi-core Processor # Part 1: Speeding up an example sine program # Computes sin across an array of float values using a Taylor series expansion
void sinx(int N, int terms, float *x, float *y) { for (int i = 0; i < N; i++) { float value = x[i]; float numer = x[i] * x[i] * x[i]; int denom = 6; // 3! int sign = -1; for (int j = 1; j <= terms; j++) { value += sign * numer / denom; numer *= x[i] * x[i]; denom *= (2 * j + 2) * (2 * j + 3); sign *= -1; } y[i] = value; } } This compiles to some assembly:"><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="A Modern Multi-core Processor # Part 1: Speeding up an example sine program # Computes sin across an array of float values using a Taylor series expansion
void sinx(int N, int terms, float *x, float *y) { for (int i = 0; i < N; i++) { float value = x[i]; float numer = x[i] * x[i] * x[i]; int denom = 6; // 3! int sign = -1; for (int j = 1; j <= terms; j++) { value += sign * numer / denom; numer *= x[i] * x[i]; denom *= (2 * j + 2) * (2 * j + 3); sign *= -1; } y[i] = value; } } This compiles to some assembly:"><meta property="og:type" content="article"><meta property="og:url" content="https://saligrama.io/notes/cs149/2022-09-29-modern-multicore-processors/"><meta property="article:section" content="cs149"><title>Modern Multicore Processors | Aditya's notes</title><link rel=manifest href=/notes/manifest.json><link rel=icon href=/notes/favicon.png type=image/x-icon><link rel=stylesheet href=/notes/book.min.395a67680f48b8d23bbf267f26d0d1259e69554b2b704e371e8e15cbe656e05f.css integrity="sha256-OVpnaA9IuNI7vyZ/JtDRJZ5pVUsrcE43Ho4Vy+ZW4F8=" crossorigin=anonymous><script defer src=/notes/flexsearch.min.js></script>
<script defer src=/notes/en.search.min.2d7fe3a768fb0a28e9426a480cd2cbbe18eefbdb19eedae57337133d12dce08c.js integrity="sha256-LX/jp2j7CijpQmpIDNLLvhju+9sZ7trlczcTPRLc4Iw=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/notes/><span>Aditya's notes</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><input type=checkbox id=section-a88f46c31d1ebdf165810583424fda37 class=toggle>
<label for=section-a88f46c31d1ebdf165810583424fda37 class="flex justify-between"><a role=button>CS 103, Fall 2020</a></label><ul><li><a href=/notes/cs103/2020-09-16-set-theory/>Set Theory</a></li><li><a href=/notes/cs103/2020-09-18-indirect-proofs/>Indirect Proofs</a></li><li><a href=/notes/cs103/2020-09-18-mathematical-proofs/>Mathematical Proofs</a></li><li><a href=/notes/cs103/2020-09-26-first-order-logic/>First Order Logic</a></li><li><a href=/notes/cs103/2020-09-26-propositional-logic/>Propositional Logic</a></li><li><a href=/notes/cs103/2020-09-27-first-order-logic-continued/>First Order Logic Continued</a></li><li><a href=/notes/cs103/2020-09-30-binary-relations/>Binary Relations</a></li><li><a href=/notes/cs103/2020-10-01-binary-relations-continued/>Binary Relations Continued</a></li><li><a href=/notes/cs103/2020-10-04-functions/>Functions</a></li><li><a href=/notes/cs103/2020-10-10-cardinality/>Cardinality</a></li><li><a href=/notes/cs103/2020-10-11-graph-theory/>Graph Theory</a></li><li><a href=/notes/cs103/2020-10-17-pigeonhole-principle/>Pigeonhole Principle</a></li><li><a href=/notes/cs103/2020-10-18-induction/>Induction</a></li><li><a href=/notes/cs103/2020-10-19-induction-variants/>Induction Variants</a></li><li><a href=/notes/cs103/2020-10-20-computability-and-formal-languages/>Computability and Formal Languages</a></li><li><a href=/notes/cs103/2020-10-25-nondeterministic-finite-automata/>Nondeterministic Finite Automata</a></li><li><a href=/notes/cs103/2020-10-26-nfa-dfa-equivalence/>Nfa Dfa Equivalence</a></li><li><a href=/notes/cs103/2020-10-26-regular-expressions/>Regular Expressions</a></li><li><a href=/notes/cs103/2020-11-01-nonregular-languages/>Nonregular Languages</a></li><li><a href=/notes/cs103/2020-11-02-context-free-grammars/>Context Free Grammars</a></li><li><a href=/notes/cs103/2020-11-03-turing-machines/>Turing Machines</a></li><li><a href=/notes/cs103/2020-11-08-turing-machine-subroutines/>Turing Machine Subroutines</a></li><li><a href=/notes/cs103/2020-11-08-universal-turing-machine/>Universal Turing Machine</a></li><li><a href=/notes/cs103/2020-11-08-unsolvable-problems/>Unsolvable Problems</a></li><li><a href=/notes/cs103/2020-11-14-unsolvable-problems-continued/>Unsolvable Problems Continued</a></li></ul></li><li><input type=checkbox id=section-cf13475a7a80a5dd57b5a55dce73d171 class=toggle>
<label for=section-cf13475a7a80a5dd57b5a55dce73d171 class="flex justify-between"><a role=button>CS 107, Fall 2020</a></label><ul><li><a href=/notes/cs107/2020-09-18-integer-representations/>Integer Representations</a></li><li><a href=/notes/cs107/2020-09-21-bitwise-operations/>Bitwise Operations</a></li><li><a href=/notes/cs107/2020-09-25-c-chars-and-strings/>C Chars and Strings</a></li><li><a href=/notes/cs107/2020-09-28-more-c-strings/>More C Strings</a></li><li><a href=/notes/cs107/2020-10-02-pointers-arrays/>Pointers Arrays</a></li><li><a href=/notes/cs107/2020-10-05-stack-and-heap/>Stack and Heap</a></li><li><a href=/notes/cs107/2020-10-09-c-generics/>C Generics</a></li><li><a href=/notes/cs107/2020-10-12-function-pointers/>Function Pointers</a></li><li><a href=/notes/cs107/2020-10-16-assembly/>Assembly</a></li><li><a href=/notes/cs107/2020-10-19-assembly-arithmetic-logic/>Assembly Arithmetic Logic</a></li><li><a href=/notes/cs107/2020-10-23-assembly-control-flow/>Assembly Control Flow</a></li><li><a href=/notes/cs107/2020-10-26-assembly-function-calls-and-return-stack/>Assembly Function Calls and Return Stack</a></li><li><a href=/notes/cs107/2020-10-30-heap-management/>Heap Management</a></li><li><a href=/notes/cs107/2020-11-09-program-optimization/>Program Optimization</a></li></ul></li><li><input type=checkbox id=section-4e3bc6e2f7a0feae33c3e43356d49c80 class=toggle>
<label for=section-4e3bc6e2f7a0feae33c3e43356d49c80 class="flex justify-between"><a role=button>CS 110L, Spring 2021</a></label><ul><li><a href=/notes/cs110l/2021-03-30-course-overview/>Course Overview</a></li><li><a href=/notes/cs110l/2021-04-01-fixing-c/>Fixing C</a></li><li><a href=/notes/cs110l/2021-04-06-intro-to-rust/>Intro to Rust</a></li><li><a href=/notes/cs110l/2021-04-08-ownership/>Ownership</a></li><li><a href=/notes/cs110l/2021-04-13-error-handling/>Error Handling</a></li><li><a href=/notes/cs110l/2021-04-22-traits/>Traits</a></li><li><a href=/notes/cs110l/2021-04-27-generics/>Generics</a></li><li><a href=/notes/cs110l/2021-04-29-multiprocessing/>Multiprocessing</a></li></ul></li><li><input type=checkbox id=section-23e7773af736437c02202412a988f2db class=toggle>
<label for=section-23e7773af736437c02202412a988f2db class="flex justify-between"><a role=button>CS 111, Spring 2021</a></label><ul><li><a href=/notes/cs111/2021-03-31-threads-and-dispatching/>Threads and Dispatching</a></li><li><a href=/notes/cs111/2021-04-02-concurrency/>Concurrency</a></li><li><a href=/notes/cs111/2021-04-05-synchronization/>Synchronization</a></li><li><a href=/notes/cs111/2021-04-07-shared-memory-and-condition-variables-and-locks/>Shared Memory and Condition Variables and Locks</a></li><li><a href=/notes/cs111/2021-04-09-lock-implementation-and-deadlocking/>Lock Implementation and Deadlocking</a></li><li><a href=/notes/cs111/2021-04-12-scheduling/>Scheduling</a></li><li><a href=/notes/cs111/2021-04-14-multiprocessing/>Multiprocessing</a></li><li><a href=/notes/cs111/2021-04-16-linking/>Linking</a></li><li><a href=/notes/cs111/2021-04-19-storage-management/>Storage Management</a></li><li><a href=/notes/cs111/2021-04-21-virtual-memory/>Virtual Memory</a></li><li><a href=/notes/cs111/2021-04-23-dynamic-address-translation/>Dynamic Address Translation</a></li><li><a href=/notes/cs111/2021-04-26-segmentation-and-paging/>Segmentation and Paging</a></li><li><a href=/notes/cs111/2021-04-30-demand-paging/>Demand Paging</a></li><li><a href=/notes/cs111/2021-05-05-disks/>Disks</a></li><li><a href=/notes/cs111/2021-05-07-file-systems/>File Systems</a></li><li><a href=/notes/cs111/2021-05-10-realworld-filesystem-structures/>Realworld Filesystem Structures</a></li><li><a href=/notes/cs111/2021-05-12-directories/>Directories</a></li><li><a href=/notes/cs111/2021-05-14-crash-recovery/>Crash Recovery</a></li><li><a href=/notes/cs111/2021-05-19-protection/>Protection</a></li><li><a href=/notes/cs111/2021-05-24-flash-memory/>Flash Memory</a></li><li><a href=/notes/cs111/2021-05-28-virtual-machines/>Virtual Machines</a></li></ul></li><li><input type=checkbox id=section-a586b190ebf0a4874cd21a1d76743261 class=toggle>
<label for=section-a586b190ebf0a4874cd21a1d76743261 class="flex justify-between"><a role=button>CS 143, Spring 2022</a></label><ul><li><a href=/notes/cs143/2022-03-29-intro/>Intro</a></li><li><a href=/notes/cs143/2022-03-31-language-design-and-cool/>Language Design and Cool</a></li><li><a href=/notes/cs143/2022-04-05-lexical-analysis/>Lexical Analysis</a></li><li><a href=/notes/cs143/2022-04-07-lexical-analysis-implementation/>Lexical Analysis Implementation</a></li><li><a href=/notes/cs143/2022-04-12-parsing/>Parsing</a></li><li><a href=/notes/cs143/2022-04-14-syntax-directed-translation/>Syntax Directed Translation</a></li><li><a href=/notes/cs143/2022-04-19-top-down-parsing/>Top Down Parsing</a></li><li><a href=/notes/cs143/2022-04-26-semantic-analysis/>Semantic Analysis</a></li></ul></li><li><input type=checkbox id=section-37836de7a703e433b2642097d511f482 class=toggle checked>
<label for=section-37836de7a703e433b2642097d511f482 class="flex justify-between"><a role=button>CS 149, Fall 2022</a></label><ul><li><a href=/notes/cs149/2022-09-27-intro/>Intro</a></li><li><a href=/notes/cs149/2022-09-29-modern-multicore-processors/ class=active>Modern Multicore Processors</a></li><li><a href=/notes/cs149/2022-10-04-parallel-abstractions/>Parallel Abstractions</a></li><li><a href=/notes/cs149/2022-10-06-parallel-models/>Parallel Models</a></li><li><a href=/notes/cs149/2022-10-11-work-distribution-and-scheduling/>Work Distribution and Scheduling</a></li><li><a href=/notes/cs149/2022-10-13-locality-communication-and-contention/>Locality Communication and Contention</a></li><li><a href=/notes/cs149/2022-10-17-gpu-architecture-and-cuda/>Gpu Architecture and Cuda</a></li></ul></li><li><input type=checkbox id=section-4f9fa520660f975442d4640415905b3c class=toggle>
<label for=section-4f9fa520660f975442d4640415905b3c class="flex justify-between"><a role=button>CS 154, Fall 2021</a></label><ul><li><a href=/notes/cs154/2021-09-28-finite-automata/>Finite Automata</a></li><li><a href=/notes/cs154/2021-10-05-pumping-lemma-and-myhill-nerode/>Pumping Lemma and Myhill Nerode</a></li><li><a href=/notes/cs154/2021-10-12-streaming-algorithms-and-turing-machines/>Streaming Algorithms and Turing Machines</a></li></ul></li><li><input type=checkbox id=section-d24cd7d7d21fe73135d596a09b50887f class=toggle>
<label for=section-d24cd7d7d21fe73135d596a09b50887f class="flex justify-between"><a role=button>CS 155, Spring 2022</a></label><ul><li><a href=/notes/cs155/2022-03-28-intro/>Intro</a></li><li><a href=/notes/cs155/2022-03-30-control-hijacking/>Control Hijacking</a></li><li><a href=/notes/cs155/2022-04-04-control-hijacking-defenses/>Control Hijacking Defenses</a></li><li><a href=/notes/cs155/2022-04-06-security-principles/>Security Principles</a></li><li><a href=/notes/cs155/2022-04-11-isolation-and-sandboxing/>Isolation and Sandboxing</a></li><li><a href=/notes/cs155/2022-04-13-vuln-finding/>Vuln Finding</a></li><li><a href=/notes/cs155/2022-04-18-web-security/>Web Security</a></li><li><a href=/notes/cs155/2022-04-20-web-attacks/>Web Attacks</a></li><li><a href=/notes/cs155/2022-04-25-web-defenses/>Web Defenses</a></li><li><a href=/notes/cs155/2022-05-04-processor-security/>Processor Security</a></li><li><a href=/notes/cs155/2022-05-09-internet-protocol-security/>Internet Protocol Security</a></li></ul></li><li><input type=checkbox id=section-471622039e8d87cd8d642483c8d218b8 class=toggle>
<label for=section-471622039e8d87cd8d642483c8d218b8 class="flex justify-between"><a role=button>CS 161, Winter 2022</a></label><ul><li><a href=/notes/cs161/2022-01-03-intro/>Intro</a></li><li><a href=/notes/cs161/2022-01-05-worst-case-and-asymptotic-analysis/>Worst Case and Asymptotic Analysis</a></li><li><a href=/notes/cs161/2022-01-10-recurrence-relations/>Recurrence Relations</a></li><li><a href=/notes/cs161/2022-01-12-median-and-selection/>Median and Selection</a></li><li><a href=/notes/cs161/2022-01-19-randomized-algorithms-and-quicksort/>Randomized Algorithms and Quicksort</a></li><li><a href=/notes/cs161/2022-01-24-sorting-lower-bounds/>Sorting Lower Bounds</a></li><li><a href=/notes/cs161/2022-01-26-binary-search-trees/>Binary Search Trees</a></li><li><a href=/notes/cs161/2022-01-31-hashing/>Hashing</a></li><li><a href=/notes/cs161/2022-02-02-graphs-and-graph-search/>Graphs and Graph Search</a></li><li><a href=/notes/cs161/2022-02-07-strongly-connected-components/>Strongly Connected Components</a></li><li><a href=/notes/cs161/2022-02-09-weighted-graphs-and-dijkstra/>Weighted Graphs and Dijkstra</a></li><li><a href=/notes/cs161/2022-02-14-dynamic-programming/>Dynamic Programming</a></li><li><a href=/notes/cs161/2022-02-16-dynamic-programming-applications/>Dynamic Programming Applications</a></li><li><a href=/notes/cs161/2022-02-23-greedy-algorithms/>Greedy Algorithms</a></li><li><a href=/notes/cs161/2022-02-28-minimum-spanning-trees/>Minimum Spanning Trees</a></li></ul></li><li><input type=checkbox id=section-0794dc87987599843ba69a8ea82e9b6a class=toggle>
<label for=section-0794dc87987599843ba69a8ea82e9b6a class="flex justify-between"><a role=button>CS 224U, Spring 2021</a></label><ul><li><a href=/notes/cs224u/2021-03-29-course-overview/>Course Overview</a></li><li><a href=/notes/cs224u/2021-03-31-vector-space-models/>Vector Space Models</a></li><li><a href=/notes/cs224u/2021-04-12-sentiment-analysis/>Sentiment Analysis</a></li></ul></li><li><input type=checkbox id=section-efae8264f5fe410feb1b40be6f99baea class=toggle>
<label for=section-efae8264f5fe410feb1b40be6f99baea class="flex justify-between"><a role=button>CS 229, Fall 2021</a></label><ul><li><a href=/notes/cs229/2021-09-21-intro/>Intro</a></li><li><a href=/notes/cs229/2021-09-23-supervised-learning-setup/>Supervised Learning Setup</a></li><li><a href=/notes/cs229/2021-09-28-logistic-regression/>Logistic Regression</a></li><li><a href=/notes/cs229/2021-09-30-generalized-linear-models/>Generalized Linear Models</a></li><li><a href=/notes/cs229/2021-10-05-generative-learning-algorithms/>Generative Learning Algorithms</a></li><li><a href=/notes/cs229/2021-10-07-naive-bayes/>Naive Bayes</a></li><li><a href=/notes/cs229/2021-10-12-kernel-methods-and-svm/>Kernel Methods and Svm</a></li><li><a href=/notes/cs229/2021-10-14-deep-learning/>Deep Learning</a></li><li><a href=/notes/cs229/2021-10-19-deep-learning-optimization/>Deep Learning Optimization</a></li><li><a href=/notes/cs229/2021-10-21-model-selection/>Model Selection</a></li></ul></li><li><input type=checkbox id=section-292ea4e126876f1048e61c4dc744546c class=toggle>
<label for=section-292ea4e126876f1048e61c4dc744546c class="flex justify-between"><a role=button>CS 251, Fall 2022</a></label><ul><li><a href=/notes/cs251/2022-09-26-intro/>Intro</a></li><li><a href=/notes/cs251/2022-09-28-bitcoin-mechanics/>Bitcoin Mechanics</a></li><li><a href=/notes/cs251/2022-10-03-bitcoin-scripts-and-wallets/>Bitcoin Scripts and Wallets</a></li><li><a href=/notes/cs251/2022-10-05-consensus/>Consensus</a></li><li><a href=/notes/cs251/2022-10-10-internet-consensus/>Internet Consensus</a></li><li><a href=/notes/cs251/2022-10-17-ethereum/>Ethereum</a></li></ul></li><li><input type=checkbox id=section-98d861db134658b3e020a9d74c50ad24 class=toggle>
<label for=section-98d861db134658b3e020a9d74c50ad24 class="flex justify-between"><a role=button>CS 255, Winter 2022</a></label><ul><li><a href=/notes/cs255/2022-01-03-intro/>Intro</a></li><li><a href=/notes/cs255/2022-01-05-stream-ciphers/>Stream Ciphers</a></li><li><a href=/notes/cs255/2022-01-10-block-ciphers/>Block Ciphers</a></li><li><a href=/notes/cs255/2022-01-12-pseudorandom-functions/>Pseudorandom Functions</a></li><li><a href=/notes/cs255/2022-01-19-data-integrity-and-macs/>Data Integrity and Macs</a></li><li><a href=/notes/cs255/2022-01-24-collision-resistance/>Collision Resistance</a></li><li><a href=/notes/cs255/2022-01-26-authenticated-encryption/>Authenticated Encryption</a></li><li><a href=/notes/cs255/2022-01-31-key-management/>Key Management</a></li><li><a href=/notes/cs255/2022-02-02-key-exchange-math/>Key Exchange Math</a></li><li><a href=/notes/cs255/2022-02-07-public-key-encryption/>Public Key Encryption</a></li><li><a href=/notes/cs255/2022-02-09-pke-schemes/>Pke Schemes</a></li><li><a href=/notes/cs255/2022-02-14-digital-signatures/>Digital Signatures</a></li><li><a href=/notes/cs255/2022-02-16-certificates/>Certificates</a></li><li><a href=/notes/cs255/2022-02-23-id-protocols/>Id Protocols</a></li><li><a href=/notes/cs255/2022-02-28-key-exchange-protocols/>Key Exchange Protocols</a></li><li><a href=/notes/cs255/2022-03-02-zero-knowledge-protocols/>Zero Knowledge Protocols</a></li><li><a href=/notes/cs255/2022-03-07-quantum-cryptography/>Quantum Cryptography</a></li></ul></li><li><input type=checkbox id=section-568ed1bc8289796d56a6d06a22d7eb40 class=toggle>
<label for=section-568ed1bc8289796d56a6d06a22d7eb40 class="flex justify-between"><a role=button>INTLPOL 268, Fall 2021</a></label><ul><li><a href=/notes/intlpol268/2021-09-20-intro/>Intro</a></li><li><a href=/notes/intlpol268/2021-09-22-legal-intro-and-electronic-communications-privacy-act/>Legal Intro and Electronic Communications Privacy Act</a></li><li><a href=/notes/intlpol268/2021-09-27-web-requests-and-attacks/>Web Requests and Attacks</a></li><li><a href=/notes/intlpol268/2021-09-29-ecpa-for-private-actors/>Ecpa for Private Actors</a></li><li><a href=/notes/intlpol268/2021-10-04-cyberattacks/>Cyberattacks</a></li><li><a href=/notes/intlpol268/2021-10-06-computer-fraud-and-abuse-act/>Computer Fraud and Abuse Act</a></li><li><a href=/notes/intlpol268/2021-10-11-network-security/>Network Security</a></li><li><a href=/notes/intlpol268/2021-10-13-cfaa-dmca-and-security-research/>Cfaa Dmca and Security Research</a></li><li><a href=/notes/intlpol268/2021-10-20-data-security-laws/>Data Security Laws</a></li><li><a href=/notes/intlpol268/2021-10-25-corporate-intrusion/>Corporate Intrusion</a></li><li><a href=/notes/intlpol268/2021-10-27-ransomware-and-foreign-hackers/>Ransomware and Foreign Hackers</a></li><li><a href=/notes/intlpol268/2021-11-01-cryptography/>Cryptography</a></li><li><a href=/notes/intlpol268/2021-11-03-cyber-conflict/>Cyber Conflict</a></li><li><a href=/notes/intlpol268/2021-11-08-dark-web-and-cryptocurrencies/>Dark Web and Cryptocurrencies</a></li><li><a href=/notes/intlpol268/2021-11-10-encryption-and-technical-assistance/>Encryption and Technical Assistance</a></li><li><a href=/notes/intlpol268/2021-11-15-malware/>Malware</a></li><li><a href=/notes/intlpol268/2021-11-17-government-hacking/>Government Hacking</a></li><li><a href=/notes/intlpol268/2021-11-29-new-frontiers/>New Frontiers</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notes/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Modern Multicore Processors</strong>
<label for=toc-control><img src=/notes/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#a-modern-multi-core-processor>A Modern Multi-core Processor</a><ul><li><a href=#part-1-speeding-up-an-example-sine-program>Part 1: Speeding up an example sine program</a><ul><li><a href=#pre-multicore-era-processor>Pre-multicore era processor</a></li><li><a href=#multicore-era-processor>Multicore-era processor</a><ul><li><a href=#transforming-a-program-to-multicore>Transforming a program to multicore</a></li><li><a href=#data-parallel-expressions>Data-parallel expressions</a></li></ul></li><li><a href=#simd-processing>SIMD processing</a><ul><li><a href=#conditional-execution-with-simd>Conditional execution with SIMD</a></li><li><a href=#simd-jargon>SIMD jargon</a></li><li><a href=#modern-cpu-examples-of-simd>Modern CPU examples of SIMD</a><ul><li><a href=#example-intel-i7-7700k-kaby-lake-2017>Example: Intel i7-7700K (Kaby Lake, 2017)</a></li></ul></li><li><a href=#simd-on-gpus>SIMD on GPUs</a></li></ul></li></ul></li><li><a href=#part-2-accessing-memory>Part 2: accessing memory</a><ul><li><a href=#loadstore-instructions-that-access-memory>Load/store: instructions that access memory</a></li><li><a href=#memory-terminology>Memory terminology</a></li><li><a href=#caches>Caches</a></li><li><a href=#data-prefetching>Data prefetching</a></li><li><a href=#multithreading-to-reduce-stalls>Multithreading to reduce stalls</a></li><li><a href=#hardware-supported-multithreading>Hardware-supported multithreading</a></li><li><a href=#latency-and-bandwidth>Latency and Bandwidth</a></li></ul></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=a-modern-multi-core-processor>A Modern Multi-core Processor
<a class=anchor href=#a-modern-multi-core-processor>#</a></h1><h2 id=part-1-speeding-up-an-example-sine-program>Part 1: Speeding up an example sine program
<a class=anchor href=#part-1-speeding-up-an-example-sine-program>#</a></h2><p>Computes <code>sin</code> across an array of <code>float</code> values using a Taylor series expansion</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#8be9fd>void</span> <span style=color:#50fa7b>sinx</span>(<span style=color:#8be9fd>int</span> N, <span style=color:#8be9fd>int</span> terms, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>x, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>y) {
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> (<span style=color:#8be9fd>int</span> i <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>; i <span style=color:#ff79c6>&lt;</span> N; i<span style=color:#ff79c6>++</span>) {
</span></span><span style=display:flex><span>        <span style=color:#8be9fd>float</span> value <span style=color:#ff79c6>=</span> x[i];
</span></span><span style=display:flex><span>        <span style=color:#8be9fd>float</span> numer <span style=color:#ff79c6>=</span> x[i] <span style=color:#ff79c6>*</span> x[i] <span style=color:#ff79c6>*</span> x[i];
</span></span><span style=display:flex><span>        <span style=color:#8be9fd>int</span> denom <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>6</span>; <span style=color:#6272a4>// 3!
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>        <span style=color:#8be9fd>int</span> sign <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> (<span style=color:#8be9fd>int</span> j <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>; j <span style=color:#ff79c6>&lt;=</span> terms; j<span style=color:#ff79c6>++</span>) {
</span></span><span style=display:flex><span>            value <span style=color:#ff79c6>+=</span> sign <span style=color:#ff79c6>*</span> numer <span style=color:#ff79c6>/</span> denom;
</span></span><span style=display:flex><span>            numer <span style=color:#ff79c6>*=</span> x[i] <span style=color:#ff79c6>*</span> x[i];
</span></span><span style=display:flex><span>            denom <span style=color:#ff79c6>*=</span> (<span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> j <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>2</span>) <span style=color:#ff79c6>*</span> (<span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> j <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>3</span>);
</span></span><span style=display:flex><span>            sign <span style=color:#ff79c6>*=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        y[i] <span style=color:#ff79c6>=</span> value;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This compiles to some assembly:</p><pre tabindex=0><code class=language-arm data-lang=arm>ld  r0, addr[r1] // x[i]
mul r1, r0, r0
mul r1, r1, r0
// ...
st  addr[r2], r0 // y[i]
</code></pre><h3 id=pre-multicore-era-processor>Pre-multicore era processor
<a class=anchor href=#pre-multicore-era-processor>#</a></h3><ul><li>Majority of chip transistors used to perform ops that help make <em>single</em> instruction streams run fast<ul><li>Out of order control logic</li><li>Fancy branch predictor</li><li>Memory prefetcher</li><li>More transistors = larger cache</li></ul></li></ul><h3 id=multicore-era-processor>Multicore-era processor
<a class=anchor href=#multicore-era-processor>#</a></h3><ul><li>Idea: use transistor count to add more cores to the processor<ul><li>Simple cores: each core maybe slowre at running a single instruction stream than original fancy core (e.g. 25% slower)</li><li>However, with two cores: 2 * 0.75 = 1.5 = 50% speedup!</li></ul></li><li>However: C program will compile to single instruction stream that runs on only one thread on one core</li></ul><h4 id=transforming-a-program-to-multicore>Transforming a program to multicore
<a class=anchor href=#transforming-a-program-to-multicore>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#ff79c6>typedef</span> <span style=color:#ff79c6>struct</span> {
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>int</span> N,
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>int</span> terms;
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>x;
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>y;
</span></span><span style=display:flex><span>} my_args;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd>void</span> <span style=color:#50fa7b>my_thread_func</span>(my_args <span style=color:#ff79c6>*</span>args) {
</span></span><span style=display:flex><span>    sinx(args<span style=color:#ff79c6>-&gt;</span>N, args<span style=color:#ff79c6>-&gt;</span>terms, args<span style=color:#ff79c6>-&gt;</span>x, args<span style=color:#ff79c6>-&gt;</span>y); <span style=color:#6272a4>// do work
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8be9fd>void</span> <span style=color:#50fa7b>parallel_sinx</span>(<span style=color:#8be9fd>int</span> N, <span style=color:#8be9fd>int</span> terms, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>x, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>y) {
</span></span><span style=display:flex><span>    std<span style=color:#ff79c6>::</span><span style=color:#ff79c6>thread</span> my_thread;
</span></span><span style=display:flex><span>    my_args args;
</span></span><span style=display:flex><span>    args.N <span style=color:#ff79c6>=</span> N<span style=color:#ff79c6>/</span><span style=color:#bd93f9>2</span>;
</span></span><span style=display:flex><span>    args.terms <span style=color:#ff79c6>=</span> terms;
</span></span><span style=display:flex><span>    args.x <span style=color:#ff79c6>=</span> x;
</span></span><span style=display:flex><span>    args.y <span style=color:#ff79c6>=</span> y;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    my_thread <span style=color:#ff79c6>=</span> std<span style=color:#ff79c6>::</span><span style=color:#ff79c6>thread</span>(my_thread_func, <span style=color:#ff79c6>&amp;</span>args); <span style=color:#6272a4>// launch thread
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    sinx(N <span style=color:#ff79c6>-</span> args.N, terms, x <span style=color:#ff79c6>+</span> args.N, y <span style=color:#ff79c6>+</span> args.N); <span style=color:#6272a4>// do work on main thread
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>/* NOTE: because of this waiting, we can only ever get 2x speedup!! */</span>
</span></span><span style=display:flex><span>    my_thread.join(); <span style=color:#6272a4>// wait for thread to complete
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>}
</span></span></code></pre></div><h4 id=data-parallel-expressions>Data-parallel expressions
<a class=anchor href=#data-parallel-expressions>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#8be9fd>void</span> <span style=color:#50fa7b>sinx</span>(<span style=color:#8be9fd>int</span> N, <span style=color:#8be9fd>int</span> terms, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>x, <span style=color:#8be9fd>float</span> <span style=color:#ff79c6>*</span>y) {
</span></span><span style=display:flex><span>    <span style=color:#6272a4>// original:
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    <span style=color:#6272a4>// for (int i = 0; i &lt; N; i++) {
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>// new: fictituous forall construct that tells the compiler
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    <span style=color:#6272a4>// that each of these iterations are independent
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    forall (<span style=color:#8be9fd>int</span> i from <span style=color:#bd93f9>0</span> to N) {
</span></span><span style=display:flex><span>        <span style=color:#8be9fd>float</span> value <span style=color:#ff79c6>=</span> x[i];
</span></span><span style=display:flex><span>        <span style=color:#8be9fd>float</span> numer <span style=color:#ff79c6>=</span> x[i] <span style=color:#ff79c6>*</span> x[i] <span style=color:#ff79c6>*</span> x[i];
</span></span><span style=display:flex><span>        <span style=color:#8be9fd>int</span> denom <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>6</span>; <span style=color:#6272a4>// 3!
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>        <span style=color:#8be9fd>int</span> sign <span style=color:#ff79c6>=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>for</span> (<span style=color:#8be9fd>int</span> j <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1</span>; j <span style=color:#ff79c6>&lt;=</span> terms; j<span style=color:#ff79c6>++</span>) {
</span></span><span style=display:flex><span>            value <span style=color:#ff79c6>+=</span> sign <span style=color:#ff79c6>*</span> numer <span style=color:#ff79c6>/</span> denom;
</span></span><span style=display:flex><span>            numer <span style=color:#ff79c6>*=</span> x[i] <span style=color:#ff79c6>*</span> x[i];
</span></span><span style=display:flex><span>            denom <span style=color:#ff79c6>*=</span> (<span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> j <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>2</span>) <span style=color:#ff79c6>*</span> (<span style=color:#bd93f9>2</span> <span style=color:#ff79c6>*</span> j <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>3</span>);
</span></span><span style=display:flex><span>            sign <span style=color:#ff79c6>*=</span> <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        y[i] <span style=color:#ff79c6>=</span> value;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>With such a construct, a compiler might be able to autogenerate threaded code.</p><h3 id=simd-processing>SIMD processing
<a class=anchor href=#simd-processing>#</a></h3><ul><li>Single instruction, multiple data</li><li>Additional execution units (ALUs) increase compute capability</li><li>Idea: Amortize cost/complexity of managing an instruction stream across many ALUs<ul><li>Same instruction broadcast to all ALUs</li><li>Operation executed in parallel on all ALUs</li></ul></li><li>e.g. <a href=#todays-example-program>Original scalar program</a> processes one array element using scalar instructions on scalar registers</li><li>Instead, use vector operations (i.e. <code>__m256</code> types and <code>__mm256</code> arith and load operations)<ul><li>These are intrinsic datatypes and functions available to C programmers</li><li>These operate on vectors of eight 32-bit values (e.g. vector of 8 floats)</li></ul><ul><li>Compiled program processes eight array elements simultaneously using vector instructions on 256-bit vector register</li></ul></li><li>Note: with <a href=#data-parallel-expressions>Data-parallel expression</a> example code, the <code>forall</code> abstraction can also facilitate autogeneration of SIMD instructions<ul><li>In addition to multi-core parallel code!</li></ul></li></ul><h4 id=conditional-execution-with-simd>Conditional execution with SIMD
<a class=anchor href=#conditional-execution-with-simd>#</a></h4><p>Consider the following example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>forall (<span style=color:#8be9fd>int</span> i from <span style=color:#bd93f9>0</span> to N) {
</span></span><span style=display:flex><span>    <span style=color:#8be9fd>float</span> t <span style=color:#ff79c6>=</span> x[i];
</span></span><span style=display:flex><span>    <span style=color:#6272a4>// unconditional code
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    <span style=color:#ff79c6>if</span> (t <span style=color:#ff79c6>&gt;</span> <span style=color:#bd93f9>0.0</span>) {
</span></span><span style=display:flex><span>        t <span style=color:#ff79c6>=</span> t <span style=color:#ff79c6>*</span> t;
</span></span><span style=display:flex><span>        t <span style=color:#ff79c6>=</span> t <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>50.0</span>;
</span></span><span style=display:flex><span>        t <span style=color:#ff79c6>=</span> t <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>100.0</span>;
</span></span><span style=display:flex><span>    } <span style=color:#ff79c6>else</span> {
</span></span><span style=display:flex><span>        t <span style=color:#ff79c6>=</span> t <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>30.0</span>;
</span></span><span style=display:flex><span>        t <span style=color:#ff79c6>=</span> t <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>10.0</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#6272a4>// resume unconditional code
</span></span></span><span style=display:flex><span><span style=color:#6272a4></span>    y[i] <span style=color:#ff79c6>=</span> t;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ul><li>How to process this on a vector-only CPU?<ul><li>Mask (discard) output of ALU</li><li>Run all with <code>true</code> branches, discard output of <code>false</code> branches</li><li>Run all with <code>false</code> branches, discard output of <code>true</code> branches</li><li>Worst case: 1/8 of peak performance<ul><li>Can create this by making the <code>if</code> branch extremely costly, but processor is still forced to send all the branch that way<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#ff79c6>if</span> (i <span style=color:#ff79c6>==</span> <span style=color:#bd93f9>0</span>) {
</span></span><span style=display:flex><span>    do_something_super_costly();
</span></span><span style=display:flex><span>} <span style=color:#ff79c6>else</span> {
</span></span><span style=display:flex><span>    do_something_normal();
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li></ul></li></ul></li></ul><h4 id=simd-jargon>SIMD jargon
<a class=anchor href=#simd-jargon>#</a></h4><ul><li>Instruction stream coherence (&ldquo;coherent execution&rdquo;)<ul><li>Property of a program where same instruction stream applies to many data elements</li><li>NECESSARY for SIMD processing to be used efficiently</li><li>NOT NECESSARY for efficient parallelization across different cores</li></ul></li><li>Divergent execution<ul><li>Lack of instruction stream coherence</li></ul></li></ul><h4 id=modern-cpu-examples-of-simd>Modern CPU examples of SIMD
<a class=anchor href=#modern-cpu-examples-of-simd>#</a></h4><ul><li>AVX2 (Intel): 256b operations: 8x32bit or 4x64bit</li><li>AVX512 (Intel): 512b: 16x32bit</li><li>ARM Neon: 128b: 4x32bit</li><li>Instructions generated by complier<ul><li>Requested by programmer using intrinsics</li><li>Or parallel language semantics (e.g. <code>forall</code>)</li><li>Inferred by dependency analysis of loops by auto-vectorizing compiler</li></ul></li><li>Explicit SIMD: SIMD parallelization performed at compile time<ul><li>SIMD instructions in binary itself (e.g., <code>vstoreps</code>, <code>vmulps</code>, etc.)</li></ul></li></ul><h5 id=example-intel-i7-7700k-kaby-lake-2017>Example: Intel i7-7700K (Kaby Lake, 2017)
<a class=anchor href=#example-intel-i7-7700k-kaby-lake-2017>#</a></h5><ul><li>4 core CPU</li><li>Three 8-wide SIMD ALUs per core for AVX2</li><li>4 cores x 8-wide SIMD * 3 * 4.2 GHz = 400 GFLOPs</li></ul><h4 id=simd-on-gpus>SIMD on GPUs
<a class=anchor href=#simd-on-gpus>#</a></h4><ul><li>&ldquo;Implicit SIMD&rdquo;<ul><li>Compiler generates binary with scalar instructions</li><li>But N instances of program always run together on processor</li><li>Hardware, not compiler, responsible for executing same instruction on multiple program instances on different data on SIMD ALUs</li></ul></li><li>SIMD width of most modern GPUs ranges from 8 to 32<ul><li>Divergent execution can result in 1/32 of peak capacity</li></ul></li></ul><h2 id=part-2-accessing-memory>Part 2: accessing memory
<a class=anchor href=#part-2-accessing-memory>#</a></h2><h3 id=loadstore-instructions-that-access-memory>Load/store: instructions that access memory
<a class=anchor href=#loadstore-instructions-that-access-memory>#</a></h3><pre tabindex=0><code class=language-arm data-lang=arm>// load 4b value in memory starting from address stored by register r2
// and put it into register r0
ld r0 mem[r2]

// store 4b value in r0 into address stored by register r2
st r0 mem[r2]
</code></pre><h3 id=memory-terminology>Memory terminology
<a class=anchor href=#memory-terminology>#</a></h3><ul><li>Memory address space<ul><li>Memory organized as sequence of bytes</li><li>Each byte identified by address in memory</li><li>Address space: total addressable memory of a program</li></ul></li><li>Memory access latency<ul><li>Amount of time it takes memory system to provide data to the processor</li><li>e.g. 100 clock cycles, 100 nsec</li></ul></li><li>Stalls<ul><li>A processor &ldquo;stalls&rdquo; when it cannot run the next instruction in an instruction stream due to dependency on a previous incomplete instruction</li><li>e.g. Accessing memory can cause stalls<pre tabindex=0><code class=language-arm data-lang=arm>ld r0 mem[r2]
ld r1 mem[r3]
// Dependency: cannot execute `add` instruction until data 
// from mem[r2] and mem[r3] have been loaded from memory
add r0, r0, r1
</code></pre></li><li>Memory access times: ~hundreds of cycles<ul><li>Measure of latency</li></ul></li></ul></li></ul><h3 id=caches>Caches
<a class=anchor href=#caches>#</a></h3><ul><li>On-chip storage that maintains copy of subset of values in memory</li><li>Address is &ldquo;in the cache&rdquo;: processor can load and store address more quickly than if data resided in memory</li><li>Hardware implementation detail that does not impact program output; only performance</li><li>For this class, abstraction: assume cache of size N keeps last N addresses accessed<ul><li>&ldquo;LIFO&rdquo; policy: on each memory access, throw out oldest data in cache to make room for newly accessed data</li><li>In real world, other policies:<ul><li>Direct mapped cache</li><li>Set-associative cache</li><li>Cache line</li></ul></li></ul></li><li>Reduce length of stalls and memory access latency<ul><li>Specifically: when accessing data that had recently been accessed</li></ul></li><li>e.g. Data access times, Intel Kaby Lake (2017)<ul><li>L1 cache: 4 cycles</li><li>L2 cache: 12 cycles</li><li>L3 cache: 38 cycles</li><li>DRAM: best case ~248 cycles</li></ul></li></ul><h3 id=data-prefetching>Data prefetching
<a class=anchor href=#data-prefetching>#</a></h3><ul><li>Logic on modern CPUs for guessing what data will be accessed in the future<ul><li>Prefetch this data into caches</li><li>Dynamically analyze program memory access patterns to make predictions</li></ul></li><li>Reduces stalls since data is already resident in cache when accessed<ul><li>However: can reduce perf. if guess is wrong (consumes bandwidth, pollutes caches)</li></ul></li></ul><h3 id=multithreading-to-reduce-stalls>Multithreading to reduce stalls
<a class=anchor href=#multithreading-to-reduce-stalls>#</a></h3><ul><li>Interleave processing of multiple threads on same core to hide stalls<ul><li>Can&rsquo;t make process on one thread? Work on another one</li><li>Once hitting a stall on one thread, immediately switch over another one</li></ul></li><li>Idea: potentially increase time to complete a single thread&rsquo;s work in order to increase overall system throuput when running multiple threads</li><li>However: this requires storing execution contexts<ul><li>Tradeoff between memory caching and storing these execution contexts</li></ul></li><li>Takeaway: a processor with multiple HW threads has the ability to avoid stalls by interleaving executions<ul><li>This doesn&rsquo;t affect mem. access latency; it just improves processor utilization</li></ul></li></ul><h3 id=hardware-supported-multithreading>Hardware-supported multithreading
<a class=anchor href=#hardware-supported-multithreading>#</a></h3><ul><li>Core manages execution contexts for multiple threads</li><li>Interleaved multi-threading</li><li>Simultaneous multi-threading (SMT)<ul><li>e.g. Intel Hyper-Threading</li></ul></li></ul><p>Broader takeaway: can combine each of these parallelism constructs!</p><ul><li>This is what modern chips (e.g., Intel Skylake/Kaby Lake, Nvidia GPUs) do</li></ul><h3 id=latency-and-bandwidth>Latency and Bandwidth
<a class=anchor href=#latency-and-bandwidth>#</a></h3><ul><li>Memory bandwidth: rate at which memory system can provide data to a processor (e.g., 20 GB/s)<ul><li>Execution speed is often limited by available bandwidth</li><li>e.g. data transfer speed = 8 bytes per clock: then simple math adds (1 add per clock) can create up to 3 outstanding load requests</li><li>This is called bandwidth-bound execution: instruction throughput limited by bandwidth, not latency or outstanding requests etc.</li><li>Influences chip design: modern GPUs place memory on or near chip</li></ul></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#a-modern-multi-core-processor>A Modern Multi-core Processor</a><ul><li><a href=#part-1-speeding-up-an-example-sine-program>Part 1: Speeding up an example sine program</a><ul><li><a href=#pre-multicore-era-processor>Pre-multicore era processor</a></li><li><a href=#multicore-era-processor>Multicore-era processor</a><ul><li><a href=#transforming-a-program-to-multicore>Transforming a program to multicore</a></li><li><a href=#data-parallel-expressions>Data-parallel expressions</a></li></ul></li><li><a href=#simd-processing>SIMD processing</a><ul><li><a href=#conditional-execution-with-simd>Conditional execution with SIMD</a></li><li><a href=#simd-jargon>SIMD jargon</a></li><li><a href=#modern-cpu-examples-of-simd>Modern CPU examples of SIMD</a><ul><li><a href=#example-intel-i7-7700k-kaby-lake-2017>Example: Intel i7-7700K (Kaby Lake, 2017)</a></li></ul></li><li><a href=#simd-on-gpus>SIMD on GPUs</a></li></ul></li></ul></li><li><a href=#part-2-accessing-memory>Part 2: accessing memory</a><ul><li><a href=#loadstore-instructions-that-access-memory>Load/store: instructions that access memory</a></li><li><a href=#memory-terminology>Memory terminology</a></li><li><a href=#caches>Caches</a></li><li><a href=#data-prefetching>Data prefetching</a></li><li><a href=#multithreading-to-reduce-stalls>Multithreading to reduce stalls</a></li><li><a href=#hardware-supported-multithreading>Hardware-supported multithreading</a></li><li><a href=#latency-and-bandwidth>Latency and Bandwidth</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>